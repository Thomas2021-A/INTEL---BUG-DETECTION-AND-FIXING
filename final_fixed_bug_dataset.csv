Project,Bug_ID,Buggy_Code,Fixed_Code
ansible,1,,
ansible,10,current_line.next.prev = current_line.prev,"self.prev = None
self.next = None
if current_line.next is not None:
current_line.next.prev = current_line.prev"
ansible,11,"banner_cmd += want['text'].strip()
rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])
if rc == 0:
output = out
else:
rc, out, err = exec_command(module,
'show running-config | begin banner %s'
% module.params['banner'])
if out:
output = re.search(r'\^C(.*?)\^C', out, re.S).group(1).strip()
if text:
text = str(text).strip()","banner_cmd += want['text'].strip('\n')
""""""
This function gets the banner config without stripping any whitespaces,
and then fetches the required banner from it.
:param module:
:return: banner config dict object.
""""""
out = get_config(module, flags='| begin banner %s' % module.params['banner'])
if out:
regex = 'banner ' + module.params['banner'] + ' ^C\n'
if search('banner ' + module.params['banner'], out, M):
output = str((out.split(regex))[1].split(""^C\n"")[0])
else:
output = None"
ansible,12,"ret.append(os.getenv(var, ''))","ret.append(py3compat.environ.get(var, ''))"
ansible,13,"name, dummy, requirement = collection_input.partition(':')
elif urlparse(collection).scheme:
b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)","requirement = None
if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \
urlparse(collection_input).scheme.lower() in ['http', 'https']:
# Arg is a file path or URL to a collection
name = collection_input
else:
name, dummy, requirement = collection_input.partition(':')
elif urlparse(collection).scheme.lower() in ['http', 'https']:
try:
b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)
except urllib_error.URLError as err:
raise AnsibleError(""Failed to download collection tar from '%s': %s""
% (to_native(collection), to_native(err)))"
ansible,14,"url = _urljoin(self.api_server, data['next_link'])
display.vvvv(""Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s""
% (role_id, related, to_text(e)))","from urllib.parse import urlparse
# Python 2
from urlparse import urlparse
# https://github.com/ansible/ansible/issues/64355
# api_server contains part of the API path but next_link includes the the /api part so strip it out.
url_info = urlparse(self.api_server)
base_url = ""%s://%s/"" % (url_info.scheme, url_info.netloc)
url = _urljoin(base_url, data['next_link'])
display.warning(""Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s""
% (role_id, related, to_text(e)))"
ansible,15,if needs_update('state') and not needs_update('vrf'):,if needs_update('state'):
ansible,16,"# Always use 'processor' count for ARM systems
if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):
'processor_count': 16,
'processor_vcpus': 16
'processor_count': 48,
'processor_vcpus': 48","# The fields for Power CPUs include 'processor' and 'cpu'.
# Always use 'processor' count for ARM and Power systems
if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):
'processor_count': 8,
'processor_vcpus': 8
'processor_count': 24,
'processor_vcpus': 24"
ansible,17,,
ansible,18,"desc=""Perform various Role related operations."",
description='your description',
display.display(""- %s was created successfully"" % obj_name)","desc=""Perform various Role and Collection related operations."",
description='your {0} description'.format(galaxy_type),
display.display(""- %s %s was created successfully"" % (galaxy_type.title(), obj_name))"
ansible,2,"def __gt__(self, other):
return not self.__lt__(other)
return self.__gt__(other) or self.__eq__(other)
def __gt__(self, other):
return not self.__lt__(other)
return self.__gt__(other) or self.__eq__(other)","def __gt__(self, other):
return not self.__le__(other)
return not self.__lt__(other)
def __gt__(self, other):
return not self.__le__(other)
return not self.__lt__(other)"
ansible,3,elif path == '/etc/lsb-release' and 'Kali' in data:,"elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:
# Kali does not provide /etc/lsb-release anymore"
ansible,4,"_collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)","_collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,
always_post_validate=True, static=True)
# This duplicates static attr checking logic from post_validate()
# because if the user attempts to template a collection name, it will
# error before it ever gets to the post_validate() warning.
env = Environment()
for collection_name in ds:
if is_template(collection_name, env):
display.warning('""collections"" is not templatable, but we found: %s, '
'it will not be templated and will be used ""as is"".' % (collection_name))"
ansible,5,"msg = ""missing required arguments: %s"" % "", "".join(missing)
expected = ""TypeError('parameters are mutually exclusive: string1|string2, box|fox|socks',)""
assert e.value == expected
assert ""TypeError: 'NoneType' object is not iterable"" in to_native(te.error)","msg = ""missing required arguments: %s"" % "", "".join(sorted(missing))
expected = ""parameters are mutually exclusive: string1|string2, box|fox|socks""
assert to_native(e.value) == expected
assert ""'NoneType' object is not iterable"" in to_native(te.value)"
ansible,6,"# In the case we are checking a new requirement on a base requirement (parent != None) we can't accept
# version as '*' (unknown version) unless the requirement is also '*'.
if parent and version == '*' and requirement != '*':
break
elif requirement == '*' or version == '*':
continue
version = manifest['version']
existing[0].add_requirement(to_text(collection_info), requirement)","# In the case we are checking a new requirement on a base requirement (parent != None) we can't accept
# version as '*' (unknown version) unless the requirement is also '*'.
if parent and version == '*' and requirement != '*':
display.warning(""Failed to validate the collection requirement '%s:%s' for %s when the existing ""
""install does not have a version set, the collection may not work.""
% (to_text(self), req, parent))
continue
elif requirement == '*' or version == '*':
continue
version = to_text(manifest['version'], errors='surrogate_or_strict')
if not hasattr(LooseVersion(version), 'version'):
display.warning(""Collection at '%s' does not have a valid version set, falling back to '*'. Found ""
""version: '%s'"" % (to_text(b_path), version))
version = '*'
existing[0].add_requirement(parent, requirement)"
ansible,7,"for key in to_remove:
commands.append(""no {0}"".format(key))","for key in to_remove:
if key in to_set.keys():
continue
commands.append(""no {0}"".format(key))"
ansible,8,"parts = []
for arg in args:
arg = self._unquote(arg).replace('/', '\\')
parts.extend([a for a in arg.split('\\') if a])
path = '\\'.join(parts)
if path.startswith('~'):
return path
return path","# use normpath() to remove doubled slashed and convert forward to backslashes
parts = [ntpath.normpath(self._unquote(arg)) for arg in args]
# Becuase ntpath.join treats any component that begins with a backslash as an absolute path,
# we have to strip slashes from at least the beginning, otherwise join will ignore all previous
# path components except for the drive.
return ntpath.join(parts[0], *[part.strip('\\') for part in parts[1:]])"
ansible,9,"args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]
pool_id, quantity = value, 1
pool_ids[pool_id] = str(quantity)","args = [SUBMAN_CMD, 'attach', '--pool', pool_id]
if quantity is not None:
args.extend(['--quantity', to_native(quantity)])
pool_id, quantity = value, None
pool_ids[pool_id] = quantity"
black,1,"executor = ProcessPoolExecutor(max_workers=worker_count)
executor.shutdown()
executor: Executor,","try:
executor = ProcessPoolExecutor(max_workers=worker_count)
except OSError:
# we arrive here if the underlying system does not support multi-processing
# like in AWS Lambda, in which case we gracefully fallback to the default
# mono-process Executor by using None
executor = None
if executor is not None:
executor.shutdown()
executor: Optional[Executor],"
black,10,"elif char == ' ':
elif char == '\t':
current_column += 4",elif char in ' \t':
black,11,"if not line.should_explode and is_line_short_enough(
line, line_length=line_length, line_str=line_str","# we don't want to split special comments like type annotations
# https://github.com/python/typing/issues/186
has_special_comment = False
for leaf in line.leaves:
for comment in line.comments_after(leaf):
if leaf.type == token.COMMA and is_special_comment(comment):
has_special_comment = True
if (
not has_special_comment
and not line.should_explode
and is_line_short_enough(line, line_length=line_length, line_str=line_str)
""""""Return True if the given leaf is a special comment.
Only returns true for type comments for now.""""""
t = leaf.type
v = leaf.value
return bool(
(t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith(""# type:""))
)"
black,12,"_for_loop_variable: int = 0
_lambda_arguments: int = 0
self._for_loop_variable += 1
if self._for_loop_variable and leaf.type == token.NAME and leaf.value == ""in"":
self._for_loop_variable -= 1
self._lambda_arguments += 1
if self._lambda_arguments and leaf.type == token.COLON:
self._lambda_arguments -= 1","_for_loop_depths: List[int] = Factory(list)
_lambda_argument_depths: List[int] = Factory(list)
self._for_loop_depths.append(self.depth)
if (
self._for_loop_depths
and self._for_loop_depths[-1] == self.depth
and leaf.type == token.NAME
and leaf.value == ""in""
):
self._for_loop_depths.pop()
self._lambda_argument_depths.append(self.depth)
if (
self._lambda_argument_depths
and self._lambda_argument_depths[-1] == self.depth
and leaf.type == token.COLON
):
self._lambda_argument_depths.pop()"
black,13,"if token == 'def':
async_def = True
async_def_indent = indents[-1]","if token in ('def', 'for'):
if token == 'def':
async_def = True
async_def_indent = indents[-1]"
black,14,"imports = set()
for import_from_child in first_child.children[3:]:
if isinstance(import_from_child, Leaf):
if import_from_child.type == token.NAME:
imports.add(import_from_child.value)
else:
assert import_from_child.type == syms.import_as_names
for leaf in import_from_child.children:
if isinstance(leaf, Leaf) and leaf.type == token.NAME:
imports.add(leaf.value)","Generator,
imports: Set[str] = set()
def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:
for child in children:
if isinstance(child, Leaf):
if child.type == token.NAME:
yield child.value
elif child.type == syms.import_as_name:
orig_name = child.children[0]
assert isinstance(orig_name, Leaf), ""Invalid syntax parsing imports""
assert orig_name.type == token.NAME, ""Invalid syntax parsing imports""
yield orig_name.value
elif child.type == syms.import_as_names:
yield from get_imports_from_children(child.children)
else:
assert False, ""Invalid syntax parsing imports""
imports |= set(get_imports_from_children(first_child.children[3:]))"
black,15,"Type,
""""""Base exception for `# fmt: on` and `# fmt: off` handling.
It holds the number of bytes of the prefix consumed before the format
control comment appeared.
""""""
def __init__(self, consumed: int) -> None:
super().__init__(consumed)
self.consumed = consumed
def trim_prefix(self, leaf: Leaf) -> None:
leaf.prefix = leaf.prefix[self.consumed :]
def leaf_from_consumed(self, leaf: Leaf) -> Leaf:
""""""Returns a new Leaf from the consumed part of the prefix.""""""
unformatted_prefix = leaf.prefix[: self.consumed]
return Leaf(token.NEWLINE, unformatted_prefix)
""""""Found a comment like `# fmt: on` in the file.""""""
""""""Found a comment like `# fmt: off` in the file.""""""
def show(cls, code: str) -> None:
list(v.visit(lib2to3_parse(code)))
""""""Just like :class:`Line` but stores lines which aren't reformatted.""""""
def append(self, leaf: Leaf, preformatted: bool = True) -> None:
""""""Just add a new `leaf` to the end of the lines.
The `preformatted` argument is ignored.
Keeps track of indentation `depth`, which is useful when the user
says `# fmt: on`. Otherwise, doesn't do anything with the `leaf`.
""""""
try:
list(generate_comments(leaf))
except FormatOn as f_on:
self.leaves.append(f_on.leaf_from_consumed(leaf))
raise
self.leaves.append(leaf)
if leaf.type == token.INDENT:
self.depth += 1
elif leaf.type == token.DEDENT:
self.depth -= 1
def __str__(self) -> str:
""""""Render unformatted lines from leaves which were added with `append()`.
`depth` is not used for indentation in this case.
""""""
if not self:
return ""\n""
res = """"
for leaf in self.leaves:
res += str(leaf)
return res
def append_comment(self, comment: Leaf) -> bool:
""""""Not implemented in this class. Raises `NotImplementedError`.""""""
raise NotImplementedError(""Unformatted lines don't store comments separately."")
def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:
""""""Does nothing and returns False.""""""
return False
def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:
""""""Does nothing and returns False.""""""
return False
if isinstance(current_line, UnformattedLines):
return 0, 0
def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:
if self.current_line.__class__ == type:
self.current_line.depth += indent
else:
self.current_line = type(depth=self.current_line.depth + indent)
self.current_line = type(depth=complete_line.depth + indent)
def visit(self, node: LN) -> Iterator[Line]:
""""""Main method to visit `node` and its children.
Yields :class:`Line` objects.
""""""
if isinstance(self.current_line, UnformattedLines):
# File contained `# fmt: off`
yield from self.visit_unformatted(node)
else:
yield from super().visit(node)
try:
for comment in generate_comments(node):
if any_open_brackets:
# any comment within brackets is subject to splitting
self.current_line.append(comment)
elif comment.type == token.COMMENT:
# regular trailing comment
self.current_line.append(comment)
yield from self.line()
else:
# regular standalone comment
yield from self.line()
self.current_line.append(comment)
yield from self.line()
except FormatOff as f_off:
f_off.trim_prefix(node)
yield from self.line(type=UnformattedLines)
yield from self.visit(node)
except FormatOn as f_on:
# This only happens here if somebody says ""fmt: on"" multiple
# times in a row.
f_on.trim_prefix(node)
yield from self.visit_default(node)
else:
normalize_prefix(node, inside_brackets=any_open_brackets)
if self.normalize_strings and node.type == token.STRING:
normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)
normalize_string_quotes(node)
if node.type not in WHITESPACE:
self.current_line.append(node)
def visit_unformatted(self, node: LN) -> Iterator[Line]:
""""""Used when file contained a `# fmt: off`.""""""
if isinstance(node, Node):
for child in node.children:
yield from self.visit(child)
else:
try:
self.current_line.append(node)
except FormatOn as f_on:
f_on.trim_prefix(node)
yield from self.line()
yield from self.visit(node)
if node.type == token.ENDMARKER:
# somebody decided not to put a final `# fmt: on`
yield from self.line()
if pc.value in FMT_ON:
raise FormatOn(pc.consumed)
if pc.value in FMT_OFF:
if pc.type == STANDALONE_COMMENT:
raise FormatOff(pc.consumed)
prev = preceding_leaf(leaf)
if not prev or prev.type in WHITESPACE:  # standalone comment in disguise
raise FormatOff(pc.consumed)
if isinstance(line, UnformattedLines) or line.is_comment:
""""""Allow `# fmt: off`/`# fmt: on` within bracket pairs.
Ignores `# fmt: off` and `# fmt: on` outside of brackets.
Raises :exc:`SyntaxError` if no matching `# fmt: on` is found for a `# fmt: off`
given inside brackets.
""""""
try_again = hide_fmt_off(node)
bt = BracketTracker()
for leaf in node.leaves():
bt.mark(leaf)
if bt.depth == 0:
continue
while container is not None:","def show(cls, code: Union[str, Leaf, Node]) -> None:
if isinstance(code, str):
code = lib2to3_parse(code)
list(v.visit(code))
def line(self, indent: int = 0) -> Iterator[Line]:
self.current_line.depth += indent
self.current_line = Line(depth=complete_line.depth + indent)
for comment in generate_comments(node):
if any_open_brackets:
# any comment within brackets is subject to splitting
self.current_line.append(comment)
elif comment.type == token.COMMENT:
# regular trailing comment
self.current_line.append(comment)
yield from self.line()
else:
# regular standalone comment
yield from self.line()
self.current_line.append(comment)
yield from self.line()
normalize_prefix(node, inside_brackets=any_open_brackets)
if self.normalize_strings and node.type == token.STRING:
normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)
normalize_string_quotes(node)
if node.type not in WHITESPACE:
self.current_line.append(node)
def visit_STANDALONE_COMMENT(self, leaf: Leaf) -> Iterator[Line]:
if not self.current_line.bracket_tracker.any_open_brackets():
yield from self.line()
yield from self.visit_default(leaf)
if parent.type == syms.file_input:
break
if line.is_comment:
""""""Convert content between `# fmt: off`/`# fmt: on` into standalone comments.""""""
try_again = convert_one_fmt_off_pair(node)
""""""Convert content of a single `# fmt: off`/`# fmt: on` into a standalone comment.
Returns True if a pair was converted.
""""""
for leaf in node.leaves():
# We only want standalone comments. If there's no previous leaf or
# the previous leaf is indentation, it's a standalone comment in
# disguise.
if comment.type != STANDALONE_COMMENT:
prev = preceding_leaf(leaf)
if prev and prev.type not in WHITESPACE:
continue
if hidden_value.endswith(""\n""):
# That happens when one of the `ignored_nodes` ended with a NEWLINE
# leaf (possibly followed by a DEDENT).
hidden_value = hidden_value[:-1]
""""""Starting from the container of `leaf`, generate all leaves until `# fmt: on`.
Stops at the end of the block.
""""""
while container is not None and container.type != token.ENDMARKER:"
black,16,"normalized_path = ""/"" + child.resolve().relative_to(root).as_posix()","Symbolic links pointing outside of the root directory are ignored.
try:
normalized_path = ""/"" + child.resolve().relative_to(root).as_posix()
except ValueError:
if child.is_symlink():
report.path_ignored(
child,
""is a symbolic link that points outside of the root directory"",
)
continue
raise"
black,17,"if src_txt[-1] != ""\n"":","if not lines:
return """", encoding, ""\n""
if src_txt[-1:] != ""\n"":"
black,18,"with tokenize.open(src) as src_buffer:
src_contents = src_buffer.read()
with open(src, ""w"", encoding=src_buffer.encoding) as f:
sys.stdout.write(diff_contents)
src = sys.stdin.read()
sys.stdout.write(dst)
sys.stdout.write(diff(src, dst, src_name, dst_name))
nl = ""\r\n"" if ""\r\n"" in src_txt[:1024] else ""\n""
src_txt += nl","with open(src, ""rb"") as buf:
newline, encoding, src_contents = prepare_input(buf.read())
with open(src, ""w"", encoding=encoding, newline=newline) as f:
f = io.TextIOWrapper(
sys.stdout.buffer,
encoding=encoding,
newline=newline,
write_through=True,
)
f.write(diff_contents)
f.detach()
newline, encoding, src = prepare_input(sys.stdin.buffer.read())
f = io.TextIOWrapper(
sys.stdout.buffer,
encoding=encoding,
newline=newline,
write_through=True,
)
f.write(dst)
f.detach()
f = io.TextIOWrapper(
sys.stdout.buffer,
encoding=encoding,
newline=newline,
write_through=True,
)
f.write(diff(src, dst, src_name, dst_name))
f.detach()
""""""Analyze `src` and return a tuple of (newline, encoding, decoded_contents)
Where `newline` is either CRLF or LF, and `decoded_contents` is decoded with
universal newlines (i.e. only LF).
""""""
srcbuf = io.BytesIO(src)
encoding, lines = tokenize.detect_encoding(srcbuf.readline)
newline = ""\r\n"" if b""\r\n"" == lines[0][-2:] else ""\n""
srcbuf.seek(0)
return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()
src_txt += ""\n"""
black,19,,
black,2,"is_fmt_on = False
for comment in list_comments(container.prefix, is_endmarker=False):
if comment.value in FMT_ON:
is_fmt_on = True
elif comment.value in FMT_OFF:
is_fmt_on = False
if is_fmt_on:
yield container
container = container.next_sibling","if fmt_on(container):
# fix for fmt: on in children
if contains_fmt_on_at_column(container, leaf.column):
for child in container.children:
if contains_fmt_on_at_column(child, leaf.column):
return
yield child
else:
yield container
container = container.next_sibling
is_fmt_on = False
for comment in list_comments(container.prefix, is_endmarker=False):
if comment.value in FMT_ON:
is_fmt_on = True
elif comment.value in FMT_OFF:
is_fmt_on = False
return is_fmt_on
for child in container.children:
if (
isinstance(child, Node)
and first_leaf_column(child) == column
or isinstance(child, Leaf)
and child.column == column
):
if fmt_on(child):
return True
return False
for child in node.children:
if isinstance(child, Leaf):
return child.column
return None"
black,20,"src_name = f""{src.name}  (original)""
dst_name = f""{src.name}  (formatted)""","src_name = f""{src}  (original)""
dst_name = f""{src}  (formatted)"""
black,21,"mode=""w"", prefix=""blk_"", suffix="".log"", delete=False","mode=""w"", prefix=""blk_"", suffix="".log"", delete=False, encoding=""utf8"""
black,22,"Dict, Generic, Iterable, Iterator, List, Optional, Set, Tuple, Type, TypeVar, Union
comments: Dict[LeafID, Leaf] = Factory(dict)
if self.maybe_adapt_standalone_comment(leaf):
return
return bool(self) and self.leaves[0].type == STANDALONE_COMMENT
self.leaves.pop()
self.leaves.pop()
self.leaves.pop()
def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:
""""""Hack a standalone comment to act as a trailing comment for line splitting.
If this line has brackets and a standalone `comment`, we need to adapt
it to be able to still reformat the line.
This is not perfect, the line to which the standalone comment gets
appended will appear ""too long"" when splitting.
""""""
if not (
comment.type = token.COMMENT
comment.prefix = '\n' + '    ' * (self.depth + 1)
return self.append_comment(comment)
def append_comment(self, comment: Leaf) -> bool:
""""""Add an inline comment to the line.""""""
try:
after = id(self.last_non_delimiter())
except LookupError:
if after in self.comments:
self.comments[after].value += str(comment)
else:
self.comments[after] = comment
def last_non_delimiter(self) -> Leaf:
""""""Return the last non-delimiter on the line. Raise LookupError otherwise.""""""
for i in range(len(self.leaves)):
last = self.leaves[-i - 1]
if not is_delimiter(last):
return last
raise LookupError(""No non-delimiters found"")
for comment in self.comments.values():
def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:
""""""Does nothing and returns False.""""""
return False
if isinstance(line, UnformattedLines):
if len(line_str) <= line_length and '\n' not in line_str:
split_funcs = [delimiter_split]
if '\n' not in line_str:
# Only attempt RHS if we don't have multiline strings or comments
# on this line.
split_funcs.append(right_hand_split)
for l in split_func(line, py36=py36):
comment_after = line.comments.get(id(leaf))
if comment_after:
comment_after = line.comments.get(id(leaf))
if comment_after:
This kind of split doesn't increase indentation.
current_line.append(leaf, preformatted=True)
comment_after = line.comments.get(id(leaf))
if comment_after:
current_line.append(comment_after, preformatted=True)
normalize_prefix(current_line.leaves[0], inside_brackets=True)
normalize_prefix(current_line.leaves[0], inside_brackets=True)","Callable,
Dict,
Generic,
Iterable,
Iterator,
List,
Optional,
Set,
Tuple,
Type,
TypeVar,
Union,
comments: List[Tuple[Index, Leaf]] = Factory(list)
def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:
""""""Like :func:`append()` but disallow invalid standalone comment structure.
Raises ValueError when any `leaf` is appended after a standalone comment
or when a standalone comment is not the first leaf on the line.
""""""
if self.bracket_tracker.depth == 0:
if self.is_comment:
raise ValueError(""cannot append to standalone comments"")
if self.leaves and leaf.type == STANDALONE_COMMENT:
raise ValueError(
""cannot append standalone comments to a populated line""
)
self.append(leaf, preformatted=preformatted)
return len(self.leaves) == 1 and self.leaves[0].type == STANDALONE_COMMENT
@property
def contains_standalone_comments(self) -> bool:
""""""If so, needs to be split before emitting.""""""
for leaf in self.leaves:
if leaf.type == STANDALONE_COMMENT:
return True
return False
self.remove_trailing_comma()
self.remove_trailing_comma()
self.remove_trailing_comma()
def append_comment(self, comment: Leaf) -> bool:
""""""Add an inline or standalone comment to the line.""""""
if (
comment.prefix = ''
after = len(self.leaves) - 1
if after == -1:
self.comments.append((after, comment))
def comments_after(self, leaf: Leaf) -> Iterator[Leaf]:
""""""Generate comments that should appear directly after `leaf`.""""""
for _leaf_index, _leaf in enumerate(self.leaves):
if leaf is _leaf:
break
else:
return
for index, comment_after in self.comments:
if _leaf_index == index:
yield comment_after
def remove_trailing_comma(self) -> None:
""""""Remove the trailing comma and moves the comments attached to it.""""""
comma_index = len(self.leaves) - 1
for i in range(len(self.comments)):
comment_index, comment = self.comments[i]
if comment_index == comma_index:
self.comments[i] = (comma_index - 1, comment)
self.leaves.pop()
for _, comment in self.comments:
if isinstance(line, UnformattedLines) or line.is_comment:
if (
len(line_str) <= line_length
and '\n' not in line_str  # multiline strings
and not line.contains_standalone_comments
):
split_funcs: List[SplitFunc]
split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]
for l in split_func(line, py36):
for comment_after in line.comments_after(leaf):
for comment_after in line.comments_after(leaf):
""""""Normalize prefix of the first leaf in every line returned by `split_func`.
This is a decorator over relevant split functions.
""""""
@wraps(split_func)
def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:
for l in split_func(line, py36):
normalize_prefix(l.leaves[0], inside_brackets=True)
yield l
return split_wrapper
def append_to_line(leaf: Leaf) -> Iterator[Line]:
""""""Append `leaf` to current line or to new line if appending impossible.""""""
nonlocal current_line
try:
current_line.append_safe(leaf, preformatted=True)
except ValueError as ve:
yield current_line
current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
current_line.append(leaf)
yield from append_to_line(leaf)
for comment_after in line.comments_after(leaf):
yield from append_to_line(comment_after)
yield current_line
""""""Split standalone comments from the rest of the line.""""""
for leaf in line.leaves:
if leaf.type == STANDALONE_COMMENT:
if leaf.bracket_depth == 0:
break
else:
raise CannotSplit(""Line does not have any standalone comments"")
current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
def append_to_line(leaf: Leaf) -> Iterator[Line]:
""""""Append `leaf` to current line or to new line if appending impossible.""""""
nonlocal current_line
try:
current_line.append_safe(leaf, preformatted=True)
except ValueError as ve:
yield current_line
current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
current_line.append(leaf)
for leaf in line.leaves:
yield from append_to_line(leaf)
for comment_after in line.comments_after(leaf):
yield from append_to_line(comment_after)
if current_line:"
black,23,"drv = driver.Driver(grammar, pytree.convert)
try:
result = drv.parse_string(src_txt, True)
except ParseError as pe:
lineno, column = pe.context[1]
lines = src_txt.splitlines()
faulty_line = lines[lineno - 1]
except IndexError:
faulty_line = ""<line number missing in source>""
raise ValueError(f""Cannot parse: {lineno}:{column}: {faulty_line}"") from None
raise AssertionError(f""cannot parse source: {exc}"") from None","pygram.python_grammar_no_print_statement_no_exec_statement,
pygram.python_grammar_no_print_statement,
pygram.python_grammar_no_exec_statement,
pygram.python_grammar,
for grammar in GRAMMARS:
drv = driver.Driver(grammar, pytree.convert)
result = drv.parse_string(src_txt, True)
break
except ParseError as pe:
lineno, column = pe.context[1]
lines = src_txt.splitlines()
try:
faulty_line = lines[lineno - 1]
except IndexError:
faulty_line = ""<line number missing in source>""
exc = ValueError(f""Cannot parse: {lineno}:{column}: {faulty_line}"")
else:
raise exc from None
elif (
prevp.type == token.RIGHTSHIFT
and prevp.parent
and prevp.parent.type == syms.shift_expr
and prevp.prev_sibling
and prevp.prev_sibling.type == token.NAME
and prevp.prev_sibling.value == 'print'
):
# Python 2 print chevron
return NO
major, minor = sys.version_info[:2]
raise AssertionError(
f""cannot use --safe with this file; failed to parse source file ""
f""with Python {major}.{minor}'s builtin AST. Re-run with --fast ""
f""or stop using deprecated Python 2 syntax. AST error message: {exc}""
)
exec(""new-style exec"", {}, {})
exec(""new-style exec"", {}, {})"
black,3,"exists=False, file_okay=True, dir_okay=False, readable=True, allow_dash=False","exists=True, file_okay=True, dir_okay=False, readable=True, allow_dash=False"
black,4,before -= self.previous_after,"before = (
# Black should not insert empty lines at the beginning
# of the file
0
if self.previous_line is None
else before - self.previous_after
)"
black,5,"if leaf.parent and leaf.parent.type == syms.arglist:
# Ensure a trailing comma for imports, but be careful not to add one after
# any comments.
if original.is_import:","if leaf.parent and leaf.parent.type in {
syms.arglist,
syms.typedargslist,
}:
# Ensure a trailing comma for imports and standalone function arguments, but
# be careful not to add one after any comments.
no_commas = original.is_def and not any(
l.type == token.COMMA for l in leaves
)
if original.is_import or no_commas:"
black,6,"TargetVersion.PY27: set(),
TargetVersion.PY33: {Feature.UNICODE_LITERALS},
TargetVersion.PY34: {Feature.UNICODE_LITERALS},
TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},
pygram.python_grammar_no_print_statement_no_exec_statement,
pygram.python_grammar_no_print_statement,
pygram.python_grammar,
return [pygram.python_grammar_no_print_statement, pygram.python_grammar]
return [pygram.python_grammar_no_print_statement_no_exec_statement]
for grammar in get_grammars(set(target_versions)):
drv = driver.Driver(grammar, pytree.convert)
def __init__(self, grammar, convert=None, logger=None):
tokens = tokenize.generate_tokens(stream.readline)
tokens = tokenize.generate_tokens(io.StringIO(text).readline)
if async_def:","# The following two feature-flags are mutually exclusive, and exactly one should be
# set for every version of python.
ASYNC_IS_VALID_IDENTIFIER = 6
ASYNC_IS_RESERVED_KEYWORD = 7
TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
TargetVersion.PY35: {
Feature.UNICODE_LITERALS,
Feature.TRAILING_COMMA_IN_CALL,
Feature.ASYNC_IS_VALID_IDENTIFIER,
},
Feature.ASYNC_IS_VALID_IDENTIFIER,
Feature.ASYNC_IS_RESERVED_KEYWORD,
Feature.ASYNC_IS_RESERVED_KEYWORD,
grammar: Grammar
tokenizer_config: TokenizerConfig = TokenizerConfig()
# Python 3.7+
ParserConfig(
pygram.python_grammar_no_print_statement_no_exec_statement,
TokenizerConfig(async_is_reserved_keyword=True),
),
# Python 3.0-3.6
ParserConfig(
pygram.python_grammar_no_print_statement_no_exec_statement,
TokenizerConfig(async_is_reserved_keyword=False),
),
# Python 2.7 with future print_function import
ParserConfig(pygram.python_grammar_no_print_statement),
# Python 2.7
ParserConfig(pygram.python_grammar),
return [
# Python 2.7 with future print_function import
ParserConfig(pygram.python_grammar_no_print_statement),
# Python 2.7
ParserConfig(pygram.python_grammar),
]
configs = []
# If we have to parse both, try to parse async as a keyword first
if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
# Python 3.7+
configs.append(
ParserConfig(
pygram.python_grammar_no_print_statement_no_exec_statement,
TokenizerConfig(async_is_reserved_keyword=True),
)
)
if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
# Python 3.0-3.6
configs.append(
ParserConfig(
pygram.python_grammar_no_print_statement_no_exec_statement,
TokenizerConfig(async_is_reserved_keyword=False),
)
)
# At least one of the above branches must have been taken, because every Python
# version has exactly one of the two 'ASYNC_IS_*' flags
return configs
for parser_config in get_parser_configs(set(target_versions)):
drv = driver.Driver(
parser_config.grammar,
pytree.convert,
tokenizer_config=parser_config.tokenizer_config,
)
def __init__(
self,
grammar,
convert=None,
logger=None,
tokenizer_config=tokenize.TokenizerConfig(),
):
self.tokenizer_config = tokenizer_config
tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
tokens = tokenize.generate_tokens(
io.StringIO(text).readline,
config=self.tokenizer_config,
)
async_is_reserved_keyword: bool = False
# If we know we're parsing 3.7+, we can unconditionally parse `async` and
# `await` as keywords.
async_is_reserved_keyword = config.async_is_reserved_keyword
if async_is_reserved_keyword or async_def:
return (await awaitable for awaitable in awaitable_list)
return (i * 2 for i in range(n) if await wrap(i))
return (await awaitable for awaitable in awaitable_list)
return (i * 2 for i in range(n) if await wrap(i))"
black,7,"node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))","# Add parentheses around long tuple unpacking in assignments.
if (
index == 0
and isinstance(child, Node)
and child.type == syms.testlist_star_expr
):
check_lpar = True
prefix = child.prefix
child.prefix = """"
new_child = Node(syms.atom, [lpar, child, rpar])
new_child.prefix = prefix
node.insert_child(index, new_child)"
black,8,"# Ensure a trailing comma when expected.
if leaves[-1].type != token.COMMA:
leaves.append(Leaf(token.COMMA, "",""))","# Ensure a trailing comma for imports, but be careful not to add one after
# any comments.
for i in range(len(leaves) - 1, -1, -1):
if leaves[i].type == STANDALONE_COMMENT:
continue
elif leaves[i].type == token.COMMA:
break
else:
leaves.insert(i + 1, Leaf(token.COMMA, "",""))
break"
black,9,"# Python 2-compatible code, so don't try Python 3 grammar.
return [pygram.python_grammar]","# Python 3-compatible code, so don't try Python 2 grammar
return [pygram.python_grammar_no_print_statement, pygram.python_grammar]"
cookiecutter,1,with open(context_file) as file_handle:,"with open(context_file, encoding='utf-8') as file_handle:"
cookiecutter,2,"return os.path.abspath(os.path.join(hooks_dir, hook_file))
return None
script = find_hook(hook_name)
if script is None:
run_script_with_context(script, project_dir, context)","scripts = []
scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))
if len(scripts) == 0:
return None
return scripts
scripts = find_hook(hook_name)
if not scripts:
for script in scripts:
run_script_with_context(script, project_dir, context)"
cookiecutter,3,"prompt, type=click.Choice(choices), default=default","prompt, type=click.Choice(choices), default=default, show_choices=False"
cookiecutter,4,"if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:
return proc.wait()
return run_script(temp.name, cwd)
return EXIT_SUCCESS
return run_script_with_context(script, project_dir, context)","""""""
Raised when a hook script fails
""""""
FailedHookException,
try:
run_hook('pre_gen_project', project_dir, context)
except FailedHookException:
shutil.rmtree(project_dir, ignore_errors=True)
exit_status = proc.wait()
if exit_status != EXIT_SUCCESS:
raise FailedHookException(
""Hook script failed (exit status: %d)"" % exit_status)
run_script(temp.name, cwd)
return
run_script_with_context(script, project_dir, context)"
fastapi,1,"include_none: bool = True,
include_none=include_none,
and (value is not None or include_none)
include_none=include_none,
include_none=include_none,
include_none=include_none,
include_none=include_none,
include_none=False,
return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)
res: Any, *, by_alias: bool = True, exclude_unset: bool
return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)
by_alias=by_alias, skip_defaults=exclude_unset
_prepare_response_content(item, exclude_unset=exclude_unset) for item in res
k: _prepare_response_content(v, exclude_unset=exclude_unset)
response_content, by_alias=by_alias, exclude_unset=exclude_unset
return ModelSubclass(sub={}, y=1)","response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
exclude_defaults: bool = False,
exclude_none: bool = False,
exclude_none=exclude_none,
exclude_defaults=exclude_defaults,
if exclude_defaults:
raise ValueError(""Cannot use exclude_defaults"")
exclude_none=exclude_none,
exclude_defaults=exclude_defaults,
and (value is not None or not exclude_none)
exclude_none=exclude_none,
exclude_none=exclude_none,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
exclude_none=True,
return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)
res: Any,
*,
by_alias: bool = True,
exclude_unset: bool,
exclude_defaults: bool = False,
exclude_none: bool = False,
return res.dict(
by_alias=by_alias,
exclude_unset=exclude_unset,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
)
by_alias=by_alias, skip_defaults=exclude_unset,
_prepare_response_content(
item,
exclude_unset=exclude_unset,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
)
for item in res
k: _prepare_response_content(
v,
exclude_unset=exclude_unset,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
)
exclude_defaults: bool = False,
exclude_none: bool = False,
response_content,
by_alias=by_alias,
exclude_unset=exclude_unset,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
exclude_defaults=exclude_defaults,
exclude_none=exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
exclude_defaults=response_model_exclude_defaults,
exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
self.response_model_exclude_defaults = response_model_exclude_defaults
self.response_model_exclude_none = response_model_exclude_none
response_model_exclude_defaults=self.response_model_exclude_defaults,
response_model_exclude_none=self.response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults=route.response_model_exclude_defaults,
response_model_exclude_none=route.response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
response_model_exclude_defaults: bool = False,
response_model_exclude_none: bool = False,
response_model_exclude_defaults=response_model_exclude_defaults,
response_model_exclude_none=response_model_exclude_none,
z: int = 0
w: int = None
w: Optional[str] = None
x: Optional[str] = None
y: str = ""y""
z: str = ""z""
return ModelSubclass(sub={}, y=1, z=0)
""/exclude_unset"", response_model=ModelDefaults, response_model_exclude_unset=True
return ModelDefaults(x=None, y=""y"")
""/exclude_defaults"",
response_model=ModelDefaults,
response_model_exclude_defaults=True,
return ModelDefaults(x=None, y=""y"")
""/exclude_none"", response_model=ModelDefaults, response_model_exclude_none=True
return ModelDefaults(x=None, y=""y"")
""/exclude_unset_none"",
response_model=ModelDefaults,
response_model_exclude_unset=True,
response_model_exclude_none=True,
return ModelDefaults(x=None, y=""y"")
response = client.get(""/exclude_unset"")
assert response.json() == {""x"": None, ""y"": ""y""}
response = client.get(""/exclude_defaults"")
assert response.json() == {}
response = client.get(""/exclude_none"")
assert response.json() == {""y"": ""y"", ""z"": ""z""}
response = client.get(""/exclude_unset_none"")
assert response.json() == {""y"": ""y""}"
fastapi,10,,
fastapi,11,"return (
)","if not (
):
return False
if field.sub_fields:
if not all(is_scalar_field(f) for f in field.sub_fields):
return False
return True"
fastapi,12,"raise HTTPException(
status_code=HTTP_403_FORBIDDEN,
detail=""Invalid authentication credentials"",
)","if self.auto_error:
raise HTTPException(
status_code=HTTP_403_FORBIDDEN,
detail=""Invalid authentication credentials"",
)
else:
return None"
fastapi,13,"if responses is None:
responses = {}
responses = {**responses, **route.responses}
responses=responses,","if responses is None:
responses = {}
combined_responses = {**responses, **route.responses}
responses=combined_responses,"
fastapi,14,"additionalProperties: Optional[Union[bool, Any]] = None
additionalProperties: Optional[Union[bool, SchemaBase]] = None
responses: Union[Responses, Dict[Union[str], Response]]","additionalProperties: Optional[Union[Dict[str, Any], bool]] = None
additionalProperties: Optional[Union[SchemaBase, bool]] = None
responses: Union[Responses, Dict[str, Response]]"
fastapi,15,,
fastapi,16,"if not obj.Config.json_encoders:
return jsonable_encoder(
obj.dict(include=include, exclude=exclude, by_alias=by_alias),
include_none=include_none,
)
else:
return jsonable_encoder(
obj.dict(include=include, exclude=exclude, by_alias=by_alias),
include_none=include_none,
custom_encoder=obj.Config.json_encoders,
)","encoder = getattr(obj.Config, ""json_encoders"", custom_encoder)
return jsonable_encoder(
obj.dict(include=include, exclude=exclude, by_alias=by_alias),
include_none=include_none,
custom_encoder=encoder,
)"
fastapi,2,"route = APIWebSocketRoute(path, endpoint=endpoint, name=name)","route = APIWebSocketRoute(
path,
endpoint=endpoint,
name=name,
dependency_overrides_provider=self.dependency_overrides_provider,
)"
fastapi,3,"if exclude_unset and isinstance(response_content, BaseModel):
if PYDANTIC_1:
response_content = response_content.dict(exclude_unset=exclude_unset)
else:
response_content = response_content.dict(
skip_defaults=exclude_unset
)  # pragma: nocover","res: Any, *, by_alias: bool = True, exclude_unset: bool
if isinstance(res, BaseModel):
if PYDANTIC_1:
return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)
else:
return res.dict(
by_alias=by_alias, skip_defaults=exclude_unset
)  # pragma: nocover
elif isinstance(res, list):
return [
_prepare_response_content(item, exclude_unset=exclude_unset) for item in res
]
elif isinstance(res, dict):
return {
k: _prepare_response_content(v, exclude_unset=exclude_unset)
for k, v in res.items()
}
return res
response_content = _prepare_response_content(
response_content, by_alias=by_alias, exclude_unset=exclude_unset
)"
fastapi,4,"operation[""parameters""] = parameters","operation[""parameters""] = list(
{param[""name""]: param for param in parameters}.values()
)"
fastapi,5,use_type.__fields__[f.name] = f,use_type.__fields__[f.name] = create_cloned_field(f)
fastapi,6,"if field.shape in sequence_shapes and isinstance(
received_body, FormData
):","if (
field.shape in sequence_shapes or field.type_ in sequence_types
) and isinstance(received_body, FormData):"
fastapi,7,"status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={""detail"": exc.errors()}","status_code=HTTP_422_UNPROCESSABLE_ENTITY,
content={""detail"": jsonable_encoder(exc.errors())},"
fastapi,8,route = self.route_class(,"route_class_override: Optional[Type[APIRoute]] = None,
route_class = route_class_override or self.route_class
route = route_class(
route_class_override=type(route),"
fastapi,9,"schema=BodySchema(None),","BodySchema_kwargs: Dict[str, Any] = dict(default=None)
body_param_media_types = [
getattr(f.schema, ""media_type"")
for f in flat_dependant.body_params
if isinstance(f.schema, params.Body)
]
if len(set(body_param_media_types)) == 1:
BodySchema_kwargs[""media_type""] = body_param_media_types[0]
schema=BodySchema(**BodySchema_kwargs),"
httpie,1,"if not exists(filename + suffix):
return filename + suffix","if len(filename) > max_len:
trim_by = len(filename) - max_len
name, ext = os.path.splitext(filename)
if trim_by >= len(name):
filename = filename[:-trim_by]
else:
filename = name[:-trim_by] + ext
return filename
try:
max_len = os.pathconf(directory, 'PC_NAME_MAX')
except OSError as e:
if e.errno == errno.EINVAL:
max_len = 255
else:
raise
return max_len
max_len = get_filename_max_length(directory) - extra
if len(filename) > max_len:
filename = trim_filename(filename, max_len)
return filename
try_filename = trim_filename_if_needed(filename, extra=len(suffix))
try_filename += suffix
if not exists(try_filename):
return try_filename"
httpie,2,"#       Network errors vs. bugs, etc.",requests_session.max_redirects = args.max_redirects
httpie,3,,
httpie,4,if 'Host' not in headers:,if 'Host' not in self._orig.headers:
httpie,5,"regex = '[^\\\\]' + sep
match = re.search(regex, string)
if match:
found[match.start() + 1] = sep","self.escapes = ['\\\\' + sep for sep in separators]
found_escapes = []
for esc in self.escapes:
found_escapes += [m.span() for m in re.finditer(esc, string)]
matches = re.finditer(sep, string)
for match in matches:
start, end = match.span()
inside_escape = False
for estart, eend in found_escapes:
if start >= estart and end <= eend:
inside_escape = True
break
if not inside_escape:
found[start] = sep"
keras,1,"return tf_state_ops.assign(x, new_x)
return tf_state_ops.assign_add(x, increment)
return tf_state_ops.assign_sub(x, decrement)
# TODO
return tf.Print(x, [x], message)
""""""Print the message and the tensor when evaluated and return the same
tensor.
return K.random_normal(shape, self.mean, self.stddev,
dtype=dtype, seed=self.seed)
return K.random_uniform(shape, self.minval, self.maxval,
dtype=dtype, seed=self.seed)
return K.truncated_normal(shape, self.mean, self.stddev,
dtype=dtype, seed=self.seed)
return K.truncated_normal(shape, 0., stddev,
dtype=dtype, seed=self.seed)
return K.random_uniform(shape, -limit, limit,
dtype=dtype, seed=self.seed)
def test_print_tensor(self):
check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)
check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)
x = np.random.randn(3, 4)
increment = np.random.randn(3, 4)
x += increment
K.eval(K.update_add(x_var, increment))
assert_allclose(x, K.eval(x_var), atol=1e-05)
x = np.random.randn(3, 4)
decrement = np.random.randn(3, 4)
x -= decrement
K.eval(K.update_sub(x_var, decrement))
assert_allclose(x, K.eval(x_var), atol=1e-05)
@pytest.mark.skipif(K.backend() != 'tensorflow',
# test standard normal as well as a normal with a different set of parameters
rand = K.eval(K.random_normal((300, 200),
mean=mean, stddev=std, seed=1337))
assert rand.shape == (300, 200)
# test that random_normal also generates different values when used
# within a function
r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)
samples = np.array([K.eval(r) for _ in range(200)])
assert np.abs(np.mean(samples) - mean) < std * 0.015
assert np.abs(np.std(samples) - std) < std * 0.015
rand = K.eval(K.random_uniform((200, 100), min_val, max_val))
assert rand.shape == (200, 100)
r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)
samples = np.array([K.eval(r) for _ in range(200)])
assert np.abs(np.mean(samples)) < 0.015
assert max_val - 0.015 < np.max(samples) <= max_val
assert min_val + 0.015 > np.min(samples) >= min_val
rand = K.eval(K.random_binomial((200, 100), p))
assert rand.shape == (200, 100)
r = K.random_binomial((10, 10), p)
samples = np.array([K.eval(r) for _ in range(200)])
assert np.abs(np.mean(samples) - p) < 0.015
assert np.max(samples) == 1
assert np.min(samples) == 0
rand = K.eval(K.truncated_normal((300, 200),
mean=mean, stddev=std, seed=1337))
assert rand.shape == (300, 200)
@pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),
reason='This test is for tensorflow parallelism.')
def test_tensorflow_session_parallelism_settings(self, monkeypatch):
for threads in [1, 2]:
K.clear_session()
monkeypatch.setenv('OMP_NUM_THREADS', str(threads))
cfg = K.get_session()._config
assert cfg.intra_op_parallelism_threads == threads
assert cfg.inter_op_parallelism_threads == threads","op = tf_state_ops.assign(x, new_x)
with tf.control_dependencies([op]):
return tf.identity(x)
op = tf_state_ops.assign_add(x, increment)
with tf.control_dependencies([op]):
return tf.identity(x)
op = tf_state_ops.assign_sub(x, decrement)
with tf.control_dependencies([op]):
return tf.identity(x)
op = tf.print(message, x, output_stream=sys.stdout)
with tf.control_dependencies([op]):
return tf.identity(x)
""""""Print the message & the tensor when evaluated & return the same tensor.
x = K.random_normal(shape, self.mean, self.stddev,
dtype=dtype, seed=self.seed)
if self.seed is not None:
self.seed += 1
return x
x = K.random_uniform(shape, self.minval, self.maxval,
dtype=dtype, seed=self.seed)
if self.seed is not None:
self.seed += 1
return x
x = K.truncated_normal(shape, self.mean, self.stddev,
dtype=dtype, seed=self.seed)
if self.seed is not None:
self.seed += 1
return x
x = K.truncated_normal(shape, 0., stddev,
dtype=dtype, seed=self.seed)
x = K.random_uniform(shape, -limit, limit,
dtype=dtype, seed=self.seed)
if self.seed is not None:
self.seed += 1
return x
self.seed += 1
def test_print_tensor(self, capsys):
for k in [KTH, KTF]:
x = k.placeholder((1, 1))
y = k.print_tensor(x, 'msg')
fn = k.function([x], [y])
_ = fn([np.ones((1, 1))])
out, err = capsys.readouterr()
# Theano inserts ""__str__ = "" for no good reason
assert out.replace('__str__ = ', '') == 'msg [[1.]]\n'
@pytest.mark.skipif(K.backend() == 'theano',
reason='theano returns tuples for update ops')
def test_update(self):
x = np.ones((3, 4))
x_var = K.variable(x)
new_x = np.random.random((3, 4))
op = K.update(x_var, new_x)
K.eval(op)
assert_allclose(new_x, K.eval(x_var), atol=1e-05)
x = np.ones((3, 4))
increment = np.random.random((3, 4))
op = K.update_add(x_var, increment)
K.eval(op)
assert_allclose(x + increment, K.eval(x_var), atol=1e-05)
x = np.ones((3, 4))
decrement = np.random.random((3, 4))
op = K.update_sub(x_var, decrement)
K.eval(op)
assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)
@pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),
# TODO: make this a parameterized test
rand = K.eval(K.random_normal((200, 200),
mean=mean,
stddev=std))
assert rand.shape == (200, 200)
rand = K.eval(K.random_uniform((200, 200), min_val, max_val))
assert rand.shape == (200, 200)
rand = K.eval(K.random_binomial((200, 200), p))
assert rand.shape == (200, 200)
rand = K.eval(K.truncated_normal((200, 200),
mean=mean,
stddev=std))
assert rand.shape == (200, 200)"
keras,10,"weight array.
if sample_weight is not None and class_weight is not None:
warnings.warn('Found both `sample_weight` and `class_weight`: '
'`class_weight` argument will be ignored.')
return sample_weight
elif isinstance(class_weight, dict):
if y.shape[1] > 1:
y_classes = np.argmax(y, axis=1)
elif y.shape[1] == 1:
y_classes = np.reshape(y, y.shape[0])
weights = np.asarray([class_weight[cls] for cls in y_classes
if cls in class_weight])
if len(weights) != len(y_classes):
return weights
if sample_weight_mode is None:
return np.ones((y.shape[0],), dtype=K.floatx())
else:
return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())","weight array. If both `sample_weights` and `class_weights` are provided,
the weights are multiplied together.
class_sample_weight = None
if isinstance(class_weight, dict):
if len(y.shape) == 2:
if y.shape[1] > 1:
y_classes = np.argmax(y, axis=1)
elif y.shape[1] == 1:
y_classes = np.reshape(y, y.shape[0])
class_sample_weight = np.asarray(
[class_weight[cls] for cls in y_classes if cls in class_weight])
if len(class_sample_weight) != len(y_classes):
if sample_weight is not None and class_sample_weight is not None:
return sample_weight * class_sample_weight
if sample_weight is not None:
return sample_weight
if class_sample_weight is not None:
return class_sample_weight
# Everything has weight 1 by default.
if sample_weight_mode is None:
return np.ones((y.shape[0],), dtype=K.floatx())
return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())"
keras,11,"is_sequence = isinstance(generator, Sequence)
if not is_sequence and use_multiprocessing and workers > 1:
if is_sequence:
isinstance(validation_data, Sequence))
if (val_gen and not isinstance(validation_data, Sequence) and
if isinstance(val_data, Sequence):
if isinstance(val_data, Sequence):
if is_sequence:
if is_sequence:
is_sequence = isinstance(generator, Sequence)
if not is_sequence and use_multiprocessing and workers > 1:
if is_sequence:
if is_sequence:
if is_sequence:
is_sequence = isinstance(generator, Sequence)
if not is_sequence and use_multiprocessing and workers > 1:
if is_sequence:
if is_sequence:
if is_sequence:
K.backend() == 'tensorflow',
K.backend() == 'tensorflow',","use_sequence_api = is_sequence(generator)
if not use_sequence_api and use_multiprocessing and workers > 1:
if use_sequence_api:
val_use_sequence_api = is_sequence(validation_data)
val_use_sequence_api)
if (val_gen and not val_use_sequence_api and
if is_sequence(val_data):
if is_sequence(val_data):
if use_sequence_api:
if use_sequence_api:
use_sequence_api = is_sequence(generator)
if not use_sequence_api and use_multiprocessing and workers > 1:
if use_sequence_api:
if use_sequence_api:
if use_sequence_api:
use_sequence_api = is_sequence(generator)
if not use_sequence_api and use_multiprocessing and workers > 1:
if use_sequence_api:
if use_sequence_api:
if use_sequence_api:
""""""Determine if an object follows the Sequence API.
# Arguments
seq: a possible Sequence object
# Returns
boolean, whether the object follows the Sequence API.
""""""
# TODO Dref360: Decide which pattern to follow. First needs a new TF Version.
return (getattr(seq, 'use_sequence_api', False)
or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))
use_sequence_api = True
K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,
K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,"
keras,12,,
keras,13,val_enqueuer_gen = iter_sequence_infinite(generator),"val_enqueuer_gen = iter_sequence_infinite(val_data)
validation_steps = validation_steps or len(val_data)"
keras,14,"return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),","# If the shape of y_true is (num_samples, 1), flatten to (num_samples,)
return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),"
keras,15,"self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''
self.csv_file = open(self.filename, 'a' + self.file_flags)
self.csv_file = open(self.filename, 'w' + self.file_flags)
fieldnames=['epoch'] + self.keys, dialect=CustomDialect)","if six.PY2:
self.file_flags = 'b'
self._open_args = {}
else:
self.file_flags = ''
self._open_args = {'newline': '\n'}
mode = 'a'
mode = 'w'
self.csv_file = io.open(self.filename,
mode + self.file_flags,
**self._open_args)
fieldnames = ['epoch'] + self.keys
if six.PY2:
fieldnames = [unicode(x) for x in fieldnames]
fieldnames=fieldnames,
dialect=CustomDialect)"
keras,16,"if self._layers:
self._layers[0].batch_input_shape = batch_shape
config = []
config.append({
return copy.deepcopy(config)
model = cls()
for conf in config:","self._build_input_shape = None
self._build_input_shape = input_shape
layer_configs = []
layer_configs.append({
config = {
'name': self.name,
'layers': copy.deepcopy(layer_configs)
}
if self._build_input_shape:
config['build_input_shape'] = self._build_input_shape
return config
if 'name' in config:
name = config['name']
build_input_shape = config.get('build_input_shape')
layer_configs = config['layers']
model = cls(name=name)
for conf in layer_configs:
if not model.inputs and build_input_shape:
model.build(build_input_shape)"
keras,17,"return K.cast(K.equal(K.max(y_true, axis=-1),","# flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)
return K.cast(K.equal(K.flatten(y_true),"
keras,18,"self.session_kwargs = session_kwargs
fetched = self._callable_fn(*array_vals)","# self.session_kwargs is used for _legacy_call
self.session_kwargs = session_kwargs.copy()
self.run_options = session_kwargs.pop('options', None)
self.run_metadata = session_kwargs.pop('run_metadata', None)
# Handle run_options.
if self.run_options:
callable_opts.run_options.CopyFrom(self.run_options)
if self.run_metadata:
fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)
else:
fetched = self._callable_fn(*array_vals)
# callable generated by Session._make_callable_from_options accepts
# `run_metadata` keyword argument since TF 1.10
if (self.run_metadata and
StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):
if py_any(is_tensor(x) for x in inputs):
raise ValueError(
'In order to feed symbolic tensors to a Keras model and set '
'`run_metadata`, you need tensorflow 1.10 or higher.')
return self._legacy_call(inputs)"
keras,19,"new_output = n_s[0]
# States are a flat list
# in reverse order of the cell stack.
# This allows to preserve the requirement
# `stack.state_size[0] == output_dim`.
# e.g. states of a 2-layer LSTM would be
# `[h2, c2, h1, c1]`
for cell in self.cells[::-1]:
for cell in self.cells[::-1]:
nested_states = nested_states[::-1]
states = []
for cell_states in new_nested_states[::-1]:
states += cell_states
return inputs, states
if hasattr(cell.state_size, '__len__'):
(one size per state). In this case, the first entry
(`state_size[0]`) should be the same as
the size of the cell output.
output_dim = state_size[0]","new_output = n_s[-1]
# reverse_state_order determines whether the state size will be in a
# reverse order of the cells' state. User might want to set this to True
# to keep the existing behavior. This is only useful when use
# `RNN(return_state=True)` since the state will be returned as the same
# order of state_size.
self.reverse_state_order = kwargs.pop('reverse_state_order', False)
if self.reverse_state_order:
warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '
'will soon be deprecated. Please update the code to '
'work with the natural order of states if you '
'reply on the RNN states, '
'eg `RNN(return_state=True)`.')
# States are a flat list of the individual cell state size.
# e.g. states of a 2-layer LSTM would be `[h1, c1, h2, c2]`.
# In the case of reverse_state_order=True, the state_size will be
# `[h2, c2, h1, c1]`.
for cell in self.cells[::-1] if self.reverse_state_order else self.cells:
@property
def output_size(self):
if getattr(self.cells[-1], 'output_size', None) is not None:
return self.cells[-1].output_size
if hasattr(self.cells[-1].state_size, '__len__'):
return self.cells[-1].state_size[0]
else:
return self.cells[-1].state_size
for cell in self.cells[::-1] if self.reverse_state_order else self.cells:
if self.reverse_state_order:
nested_states = nested_states[::-1]
new_states = []
if self.reverse_state_order:
new_nested_states = new_nested_states[::-1]
for cell_states in new_nested_states:
new_states += cell_states
return inputs, new_states
if getattr(cell, 'output_size', None) is not None:
output_dim = cell.output_size
elif hasattr(cell.state_size, '__len__'):
(one size per state).
- a `output_size` attribute. This can be a single integer or a
TensorShape, which represent the shape of the output. For
backward compatible reason, if this attribute is not available
for the cell, the value will be inferred by the first element
of the `state_size`.
if getattr(self.cell, 'output_size', None) is not None:
output_dim = self.cell.output_size
else:
output_dim = state_size[0]
self.output_size = self.units
self.output_size = self.units
self.output_size = self.units"
keras,2,,
keras,20,"padding='valid', data_format=None):
output_shape=output_shape)
if not _has_nchw_support():
padding='valid', data_format=None):
x, tf_data_format = _preprocess_conv2d_input(x, data_format)
x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
padding=padding,
data_format=tf_data_format)
padding='valid', data_format=None):
in inputs/kernels/outputs.
filter_flip=not flip_filters)
out_pad_h)
out_pad_w)
data_format=self.data_format)
out_pad_h)
out_pad_w)
config.pop('dilation_rate')","padding='valid', data_format=None, dilation_rate=(1, 1)):
output_shape=output_shape,
dilation=dilation_rate)
force_transpose: boolean, whether force to transpose input from NCHW to NHWC
if the `data_format` is `""channels_first""`.
if not _has_nchw_support() or force_transpose:
padding='valid', data_format=None, dilation_rate=(1, 1)):
dilation_rate: tuple of 2 integers.
# tf.nn.atrous_conv2d_transpose input only supports NHWC format
if data_format == 'channels_first' and dilation_rate != (1, 1):
force_transpose = True
else:
force_transpose = False
x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)
if dilation_rate == (1, 1):
x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
padding=padding,
data_format=tf_data_format)
else:
assert dilation_rate[0] == dilation_rate[1]
x = tf.nn.atrous_conv2d_transpose(
x, kernel, output_shape, dilation_rate[0], padding)
padding='valid', data_format=None, dilation_rate=(1, 1)):
in inputs/kernels/outputs.
dilation_rate: tuple of 2 integers.
filter_flip=not flip_filters,
filter_dilation=dilation_rate)
dilation_rate=(1, 1),
dilation_rate=dilation_rate,
out_pad_h,
self.dilation_rate[0])
out_pad_w,
self.dilation_rate[1])
data_format=self.data_format,
dilation_rate=self.dilation_rate)
out_pad_h,
self.dilation_rate[0])
out_pad_w,
self.dilation_rate[1])
output_padding, dilation=1):
dilation: dilation rate, integer.
# Get the dilated kernel size
kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)"
keras,21,baseline=None):,"restore_best_weights: whether to restore model weights from
the epoch with the best value of the monitored quantity.
If False, the model weights obtained at the last step of
training are used.
baseline=None,
restore_best_weights=False):
self.restore_best_weights = restore_best_weights
self.best_weights = None
if self.restore_best_weights:
self.best_weights = self.model.get_weights()
if self.restore_best_weights:
if self.verbose > 0:
print(""Restoring model weights from the end of the best epoch"")
self.model.set_weights(self.best_weights)"
keras,22,,
keras,23,"batch_shape = first_layer.batch_input_shape
dtype = first_layer.dtype",
keras,24,"tf.summary.histogram('{}_out'.format(layer.name),
layer.output)","if isinstance(layer.output, list):
for i, output in enumerate(layer.output):
tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)
else:
tf.summary.histogram('{}_out'.format(layer.name),
layer.output)"
keras,25,,
keras,26,"new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]","new_states = [
tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),
new_states[i], states[i]) for i in range(len(states))
]"
keras,27,,
keras,28,"(self.end_index - self.start_index) /
self.start_index, self.end_index, size=self.batch_size)
self.stride, self.end_index), self.stride)","if self.start_index > self.end_index:
raise ValueError('`start_index+length=%i > end_index=%i` '
'is disallowed, as no part of the sequence '
'would be left to be used as current step.'
% (self.start_index, self.end_index))
(self.end_index - self.start_index + 1) /
self.start_index, self.end_index + 1, size=self.batch_size)
self.stride, self.end_index + 1), self.stride)"
keras,29,"for m in self.metrics:
if isinstance(m, Layer) and m.stateful:
m.reset_states()
for m in self.metrics:
if isinstance(m, Layer) and m.stateful:
m.reset_states()
for m in self.metrics:
if isinstance(m, Layer) and m.stateful:
m.reset_states()
for i, m in enumerate(self.metrics):
if isinstance(m, Layer) and m.stateful:
m.reset_states()","self.stateful_metric_functions = []
self.stateful_metric_functions.append(metric_fn)
for m in self.stateful_metric_functions:
m.reset_states()
for m in self.stateful_metric_functions:
m.reset_states()
for m in self.stateful_metric_functions:
m.reset_states()
for m in self.stateful_metric_functions:
m.reset_states()"
keras,3,"output_masks = to_list(
layer.compute_mask(computed_tensor,
computed_mask))
output_masks = to_list(
layer.compute_mask(computed_tensors,
computed_masks))","if layer.supports_masking:
output_masks = to_list(
layer.compute_mask(computed_tensor,
computed_mask))
else:
output_masks = [None] * len(output_tensors)
if layer.supports_masking:
output_masks = to_list(
layer.compute_mask(computed_tensors,
computed_masks))
else:
output_masks = [None] * len(output_tensors)"
keras,30,"if isinstance(x, list):
if isinstance(x, list):","if x is None or len(x) == 0:
# Handle data tensors support when no input given
# step-size = 1 for data tensors
batch_size = 1
elif isinstance(x, list):
if x is None or len(x) == 0:
# Handle data tensors support when no input given
# step-size = 1 for data tensors
batch_size = 1
elif isinstance(x, list):"
keras,31,"label_length = tf.to_int32(tf.squeeze(label_length))
input_length = tf.to_int32(tf.squeeze(input_length))","label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))
input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))"
keras,32,"epsilon: threshold for measuring the new optimum,
verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):
self.epsilon = epsilon
self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)
self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)
self.wait += 1","min_delta: threshold for measuring the new optimum,
verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,
**kwargs):
if 'epsilon' in kwargs:
min_delta = kwargs.pop('epsilon')
warnings.warn('`epsilon` argument is deprecated and '
'will be removed, use `min_delta` insted.')
self.min_delta = min_delta
self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
self.wait += 1"
keras,33,"if sys.version_info < (3,) and isinstance(text, unicode):
translate_map = dict((ord(c), unicode(split)) for c in filters)
translate_map = maketrans(filters, split * len(filters))
text = text.translate(translate_map)","if sys.version_info < (3,):
if isinstance(text, unicode):
translate_map = dict((ord(c), unicode(split)) for c in filters)
text = text.translate(translate_map)
elif len(split) == 1:
translate_map = maketrans(filters, split * len(filters))
text = text.translate(translate_map)
else:
for c in filters:
text = text.replace(c, split)
translate_dict = dict((c, split) for c in filters)
translate_map = maketrans(translate_dict)
text = text.translate(translate_map)"
keras,34,"validation_generator = validation_data
output_generator = generator
output_generator = generator
output_generator = generator","if isinstance(validation_data, Sequence):
validation_generator = iter(validation_data)
else:
validation_generator = validation_data
if is_sequence:
output_generator = iter(generator)
else:
output_generator = generator
if is_sequence:
output_generator = iter(generator)
else:
output_generator = generator
if is_sequence:
output_generator = iter(generator)
else:
output_generator = generator
def __iter__(self):
""""""Create an infinite generator that iterate over the Sequence.""""""
while True:
for item in (self[i] for i in range(len(self))):
yield item"
keras,35,"if self.preprocessing_function:
x = self.preprocessing_function(x)
target_size=self.target_size,","if self.image_data_generator.preprocessing_function:
x = self.image_data_generator.preprocessing_function(x)
target_size=None,
if self.image_data_generator.preprocessing_function:
img = self.image_data_generator.preprocessing_function(img)
if self.target_size is not None:
width_height_tuple = (self.target_size[1], self.target_size[0])
if img.size != width_height_tuple:
if self.interpolation not in _PIL_INTERPOLATION_METHODS:
raise ValueError(
'Invalid interpolation method {} specified. Supported '
'methods are {}'.format(
self.interpolation,
"", "".join(_PIL_INTERPOLATION_METHODS.keys())))
resample = _PIL_INTERPOLATION_METHODS[self.interpolation]
img = img.resize(width_height_tuple, resample)"
keras,36,"strides = (1, 1) + strides + (1,)
strides = (1, 1, 1) + strides","strides = (1,) + strides * 2 + (1,)
strides = (1, 1) + strides * 2"
keras,37,"is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')
if hasattr(tensor, '_keras_history') != is_keras_tensor:
' Keras tensors and non-Keras tensors')
if not isinstance(initial_state, list):
raise ValueError(
'When passing `initial_state` to a Bidirectional RNN, the state '
'should be a list containing the states of the underlying RNNs. '
'Found: ' + str(initial_state))","is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
if K.is_keras_tensor(tensor) != is_keras_tensor:
' Keras tensors and non-Keras tensors'
' (a ""Keras tensor"" is a tensor that was'
' returned by a Keras layer, or by `Input`)')
self.input_spec = layer.input_spec
def __call__(self, inputs, initial_state=None, **kwargs):
if isinstance(inputs, list):
if len(inputs) > 1:
initial_state = inputs[1:]
inputs = inputs[0]
if initial_state is None:
return super(Bidirectional, self).__call__(inputs, **kwargs)
# Standardize `initial_state` into list
if isinstance(initial_state, tuple):
initial_state = list(initial_state)
elif not isinstance(initial_state, list):
initial_state = [initial_state]
# Check if `initial_state` can be splitted into half
num_states = len(initial_state)
if num_states % 2 > 0:
raise ValueError(
'When passing `initial_state` to a Bidirectional RNN, the state '
'should be a list containing the states of the underlying RNNs. '
'Found: ' + str(initial_state))
# Applies the same workaround as in `RNN.__call__`, without handling constants
kwargs['initial_state'] = initial_state
additional_inputs = initial_state
additional_specs = [InputSpec(shape=K.int_shape(state))
for state in initial_state]
self.forward_layer.state_spec = additional_specs[:num_states // 2]
self.backward_layer.state_spec = additional_specs[num_states // 2:]
is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
for tensor in additional_inputs:
if K.is_keras_tensor(tensor) != is_keras_tensor:
raise ValueError('The initial state of a Bidirectional'
' layer cannot be specified with a mix of'
' Keras tensors and non-Keras tensors'
' (a ""Keras tensor"" is a tensor that was'
' returned by a Keras layer, or by `Input`)')
if is_keras_tensor:
# Compute the full input spec, including state
full_input = [inputs] + additional_inputs
full_input_spec = self.input_spec + additional_specs
# Perform the call with temporarily replaced input_spec
original_input_spec = self.input_spec
self.input_spec = full_input_spec
output = super(Bidirectional, self).__call__(full_input, **kwargs)
self.input_spec = original_input_spec
return output
else:
return super(Bidirectional, self).__call__(inputs, **kwargs)"
keras,38,"input_shape = (input_shape[0], input_shape[1], output_dim)","input_shape = (input_shape[0], output_dim)"
keras,39,current < self.target):,(self.target is not None and current < self.target)):
keras,4,"grads = self.optimizer.compute_gradients(loss, params)","grads = self.optimizer.compute_gradients(loss, var_list=params)"
keras,40,"output_dim = self.cell.state_size[0]
output_dim = self.cell.state_size
state_shape = [(input_shape[0], output_dim) for _ in self.states]","state_size = self.cell.state_size
state_size = [self.cell.state_size]
output_dim = state_size[0]
state_shape = [(input_shape[0], dim) for dim in state_size]"
keras,41,"raise StopIteration(e)
self.queue.put(generator_output)
except Exception:
raise
self.queue = multiprocessing.Queue(maxsize=max_queue_size)
if self._use_multiprocessing:
if self.queue is not None:
self.queue.close()
inputs = self.queue.get()
if inputs is not None:
yield inputs
np.random.randint(batch_size, 2, 50))
with pytest.raises(StopIteration):
with pytest.raises(StopIteration):
np.random.randint(batch_size, 2, 50))
with pytest.raises(StopIteration):
custom_generator(), good_batches + 1, 1,
workers=4, use_multiprocessing=True,
with pytest.raises(StopIteration):
with pytest.raises(StopIteration):
with pytest.raises(StopIteration):","six.raise_from(StopIteration(e), e)
self._manager = None
self.queue.put((True, generator_output))
except Exception as e:
# Can't pick tracebacks.
# As a compromise, print the traceback and pickle None instead.
if self._use_multiprocessing:
traceback.print_exc()
setattr(e, '__traceback__', None)
elif not hasattr(e, '__traceback__'):
setattr(e, '__traceback__', sys.exc_info()[2])
self.queue.put((False, e))
break
self._manager = multiprocessing.Manager()
self.queue = self._manager.Queue(maxsize=max_queue_size)
if self._manager:
self._manager.shutdown()
success, value = self.queue.get()
# Rethrow any exceptions found in the queue
if not success:
six.reraise(value.__class__, value, value.__traceback__)
# Yield regular values
if value is not None:
yield value
# Make sure to rethrow the first exception in the queue, if any
while not self.queue.empty():
success, value = self.queue.get()
if not success:
six.reraise(value.__class__, value, value.__traceback__)
np.random.randint(batch_size, 12, 50))
with pytest.raises(RuntimeError):
with pytest.raises(RuntimeError):
workers = 4
np.random.randint(batch_size, 12, 50))
with pytest.raises(RuntimeError):
custom_generator(), good_batches * workers + 1, 1,
workers=workers, use_multiprocessing=True,
with pytest.raises(RuntimeError):
with pytest.raises(RuntimeError):
with pytest.raises(RuntimeError):"
keras,42,"steps_per_epoch,
divided by the batch size. Not used if using `Sequence`.
if val_gen and not validation_steps:
raise ValueError('When using a generator for validation data, '
'you must specify a value for '
'`validation_steps`.')
is_sequence = isinstance(generator, Sequence)
if not is_sequence and use_multiprocessing and workers > 1:
warnings.warn(
UserWarning('Using a generator with `use_multiprocessing=True`'
' and multiple workers may duplicate your data.'
' Please consider using the`keras.utils.Sequence'
' class.'))
if is_sequence:
steps_per_epoch = len(generator)
def evaluate_generator(self, generator, steps,
Not used if using Sequence.
if is_sequence:
steps = len(generator)
def predict_generator(self, generator, steps,
Not used if using Sequence.
if is_sequence:
steps = len(generator)
steps_per_epoch,
def evaluate_generator(self, generator, steps,
def predict_generator(self, generator, steps,","steps_per_epoch=None,
divided by the batch size.
Optional for `Sequence`: if unspecified, will use
the `len(generator)` as a number of steps.
Optional for `Sequence`: if unspecified, will use
the `len(validation_data)` as a number of steps.
is_sequence = isinstance(generator, Sequence)
if not is_sequence and use_multiprocessing and workers > 1:
warnings.warn(
UserWarning('Using a generator with `use_multiprocessing=True`'
' and multiple workers may duplicate your data.'
' Please consider using the`keras.utils.Sequence'
' class.'))
if steps_per_epoch is None:
if is_sequence:
steps_per_epoch = len(generator)
else:
raise ValueError('`steps_per_epoch=None` is only valid for a'
' generator based on the `keras.utils.Sequence`'
' class. Please specify `steps_per_epoch` or use'
' the `keras.utils.Sequence` class.')
if (val_gen and not isinstance(validation_data, Sequence) and
not validation_steps):
raise ValueError('`validation_steps=None` is only valid for a'
' generator based on the `keras.utils.Sequence`'
' class. Please specify `validation_steps` or use'
' the `keras.utils.Sequence` class.')
def evaluate_generator(self, generator, steps=None,
Optional for `Sequence`: if unspecified, will use
the `len(generator)` as a number of steps.
if steps is None:
if is_sequence:
steps = len(generator)
else:
raise ValueError('`steps=None` is only valid for a generator'
' based on the `keras.utils.Sequence` class.'
' Please specify `steps` or use the'
' `keras.utils.Sequence` class.')
def predict_generator(self, generator, steps=None,
Optional for `Sequence`: if unspecified, will use
the `len(generator)` as a number of steps.
if steps is None:
if is_sequence:
steps = len(generator)
else:
raise ValueError('`steps=None` is only valid for a generator'
' based on the `keras.utils.Sequence` class.'
' Please specify `steps` or use the'
' `keras.utils.Sequence` class.')
steps_per_epoch=None,
Optional for `Sequence`: if unspecified, will use
the `len(generator)` as a number of steps.
Optional for `Sequence`: if unspecified, will use
the `len(validation_data)` as a number of steps.
def evaluate_generator(self, generator, steps=None,
Optional for `Sequence`: if unspecified, will use
the `len(generator)` as a number of steps.
def predict_generator(self, generator, steps=None,
Optional for `Sequence`: if unspecified, will use
the `len(generator)` as a number of steps."
keras,43,,
keras,44,,
keras,45,"x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i
x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f
x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c
x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o","x_i = K.dot(inputs_i, self.kernel_i)
x_f = K.dot(inputs_f, self.kernel_f)
x_c = K.dot(inputs_c, self.kernel_c)
x_o = K.dot(inputs_o, self.kernel_o)
if self.use_bias:
x_i = K.bias_add(x_i, self.bias_i)
x_f = K.bias_add(x_f, self.bias_f)
x_c = K.bias_add(x_c, self.bias_c)
x_o = K.bias_add(x_o, self.bias_o)"
keras,5,"cache_dir = os.path.join(os.path.expanduser('~'), '.keras')","if 'KERAS_HOME' in os.environ:
cache_dir = os.environ.get('KERAS_HOME')
else:
cache_dir = os.path.join(os.path.expanduser('~'), '.keras')"
keras,6,score_array /= K.mean(mask),score_array /= K.mean(mask) + K.epsilon()
keras,7,"return np.squeeze(self.model.predict(x, **kwargs))","return np.squeeze(self.model.predict(x, **kwargs), axis=-1)"
keras,8,"add_unprocessed_node(layer, node_data)
return
for node_data in unprocessed_nodes.pop(layer):
process_node(layer, node_data)","""""""Add node to layer list
Args:
layer: layer object
node_data: Node data specifying layer call
""""""
""""""Reconstruct node by linking to inbound layers
Args:
layer: Layer to process
node_data: List of layer configs
Raises:
ValueError: For incorrect layer config
LookupError: If layer required is not found
""""""
# Raise an error if the corresponding layer node
# has not yet been created
raise LookupError
# Process all nodes in layer, if not yet processed
node_data_list = unprocessed_nodes[layer]
# Process nodes in order
node_index = 0
while node_index < len(node_data_list):
node_data = node_data_list[node_index]
try:
process_node(layer, node_data)
# If the node does not have all inbound layers
# available, stop processing and continue later
except LookupError:
break
node_index += 1
# If not all nodes processed then store unprocessed nodes
if node_index < len(node_data_list):
unprocessed_nodes[layer] = node_data_list[node_index:]
# If all nodes processed remove the layer
else:
del unprocessed_nodes[layer]
# Create lits of input and output tensors and return new class"
keras,9,"block = docstring[starting_point:(None if ending_point == -1 else
ending_point - 1)]","block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else
section_end)]"
luigi,1,"metrics = self._scheduler._state._metrics_collector.generate_latest()
metrics.configure_http_handler(self)","metrics_collector = self._scheduler._state._metrics_collector
metrics = metrics_collector.generate_latest()
metrics_collector.configure_http_handler(self)"
luigi,10,return state.get_pending_tasks(),"return six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())"
luigi,11,"task.params.get(name) == value for name, value in unbatched_params.items())):","task.params.get(name) == value for name, value in unbatched_params.items()) and
self._schedulable(task)):"
luigi,12,"configured_client = hdfs_config.get_configured_hdfs_client()
if configured_client == ""webhdfs"":
return hdfs_webhdfs_client.WebHdfsClient()
if configured_client == ""snakebite"":
return hdfs_snakebite_client.SnakebiteHdfsClient()
if configured_client == ""snakebite_with_hadoopcli_fallback"":
return luigi.contrib.target.CascadingClient([hdfs_snakebite_client.SnakebiteHdfsClient(),
hdfs_hadoopcli_clients.create_hadoopcli_client()])
if configured_client == ""hadoopcli"":
return hdfs_hadoopcli_clients.create_hadoopcli_client()
raise Exception(""Unknown hdfs client "" + configured_client)","try:
return client_cache.client
except AttributeError:
configured_client = hdfs_config.get_configured_hdfs_client()
if configured_client == ""webhdfs"":
client_cache.client = hdfs_webhdfs_client.WebHdfsClient()
elif configured_client == ""snakebite"":
client_cache.client = hdfs_snakebite_client.SnakebiteHdfsClient()
elif configured_client == ""snakebite_with_hadoopcli_fallback"":
client_cache.client = luigi.contrib.target.CascadingClient([
hdfs_snakebite_client.SnakebiteHdfsClient(),
hdfs_hadoopcli_clients.create_hadoopcli_client(),
])
elif configured_client == ""hadoopcli"":
client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()
else:
raise Exception(""Unknown hdfs client "" + configured_client)
return client_cache.client"
luigi,13,self.fs.mkdir(d),self.mkdir(d)
luigi,14,"disable_failures = parameter.IntParameter(default=None,
disable_hard_timeout = parameter.IntParameter(default=None,
if (self.failures.first_failure_time is not None and
self.disable_hard_timeout):
def can_disable(self):
return (self.disable_failures is not None or
self.disable_hard_timeout is not None)
if new_status == FAILED and task.can_disable() and task.status != DISABLED:","disable_failures = parameter.IntParameter(default=999999999,
disable_hard_timeout = parameter.IntParameter(default=999999999,
if self.failures.first_failure_time is not None:
if new_status == FAILED and task.status != DISABLED:"
luigi,15,"if task.status not in (DONE, DISABLED) or \
getattr(task, 'scheduler_disable_time', None) is not None:","if task.status not in (DONE, DISABLED, UNKNOWN) or \
task.scheduler_disable_time is not None:"
luigi,16,"if task.id not in necessary_tasks and self._state.prune(task, self._config):","removed = self._state.prune(task, self._config)
if removed and task.id not in necessary_tasks:"
luigi,17,return scheduler.CentralPlannerScheduler(prune_on_get_work=True),"return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)"
luigi,18,elif task.scheduler_disable_time is not None:,elif task.scheduler_disable_time is not None and new_status != DISABLED:
luigi,19,if new_status == FAILED and task.can_disable():,if new_status == FAILED and task.can_disable() and task.status != DISABLED:
luigi,2,"""{}:{}.{}"".format(target.project_id, target.dataset_id, target.table_id)
raise ValueError(""Target not supported"")","super(BeamDataflowJobTask, self).__init__()
""""""
Given a luigi Target, determine a stringly typed path to pass as a
Dataflow job argument.
""""""
return ""{}:{}.{}"".format(target.table.project_id, target.table.dataset_id, target.table.table_id)
raise ValueError(""Target %s not supported"" % target)"
luigi,20,"if params[param_name].significant:
params_str[param_name] = params[param_name].serialize(param_value)",params_str[param_name] = params[param_name].serialize(param_value)
luigi,21,,
luigi,22,"def __init__(self, worker_id, last_active=None):","def __init__(self, worker_id, last_active=time.time()):"
luigi,23,"return scheduler.CentralPlannerScheduler()
self.last_active = last_active  # seconds since epoch","return scheduler.CentralPlannerScheduler(prune_on_get_work=True)
prune_on_get_work = parameter.BoolParameter(default=False)
self.last_active = last_active or time.time()  # seconds since epoch
if self._config.prune_on_get_work:
self.prune()"
luigi,24,"command += [name, '""{0}={1}""'.format(prop, value)]","command += [name, '{0}={1}'.format(prop, value)]"
luigi,25,path = self.s3_load_path(),path = self.s3_load_path
luigi,26,if not job.jar() or not os.path.exists(job.jar()):,"if not job.jar():
raise HadoopJarJobError(""Jar not defined"")
if not os.path.exists(job.jar()):"
luigi,27,"def parse_from_input(self, param_name, x):
if self.has_value:
return self.value
if self.has_value:
description.append("" [default: %s]"" % (self.value,))
params[param_name] = self.parse_from_input(param_name, value)
self.set_global(self.parse_from_input(param_name, value))","def parse_from_input(self, param_name, x, task_name=None):
if self.has_task_value(param_name=param_name, task_name=task_name):
return self.task_value(param_name=param_name, task_name=task_name)
if self.has_task_value(param_name=param_name, task_name=task_name):
value = self.task_value(param_name=param_name, task_name=task_name)
description.append("" [default: %s]"" % (value,))
params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)
self.set_global(self.parse_from_input(param_name, value, task_name=task_name))"
luigi,28,return stdout and table in stdout,return stdout and table.lower() in stdout
luigi,29,"if cls.run == NotImplemented:
continue
pass
def run(self):
NonAmbiguousClass.has_run = True
@mock.patch(""logging.getLogger"")
@mock.patch(""warnings.warn"")
def test_cmdline_non_ambiguous_class(self, warn, logger):
luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])
self.assertTrue(NonAmbiguousClass.has_run)",
luigi,3,"except ValueError:
return literal_eval(x)  # if this causes an error, let that error be raised.","except (ValueError, TypeError):
return tuple(literal_eval(x))  # if this causes an error, let that error be raised."
luigi,30,"try:
new_deps = self._run_get_new_deps()
if new_deps is None:
status = RUNNING
else:
status = SUSPENDED
logger.info(
'[pid %s] Worker %s new requirements      %s',
os.getpid(), self.worker_id, self.task.task_id)
return
finally:
if status != SUSPENDED:
self.task.trigger_event(
Event.PROCESSING_TIME, self.task, time.time() - t0)
error_message = json.dumps(self.task.on_success())
logger.info('[pid %s] Worker %s done      %s', os.getpid(),
self.worker_id, self.task.task_id)
self.task.trigger_event(Event.SUCCESS, self.task)
status = DONE","new_deps = self._run_get_new_deps()
if new_deps is None:
status = DONE
self.task.trigger_event(
Event.PROCESSING_TIME, self.task, time.time() - t0)
error_message = json.dumps(self.task.on_success())
logger.info('[pid %s] Worker %s done      %s', os.getpid(),
self.worker_id, self.task.task_id)
self.task.trigger_event(Event.SUCCESS, self.task)
else:
status = SUSPENDED
logger.info(
'[pid %s] Worker %s new requirements      %s',
os.getpid(), self.worker_id, self.task.task_id)"
luigi,31,in_workers = assistant or worker in task.workers,in_workers = (assistant and task.workers) or worker in task.workers
luigi,32,,
luigi,33,"positional_params = [(n, p) for n, p in params if p.significant]","positional_params = [(n, p) for n, p in params if not p.is_global]"
luigi,4,if len(self.columns) > 0:,if self.columns and len(self.columns) > 0:
luigi,5,"# Modify task_that_inherits by subclassing it and adding methods
@task._task_wraps(task_that_inherits)
class Wrapped(task_that_inherits):
def clone_parent(_self, **args):
return _self.clone(cls=self.task_to_inherit, **args)
return Wrapped
# Modify task_that_requres by subclassing it and adding methods
@task._task_wraps(task_that_requires)
class Wrapped(task_that_requires):
def requires(_self):
return _self.clone_parent()
return Wrapped","# Get all parameter objects from the underlying task
# Check if the parameter exists in the inheriting task
# If not, add it to the inheriting task
# Modify task_that_inherits by adding methods
def clone_parent(_self, **args):
return _self.clone(cls=self.task_to_inherit, **args)
task_that_inherits.clone_parent = clone_parent
return task_that_inherits
# Modify task_that_requres by adding methods
def requires(_self):
return _self.clone_parent()
task_that_requires.requires = requires
return task_that_requires"
luigi,6,"elif isinstance(value, list):
class _DictParamEncoder(JSONEncoder):
""""""
JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.
""""""
def default(self, obj):
if isinstance(obj, _FrozenOrderedDict):
return obj.get_wrapped()
return json.JSONEncoder.default(self, obj)
return json.dumps(x, cls=DictParameter._DictParamEncoder)
Ensure that list parameter is converted to a tuple so it can be hashed.
return list(json.loads(x))
return json.dumps(x)
return tuple(tuple(x) for x in json.loads(x))  # loop required to parse tuple of tuples
def serialize(self, x):
""""""
Opposite of :py:meth:`parse`.
Converts the value ``x`` to a string.
:param x: the value to serialize.
""""""
return json.dumps(x)","elif isinstance(value, list) or isinstance(value, tuple):
""""""
JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.
""""""
def default(self, obj):
if isinstance(obj, _FrozenOrderedDict):
return obj.get_wrapped()
return json.JSONEncoder.default(self, obj)
return json.dumps(x, cls=_DictParamEncoder)
Ensure that struct is recursively converted to a tuple so it can be hashed.
return list(json.loads(x, object_pairs_hook=_FrozenOrderedDict))
return json.dumps(x, cls=_DictParamEncoder)
# loop required to parse tuple of tuples
return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))"
luigi,7,"if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:
# don't allow re-scheduling of task while it is running, it must either fail or succeed first","if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:
# don't allow re-scheduling of task while it is running, it must either fail or succeed on the worker actually running it"
luigi,8,"""where table_schema = %s and table_name = %s limit 1"")
""where tablename = %s limit 1"")","""where table_schema = lower(%s) and table_name = lower(%s) limit 1"")
""where tablename = lower(%s) limit 1"")"
luigi,9,"set_tasks[""failed""] = {task for (task, status, ext) in task_history if status == 'FAILED'}
if status == 'PENDING' and task not in set_tasks[""failed""] and task not in set_tasks[""completed""] and not ext}
if status == 'PENDING' and task not in set_tasks[""failed""] and task not in set_tasks[""completed""] and ext}
if task in set_tasks[""failed""] or task in set_tasks[""upstream_failure""]:
if set_tasks[""failed""]:
smiley = "":(""
reason = ""there were failed tasks""
if set_tasks[""scheduling_error""]:
reason += "" and tasks whose scheduling failed""","set_tasks[""ever_failed""] = {task for (task, status, ext) in task_history if status == 'FAILED'}
set_tasks[""failed""] = set_tasks[""ever_failed""] - set_tasks[""completed""]
if status == 'PENDING' and task not in set_tasks[""ever_failed""] and task not in set_tasks[""completed""] and not ext}
if status == 'PENDING' and task not in set_tasks[""ever_failed""] and task not in set_tasks[""completed""] and ext}
if task in set_tasks[""ever_failed""] or task in set_tasks[""upstream_failure""]:
""ever_failed"",
if set_tasks[""ever_failed""]:
if not set_tasks[""failed""]:
smiley = "":)""
reason = ""there were failed tasks but they all suceeded in a retry""
else:
smiley = "":(""
reason = ""there were failed tasks""
if set_tasks[""scheduling_error""]:
reason += "" and tasks whose scheduling failed""
""""""
Test that a task once crashing and then succeeding should be counted as no failure.
""""""
def test_retry_sucess_task(self):
class Foo(luigi.Task):
run_count = 0
def run(self):
self.run_count += 1
if self.run_count == 1:
raise ValueError()
def complete(self):
return self.run_count > 0
self.run_and_expect('Foo --scheduler-retry-delay=0', 0)
self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)
self.run_with_config(dict(task_failed='3'), 'Foo', 0)"
matplotlib,1,"If *draw_disabled* is True, additionally replace drawing methods on
*renderer* by no-ops.  This is used by the tight-bbox-saving renderer,
which needs to walk through the artist tree to compute the tight-bbox, but
for which the output file may be closed early.
if draw_disabled:
for meth_name in dir(RendererBase):
if (meth_name.startswith(""draw_"")
or meth_name in [""open_group"", ""close_group""]):
setattr(renderer, meth_name, lambda *args, **kwargs: None)
print_method, orientation=orientation),
draw_disabled=True)
self.figure.draw(renderer)
kwargs = get_tight_layout_figure(
self, self.axes, subplotspec_list, renderer,
pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)
return backend_bases._get_renderer(fig, draw_disabled=True)","If you need a renderer without any active draw methods use
cbook._setattr_cm to temporary patch them out at your call site.
print_method, orientation=orientation)
)
no_ops = {
meth_name: lambda *args, **kwargs: None
for meth_name in dir(RendererBase)
if (meth_name.startswith(""draw_"")
or meth_name in [""open_group"", ""close_group""])
}
with _setattr_cm(renderer, **no_ops):
self.figure.draw(renderer)
from .cbook import _setattr_cm
from .backend_bases import RendererBase
no_ops = {
meth_name: lambda *args, **kwargs: None
for meth_name in dir(RendererBase)
if (meth_name.startswith(""draw_"")
or meth_name in [""open_group"", ""close_group""])
}
with _setattr_cm(renderer, **no_ops):
kwargs = get_tight_layout_figure(
self, self.axes, subplotspec_list, renderer,
pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)
return backend_bases._get_renderer(fig)"
matplotlib,10,# special-case label color to also apply to the offset text,"# labelOn and labelcolor also apply to the offset text.
if 'label1On' in kwtrans or 'label2On' in kwtrans:
self.offsetText.set_visible(
self._major_tick_kw.get('label1On', False)
or self._major_tick_kw.get('label2On', False))"
matplotlib,11,"if dpi is not None:
dpi_orig = self.figure.dpi
self.figure.dpi = dpi
tx, ty = self._get_xy_display()
return Bbox.from_bounds(tx, ty, 0, 0)
bbox, info, descent = self._get_layout(self._renderer)
x, y = self.get_unitless_position()
x, y = self.get_transform().transform((x, y))
bbox = bbox.translated(x, y)
if dpi is not None:
self.figure.dpi = dpi_orig
return bbox","if dpi is None:
dpi = self.figure.dpi
with cbook._setattr_cm(self.figure, dpi=dpi):
tx, ty = self._get_xy_display()
return Bbox.from_bounds(tx, ty, 0, 0)
with cbook._setattr_cm(self.figure, dpi=dpi):
bbox, info, descent = self._get_layout(self._renderer)
x, y = self.get_unitless_position()
x, y = self.get_transform().transform((x, y))
bbox = bbox.translated(x, y)
return bbox"
matplotlib,12,"y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)
xmin = np.resize(xmin, y.shape)
xmax = np.resize(xmax, y.shape)
verts = [((thisxmin, thisy), (thisxmax, thisy))
for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]
lines = mcoll.LineCollection(verts, colors=colors,
x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)
ymin = np.resize(ymin, x.shape)
ymax = np.resize(ymax, x.shape)
verts = [((thisx, thisymin), (thisx, thisymax))
for thisx, thisymin, thisymax in zip(x, ymin, ymax)]
lines = mcoll.LineCollection(verts, colors=colors,","# Create and combine masked_arrays from input
y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)
xmin = np.ravel(xmin)
xmax = np.ravel(xmax)
masked_verts = np.ma.empty((len(y), 2, 2))
masked_verts[:, 0, 0] = xmin
masked_verts[:, 0, 1] = y
masked_verts[:, 1, 0] = xmax
masked_verts[:, 1, 1] = y
lines = mcoll.LineCollection(masked_verts, colors=colors,
# Create and combine masked_arrays from input
x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)
ymin = np.ravel(ymin)
ymax = np.ravel(ymax)
masked_verts = np.ma.empty((len(x), 2, 2))
masked_verts[:, 0, 0] = x
masked_verts[:, 0, 1] = ymin
masked_verts[:, 1, 0] = x
masked_verts[:, 1, 1] = ymax
lines = mcoll.LineCollection(masked_verts, colors=colors,"
matplotlib,13,,
matplotlib,14,"# Update bbox last, as it depends on font properties.","# Update fontproperties first, as it has lowest priority.
fontproperties = kwargs.pop(""fontproperties"", sentinel)
if fontproperties is not sentinel:
self.set_fontproperties(fontproperties)
# Update bbox last, as it depends on font properties."
matplotlib,15,"vmin=-1.0, vmax=1.0),
vmin=-1.0, vmax=1.0),
def __init__(self, linthresh, linscale=1.0,
vmin=None, vmax=None, clip=False):
number of decades to use for each half of the linear range. For
example, when *linscale* == 1.0 (the default), the space used for
the positive and negative halves of the linear range will be equal
to one decade in the logarithmic range.
self._linscale_adj = (linscale / (1.0 - np.e ** -1))
log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))
exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)
vmin=-1.0, vmax=1.0),","vmin=-1.0, vmax=1.0, base=10),
vmin=-1.0, vmax=1.0, base=10),
def __init__(self, linthresh, linscale=1.0, vmin=None, vmax=None,
clip=False, base=None):
number of powers of *base* (decades for base 10) to use for each
half of the linear range. For example, when *linscale* == 1.0
(the default), the space used for the positive and negative halves
of the linear range will be equal to a decade in the logarithmic
range if ``base=10``.
base : float, default: None
For v3.2 the default is the old value of ``np.e``, but that is
deprecated for v3.3 when base will default to 10.  During the
transition, specify the *base* kwarg to avoid a deprecation
warning.
if base is None:
self._base = np.e
cbook.warn_deprecated(""3.3"", message=""default base will change ""
""from np.e to 10.  To suppress this warning specify the base ""
""kwarg."")
else:
self._base = base
self._log_base = np.log(self._base)
self._linscale_adj = (linscale / (1.0 - self._base ** -1))
log = (self._linscale_adj +
np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)
exp = np.power(self._base,
sign * a[masked] / self.linthresh - self._linscale_adj)
vmin=-1.0, vmax=1.0, base=10),"
matplotlib,16,,
matplotlib,17,,
matplotlib,18,,
matplotlib,19,,
matplotlib,2,"For non-filled markers, the *edgecolors* kwarg is ignored and
forced to 'face' internally.
edgecolors = 'face'
facecolors=colors,
edgecolors=edgecolors,","For non-filled markers, *edgecolors* is ignored. Instead, the color
is determined like with 'face', i.e. from *c*, *colors*, or
*facecolors*.
facecolors=colors if marker_obj.is_filled() else 'none',
edgecolors=edgecolors if marker_obj.is_filled() else colors,"
matplotlib,20,"ax.set_frame_on(False)
ax.set_xticks([])
ax.set_yticks([])
ax.set_facecolor((1, 0, 0, 0))
Check if a point is in an axes.
axes: topmost axes containing the point, or None if no axes.
if a.patch.contains_point(xy)]","ax.set_visible(False)
Return the topmost visible `~.axes.Axes` containing the point *xy*.
axes : `~matplotlib.axes.Axes` or None
The topmost visible axes containing the point, or None if no axes.
if a.patch.contains_point(xy) and a.get_visible()]"
matplotlib,21,"def line_props_with_rcdefaults(subkey, explicit, zdelta=0):
final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)
'whiskerprops', whiskerprops)
'capprops', capprops)
'medianprops', medianprops, zdelta)","def line_props_with_rcdefaults(subkey, explicit, zdelta=0,
use_marker=True):
if not use_marker:
d['marker'] = ''
final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,
use_marker=False)
'whiskerprops', whiskerprops, use_marker=False)
'capprops', capprops, use_marker=False)
'medianprops', medianprops, zdelta, use_marker=False)"
matplotlib,22,,
matplotlib,23,"x0, x1 = map(x_trf.inverted().transform, dL.intervalx)
y0, y1 = map(y_trf.inverted().transform, dL.intervaly)","x0, x1 = map(x_trf.transform, dL.intervalx)
y0, y1 = map(y_trf.transform, dL.intervaly)"
matplotlib,24,"setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),","setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),"
matplotlib,25,"if positions is None or len(positions) == 0:
elif hasattr(positions, 'ndim') and positions.ndim > 1:","if positions is None:
raise ValueError('positions must be an array-like object')
# Force a copy of positions
positions = np.array(positions, copy=True)
if positions.size == 0:
elif positions.ndim > 1:"
matplotlib,26,"setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),","setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),"
matplotlib,27,self._label = str(label),self._label = label
matplotlib,28,if self.get_xscale() == 'log':,"if self.get_xscale() == 'log' and (left <= 0 or right <= 0):
# Axes init calls set_xlim(0, 1) before get_xlim() can be called,
# so only grab the limits if we really need them.
old_left, old_right = self.get_xlim()"
matplotlib,29,"a, b = self.get_view_interval()
if inverted:
self.set_view_interval(max(a, b), min(a, b), ignore=True)
else:
self.set_view_interval(min(a, b), max(a, b), ignore=True)","# Currently, must be implemented in subclasses using set_xlim/set_ylim
# rather than generically using set_view_interval, so that shared
# axes get updated as well.
raise NotImplementedError('Derived must override')
def set_inverted(self, inverted):
# docstring inherited
a, b = self.get_view_interval()
self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)
def set_inverted(self, inverted):
# docstring inherited
a, b = self.get_view_interval()
self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)"
matplotlib,3,self._filled = True,"# Initial guess: Assume the marker is filled unless the fillstyle is
# set to 'none'. The marker function will override this for unfilled
# markers.
self._filled = self._fillstyle != 'none'"
matplotlib,30,"x = x * (N - 1)
xind = (N - 1) * np.linspace(0, 1, N) ** gamma
ind = np.searchsorted(x, xind)[1:-1]
distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])
lut = np.concatenate([
[y1[0]],
distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],
[y0[-1]],
])","if N == 1:
# convention: use the y = f(x=1) value for a 1-element lookup table
lut = np.array(y0[-1])
else:
x = x * (N - 1)
xind = (N - 1) * np.linspace(0, 1, N) ** gamma
ind = np.searchsorted(x, xind)[1:-1]
distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])
lut = np.concatenate([
[y1[0]],
distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],
[y0[-1]],
])"
matplotlib,4,"def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',
colors : list of colors, default: 'k'
def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',
colors : list of colors, default: 'k'
y, xmin, xmax, colors='k', linestyles='solid', label='', *,
x, ymin, ymax, colors='k', linestyles='solid', label='', *,","def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',
colors : list of colors, default: :rc:`lines.color`
def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',
colors : list of colors, default: :rc:`lines.color`
y, xmin, xmax, colors=None, linestyles='solid', label='', *,
x, ymin, ymax, colors=None, linestyles='solid', label='', *,"
matplotlib,5,linewidths = rcParams['lines.linewidth'],"if linewidths is None:
linewidths = rcParams['lines.linewidth']
elif np.iterable(linewidths):
linewidths = [
lw if lw is not None else rcParams['lines.linewidth']
for lw in linewidths]"
matplotlib,6,if c.size == xsize:,"# handle the documented special case of a 2D array with 1
# row which as RGB(A) to broadcast.
if c.shape == (1, 4) or c.shape == (1, 3):
c_is_mapped = False
if c.size != xsize:
valid_shape = False
elif c.size == xsize:"
matplotlib,7,"if hasattr(intensity, 'mask'):",if np.ma.is_masked(intensity):
matplotlib,8,,
matplotlib,9,,
pandas,1,"is_excluded_checks = (is_period_dtype, is_interval_dtype)","is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)"
pandas,10,"if isinstance(new, np.ndarray) and len(new) == len(mask):","if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):"
pandas,100,"rs = rs.loc[~rs.index.duplicated()]
rs = rs.reindex_like(data)
if freq is None:
mask = isna(com.values_from_object(data))
np.putmask(rs.values, mask, np.nan)","if freq is not None:
# Shift method is implemented differently when freq is not None
# We want to restore the original index
rs = rs.loc[~rs.index.duplicated()]
rs = rs.reindex_like(data)"
pandas,101,,
pandas,102,"columns = [0]
return arrays_to_mgr([values], columns, index, columns, dtype=dtype)","if isinstance(values, np.ndarray) and values.ndim > 1:
# GH#12513 a EA dtype passed with a 2D array, split into
#  multiple EAs that view the values
values = [values[:, n] for n in range(values.shape[1])]
else:
values = [values]
columns = list(range(len(values)))
return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"
pandas,103,,
pandas,104,"order = np.roll(list(range(result.index.nlevels)), -1)
result = result.reorder_levels(order)
result = result.reindex(q, level=-1)
# fix order.
hi = len(q) * self.ngroups
arr = np.arange(0, hi, self.ngroups)
arrays = []
for i in range(self.ngroups):
arr2 = arr + i
arrays.append(arr2)
indices = np.concatenate(arrays)
assert len(indices) == len(result)","order = list(range(1, result.index.nlevels)) + [0]
# temporarily saves the index names
index_names = np.array(result.index.names)
# set index names to positions to avoid confusion
result.index.names = np.arange(len(index_names))
# place quantiles on the inside
result = result.reorder_levels(order)
# restore the index names in order
result.index.names = index_names[order]
# reorder rows to keep things sorted
indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()"
pandas,105,"def transpose(self, *args, **kwargs):
*args, **kwargs
Additional arguments and keywords have no effect but might be
accepted for compatibility with numpy.
return super().transpose(1, 0, **kwargs)
def transpose(self, *args, **kwargs):
""""""
Permute the dimensions of the %(klass)s
Parameters
----------
args : %(args_transpose)s
copy : bool, default False
Make a copy of the underlying data. Mixed-dtype data will
always result in a copy
**kwargs
Additional keyword arguments will be passed to the function.
Returns
-------
y : same as input
Examples
--------
>>> p.transpose(2, 0, 1)
>>> p.transpose(2, 0, 1, copy=True)
""""""
# construct the args
axes, kwargs = self._construct_axes_from_arguments(
args, kwargs, require_all=True
)
axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)
axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)
# we must have unique axes
if len(axes) != len(set(axes)):
raise ValueError(f""Must specify {self._AXIS_LEN} unique axes"")
new_axes = self._construct_axes_dict_from(
self, [self._get_axis(x) for x in axes_names]
)
new_values = self.values.transpose(axes_numbers)
if kwargs.pop(""copy"", None) or (len(args) and args[-1]):
new_values = new_values.copy()
nv.validate_transpose(tuple(), kwargs)
return self._constructor(new_values, **new_axes).__finalize__(self)
params=[
(pd.Index, False),
(pd.Series, False),
(pd.DataFrame, False),
pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),
(tm.to_array, False),
],
ids=id_func,
""""""
Fixture similar to `box` but testing both transpose cases for DataFrame,
with the transpose=True case xfailed.
""""""
# GH#23620
return request.param","def transpose(self, *args, copy: bool = False):
*args : tuple, optional
Accepted for compatibility with NumPy.
copy : bool, default False
Whether to copy the data after transposing, even for DataFrames
with a single dtype.
Note that a copy is always required for mixed dtype DataFrames,
or for DataFrames with any extension types.
# construct the args
dtypes = list(self.dtypes)
if self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0]):
# We have EAs with the same dtype. We can preserve that dtype in transpose.
dtype = dtypes[0]
arr_type = dtype.construct_array_type()
values = self.values
new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]
result = self._constructor(
dict(zip(self.index, new_values)), index=self.columns
)
else:
new_values = self.values.T
if copy:
new_values = new_values.copy()
result = self._constructor(
new_values, index=self.columns, columns=self.index
)
return result.__finalize__(self)
def test_transpose(self, data):
df = pd.DataFrame({""A"": data[:4], ""B"": data[:4]}, index=[""a"", ""b"", ""c"", ""d""])
result = df.T
expected = pd.DataFrame(
{
""a"": type(data)._from_sequence([data[0]] * 2, dtype=data.dtype),
""b"": type(data)._from_sequence([data[1]] * 2, dtype=data.dtype),
""c"": type(data)._from_sequence([data[2]] * 2, dtype=data.dtype),
""d"": type(data)._from_sequence([data[3]] * 2, dtype=data.dtype),
},
index=[""A"", ""B""],
)
self.assert_frame_equal(result, expected)
self.assert_frame_equal(np.transpose(np.transpose(df)), df)
self.assert_frame_equal(np.transpose(np.transpose(df[[""A""]])), df[[""A""]])
@pytest.mark.xfail(reason=""Inconsistent sizes."")
def test_transpose(self, data):
super().test_transpose(data)
@skip_nested
def test_transpose(self, data):
super().test_transpose(data)"
pandas,106,elif self.is_all_dates:,elif self.is_all_dates and target.is_all_dates:  # GH 30399
pandas,107,"if other.name is None:
index = None
else:
# other must have the same index name as self, otherwise
# index name will be reset
index = Index([other.name], name=self.index.name)
other = other.reindex(combined_columns, copy=False)
other = DataFrame(
other.values.reshape((1, len(other))),
index=index,
columns=combined_columns,
other = other._convert(datetime=True, timedelta=True)","index = Index([other.name], name=self.index.name)
other = (
other.reindex(combined_columns, copy=False)
.to_frame()
.T.infer_objects()
.rename_axis(index.names, copy=False)"
pandas,108,,
pandas,109,,
pandas,11,"try:
i = level.get_loc(key)
except KeyError as err:
raise ValueError(f""Key {key} not in level {level}"") from err","mask = level == key
if not mask.any():
raise ValueError(f""Key {key} not in level {level}"")
i = np.nonzero(level == key)[0][0]"
pandas,110,is_positional = is_index_slice and not self.is_integer(),"is_positional = is_index_slice and not (
self.is_integer() or self.is_categorical()
)
@Appender(_index_shared_docs[""_maybe_cast_slice_bound""])
def _maybe_cast_slice_bound(self, label, side, kind):
if kind == ""loc"":
return label
return super()._maybe_cast_slice_bound(label, side, kind)"
pandas,111,"return self._invalid_indexer(""label"", key)
return self._invalid_indexer(""label"", key)
if self.categories._defer_to_indexing:
return self.categories._convert_scalar_indexer(key, kind=kind)","self._invalid_indexer(""label"", key)
self._invalid_indexer(""label"", key)
if kind == ""loc"":
try:
return self.categories._convert_scalar_indexer(key, kind=kind)
except TypeError:
self._invalid_indexer(""label"", key)"
pandas,112,,
pandas,113,"result = op(self._data, other)","""""""
Parameterized fixture for any nullable integer dtype.
* 'UInt8'
* 'Int8'
* 'UInt16'
* 'Int16'
* 'UInt32'
* 'Int32'
* 'UInt64'
* 'Int64'
""""""
return request.param
method = getattr(self._data, f""__{op_name}__"")
result = method(other)
if result is NotImplemented:
result = invalid_comparison(self._data, other, op)"
pandas,114,"s = getattr(series, ""_values"", series)
if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):
# GH 20882, 21257
# Unify Index and ExtensionArray treatment
# First try to convert the key to a location
# If that fails, raise a KeyError if an integer
# index, otherwise, see if key is an integer, and
# try that
try:
iloc = self.get_loc(key)
return s[iloc]
except KeyError:
if len(self) > 0 and (self.holds_integer() or self.is_boolean()):
raise
elif is_integer(key):
return s[key]","s = extract_array(series, extract_numpy=True)
if isinstance(s, ExtensionArray):
if is_scalar(key):
# GH 20882, 21257
# First try to convert the key to a location
# If that fails, raise a KeyError if an integer
# index, otherwise, see if key is an integer, and
# try that
try:
iloc = self.get_loc(key)
return s[iloc]
except KeyError:
if len(self) > 0 and (self.holds_integer() or self.is_boolean()):
raise
elif is_integer(key):
return s[key]
else:
# if key is not a scalar, directly raise an error (the code below
# would convert to numpy arrays and raise later any way) - GH29926
raise InvalidIndexError(key)"
pandas,115,"result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])","# np.interp requires sorted X values, #21037
indexer = np.argsort(inds[valid])
result[invalid] = np.interp(
inds[invalid], inds[valid][indexer], yvalues[valid][indexer]
)"
pandas,116,left_keys = [self.left.index.values],left_keys = [self.left.index._values]
pandas,117,"elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):","elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):"
pandas,118,"missing = Index(np.ravel(id_vars)).difference(cols)
missing = Index(np.ravel(value_vars)).difference(cols)","missing = Index(com.flatten(id_vars)).difference(cols)
missing = Index(com.flatten(value_vars)).difference(cols)"
pandas,119,margin_dummy[cols] = margin_dummy[cols].astype(dtype),"# check the result column and leave floats
margin_dummy[cols] = margin_dummy[cols].apply(
maybe_downcast_to_dtype, args=(dtype,)
)"
pandas,12,"ensure_float64,
mat = numeric_df.values
correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)
correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)
mat = ensure_float64(mat).T
mat = numeric_df.values
baseCov = np.empty((mat.shape[1], mat.shape[1]))
baseCov.fill(np.nan)
baseCov = np.cov(mat.T)
baseCov = baseCov.reshape((len(cols), len(cols)))
baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)
return self._constructor(baseCov, index=idx, columns=cols)","mat = numeric_df.astype(float, copy=False).to_numpy()
correl = libalgos.nancorr(mat, minp=min_periods)
correl = libalgos.nancorr_spearman(mat, minp=min_periods)
mat = mat.T
mat = numeric_df.astype(float, copy=False).to_numpy()
base_cov = np.empty((mat.shape[1], mat.shape[1]))
base_cov.fill(np.nan)
base_cov = np.cov(mat.T)
base_cov = base_cov.reshape((len(cols), len(cols)))
base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)
return self._constructor(base_cov, index=idx, columns=cols)"
pandas,120,"return Series(res, index=ri, name=self._selection_name)
return Series(
return result
def _reindex_output(self, output):
output: Series or DataFrame
d = {self.obj._get_axis_name(self.axis): index, ""copy"": False}
output = output.set_index(self.grouper.result_index).reindex(index, copy=False)","result = Series(res, index=ri, name=self._selection_name)
return self._reindex_output(result, fill_value=0)
result = Series(
return self._reindex_output(result, fill_value=0)
return self._reindex_output(result, fill_value=0)
out = self._reindex_output(out)
def _reindex_output(
self, output: FrameOrSeries, fill_value: Scalar = np.NaN
) -> FrameOrSeries:
output : Series or DataFrame
fill_value : scalar, default np.NaN
Value to use for unobserved categories if self.observed is False.
d = {
self.obj._get_axis_name(self.axis): index,
""copy"": False,
""fill_value"": fill_value,
}
output = output.set_index(self.grouper.result_index).reindex(
index, copy=False, fill_value=fill_value
)"
pandas,121,if m.any():,"if convert:
return [self.convert(numeric=False, copy=True)]
if m.any() or convert:"
pandas,122,"# canonicalize block order, using a tuple combining the type
# name and then mgr_locs because there might be unconsolidated
return (block.dtype.name, block.mgr_locs.as_array.tolist())","# canonicalize block order, using a tuple combining the mgr_locs
# then type name because there might be unconsolidated
return (block.mgr_locs.as_array.tolist(), block.dtype.name)"
pandas,123,"return Float64Index(data, copy=copy, dtype=dtype, name=name)
is_int64_dtype,
@staticmethod
def _validate_dtype(dtype):
"""""" require dtype to be None or int64 """"""
if not (dtype is None or is_int64_dtype(dtype)):
raise TypeError(""Invalid to pass a non-int64 dtype to RangeIndex"")","return Float64Index(data, copy=copy, name=name)
is_signed_integer_dtype,
is_unsigned_integer_dtype,
cls._validate_dtype(dtype)
@classmethod
def _validate_dtype(cls, dtype: Dtype) -> None:
if dtype is None:
return
validation_metadata = {
""int64index"": (is_signed_integer_dtype, ""signed integer""),
""uint64index"": (is_unsigned_integer_dtype, ""unsigned integer""),
""float64index"": (is_float_dtype, ""float""),
""rangeindex"": (is_signed_integer_dtype, ""signed integer""),
}
validation_func, expected = validation_metadata[cls._typ]
if not validation_func(dtype):
msg = f""Incorrect `dtype` passed: expected {expected}, received {dtype}""
raise ValueError(msg)"
pandas,124,,
pandas,125,,
pandas,126,"elif isinstance(other, list) and not isinstance(other[0], DataFrame):
other = DataFrame(other)
if (self.columns.get_indexer(other.columns) >= 0).all():
other = other.reindex(columns=self.columns)","elif isinstance(other, list):
if not other:
pass
elif not isinstance(other[0], DataFrame):
other = DataFrame(other)
if (self.columns.get_indexer(other.columns) >= 0).all():
other = other.reindex(columns=self.columns)"
pandas,127,,
pandas,128,,
pandas,129,,
pandas,13,"return _isna_ndarraylike(obj)
return _isna_ndarraylike(np.asarray(obj, dtype=object))
return _isna_ndarraylike(np.asarray(obj))
return _isna_ndarraylike_old(obj)
return _isna_ndarraylike_old(np.asarray(obj, dtype=object))
return _isna_ndarraylike_old(np.asarray(obj))
values = getattr(obj, ""_values"", obj)
dtype = values.dtype
if is_extension_array_dtype(dtype):
result = values.isna()
elif is_string_dtype(dtype):
result = _isna_string_dtype(values, dtype, old=False)
elif needs_i8_conversion(dtype):
# this is the NaT pattern
result = values.view(""i8"") == iNaT
else:
result = np.isnan(values)
# box
if isinstance(obj, ABCSeries):
result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)
return result
if is_string_dtype(dtype):
result = _isna_string_dtype(values, dtype, old=True)
result = ~np.isfinite(values)","return _isna_ndarraylike(obj, old=False)
return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)
return _isna_ndarraylike(np.asarray(obj), old=False)
return _isna_ndarraylike(obj, old=True)
return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)
return _isna_ndarraylike(np.asarray(obj), old=True)
""""""
Return an array indicating which values of the input array are NaN / NA.
Parameters
----------
obj: array-like
The input array whose elements are to be checked.
old: bool
Whether or not to treat infinite values as NA.
Returns
-------
array-like
Array of boolean values denoting the NA status of each element.
""""""
if is_extension_array_dtype(dtype):
if old:
result = values.isna() | (values == -np.inf) | (values == np.inf)
else:
result = values.isna()
elif is_string_dtype(dtype):
result = _isna_string_dtype(values, dtype, old=old)
if old:
result = ~np.isfinite(values)
else:
result = np.isnan(values)"
pandas,130,,
pandas,131,"# blow up if we operate on categories
result = take_1d(result, self.orig.cat.codes)
data = Series(orig.values.categories, name=orig.name, copy=False)","data = Series(
orig.array,
name=orig.name,
copy=False,
dtype=orig.values.categories.dtype,
)"
pandas,132,"return _wrap_results(result, values.dtype)","orig_dtype = values.dtype
values, mask, dtype, dtype_max, fill_value = _get_values(values, skipna, mask=mask)
return _wrap_results(result, orig_dtype)"
pandas,133,"else:
_maybe_transposed_self = self",axis = self._get_axis_number(axis)
pandas,134,"# http://stackoverflow.com/questions/23814368/sorting-pandas-categorical-labels-after-groupby  # noqa: E501
end_date = Timestamp(datetime(2030, 12, 31))","# http://stackoverflow.com/questions/23814368/sorting-pandas-
#        categorical-labels-after-groupby
end_date = Timestamp(datetime(2200, 12, 31))"
pandas,135,"return self._aggregate_series_pure_python(obj, func)","except TypeError as err:
if ""ndarray"" in str(err):
# raised in libreduction if obj's values is no ndarray
pass
else:
raise
return self._aggregate_series_pure_python(obj, func)"
pandas,136,"is_int64_dtype,
elif is_int64_dtype(lt):",elif is_integer_dtype(lt):
pandas,137,,
pandas,138,"if the passed data is of datetime/timedelta type,
this method converts it to numeric so that cut method can","is_bool_dtype,
if the passed data is of datetime/timedelta or bool type,
this method converts it to numeric so that cut or qcut method can
elif is_bool_dtype(x):
# GH 20303
x = x.astype(np.int64)
""box, compare"",
[
(Series, tm.assert_series_equal),
(np.array, tm.assert_categorical_equal),
(list, tm.assert_equal),
],
# issue 20303
data_expected = box([0, 1, 1, 0, 1] * 10)
data_result = box([False, True, True, False, True] * 10)
expected = cut(data_expected, bins, duplicates=""drop"")
result = cut(data_result, bins, duplicates=""drop"")
compare(result, expected)"
pandas,139,),"),
name=self.name,"
pandas,14,"if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):
# in particular case where right is an array of DateOffsets","if (is_datetime64_dtype(ldtype) and is_object_dtype(rdtype)) or (
is_datetime64_dtype(rdtype) and is_object_dtype(ldtype)
):
# in particular case where one is an array of DateOffsets
params=[
(""foo"", None, None),
(""Egon"", ""Venkman"", None),
(""NCC1701D"", ""NCC1701D"", ""NCC1701D""),
]
""""""
A 3-tuple of names, the first two for operands, the last for a result.
""""""
return request.param"
pandas,140,idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx]),"idx
for idx in range(len(result.columns))
if is_object_dtype(result.dtypes.iloc[idx])"
pandas,141,"# Work on reversed range for simplicity:
start, stop, step = (self.stop - self.step, self.start + 1, -self.step)","# GH 28678: work on reversed range for simplicity
reverse = self._range[::-1]
start, stop, step = reverse.start, reverse.stop, reverse.step"
pandas,142,"def test_diff(self):
# Just run the function
self.ts.diff()
# int dtype
a = 10000000000000000
b = a + 1
s = Series([a, b])
rs = s.diff()
assert rs[1] == 1
# neg n
rs = self.ts.diff(-1)
xp = self.ts - self.ts.shift(-1)
assert_series_equal(rs, xp)
# 0
rs = self.ts.diff(0)
xp = self.ts - self.ts
assert_series_equal(rs, xp)
# datetime diff (GH3100)
s = Series(date_range(""20130102"", periods=5))
rs = s - s.shift(1)
xp = s.diff()
assert_series_equal(rs, xp)
# timedelta diff
nrs = rs - rs.shift(1)
nxp = xp.diff()
assert_series_equal(nrs, nxp)
# with tz
s = Series(
date_range(""2000-01-01 09:00:00"", periods=5, tz=""US/Eastern""), name=""foo""
)
result = s.diff()
assert_series_equal(
result, Series(TimedeltaIndex([""NaT""] + [""1 days""] * 4), name=""foo"")
)","is_bool = False
is_bool = True
elif is_bool:
out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]"
pandas,143,"if not (method is None and tolerance is None and is_list_like(target)):
return super().get_indexer(target, method=method, tolerance=tolerance)","if com.any_not_none(method, tolerance, limit) or not is_list_like(target):
return super().get_indexer(
target, method=method, tolerance=tolerance, limit=limit
)"
pandas,144,"ax.set_xticks(self.tick_pos)
ax.set_xticklabels(ticklabels)","if self.xticks is not None:
ax.set_xticks(np.array(self.xticks))
else:
ax.set_xticks(self.tick_pos)
ax.set_xticklabels(ticklabels)"
pandas,145,"def column_op(a, b):
return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}","if right.dtype == ""timedelta64[ns]"":
# ensure we treat NaT values as the correct dtype
# Note: we do not do this unconditionally as it may be lossy or
#  expensive for EA dtypes.
right = np.asarray(right)
def column_op(a, b):
return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}
else:
def column_op(a, b):
return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}"
pandas,146,"if np.any(left_value != right_value):
return False
try:
return array_equivalent(
com.values_from_object(self), com.values_from_object(other)
)
except Exception:
return False","try:
if np.any(left_value != right_value):
return False
except TypeError as err:
if ""Cannot compare tz-naive"" in str(err):
# tzawareness compat failure, see GH#28507
return False
raise
return array_equivalent(
com.values_from_object(self), com.values_from_object(other)
)"
pandas,147,"elif tz is None:
try:
match = cls._match.match(string)
if match:
d = match.groupdict()
except Exception:
# TODO(py3): Change this pass to `raise TypeError(msg) from e`
pass","if tz is None:
match = cls._match.match(string)
if match:
d = match.groupdict()
try:
except (KeyError, TypeError, ValueError) as err:
# KeyError if maybe_get_tz tries and fails to get a
#  pytz timezone (actually pytz.UnknownTimeZoneError).
# TypeError if we pass a nonsense tz;
# ValueError if we pass a unit other than ""ns""
raise TypeError(msg.format(string)) from err"
pandas,148,"EMPTY_SERIES = Series([])
r = self.f(EMPTY_SERIES, *self.args, **self.kwds)
return self.obj._constructor_sliced(np.nan, index=self.agg_axis)","r = self.f(Series([]))
if len(self.agg_axis):
r = self.f(Series([]))
else:
r = np.nan
return self.obj._constructor_sliced(r, index=self.agg_axis)"
pandas,149,"if is_s3_url(path):
# path is s3:// so we need to open the s3file in 'wb' mode.
# And pass the opened s3file to the fastparquet internal impl.","if is_s3_url(path) or is_gcs_url(path):
# if path is s3:// or gs:// we need to open the file in 'wb' mode.
# And pass the opened file to the fastparquet internal impl."
pandas,15,"is_categorical_dtype,
is_datetime64_dtype,
is_datetime64tz_dtype,
is_datetime_or_timedelta_dtype,
is_float_dtype,
is_integer_dtype,
is_string_dtype,
is_unsigned_integer_dtype,
pandas_dtype,
""""""
Wrap comparison operations to convert Timestamp/Timedelta/Period-like to
boxed scalars/arrays.
""""""
opname = f""__{op.__name__}__""
nat_result = opname == ""__ne__""
class InvalidComparison(Exception):
pass
def _validate_comparison_value(self, other):
if isinstance(other, str):
try:
# GH#18435 strings get a pass from tzawareness compat
other = self._scalar_from_string(other)
except ValueError:
# failed to parse as Timestamp/Timedelta/Period
raise InvalidComparison(other)
if isinstance(other, self._recognized_scalars) or other is NaT:
other = self._scalar_type(other)
self._check_compatible_with(other)
elif not is_list_like(other):
raise InvalidComparison(other)
elif len(other) != len(self):
raise ValueError(""Lengths must match"")
else:
if isinstance(other, list):
# TODO: could use pd.Index to do inference?
other = np.array(other)
if not isinstance(other, (np.ndarray, type(self))):
raise InvalidComparison(other)
elif is_object_dtype(other.dtype):
pass
elif not type(self)._is_recognized_dtype(other.dtype):
raise InvalidComparison(other)
else:
# For PeriodDType this casting is unnecessary
# TODO: use Index to do inference?
other = type(self)._from_sequence(other)
self._check_compatible_with(other)
return other
@unpack_zerodim_and_defer(opname)
def wrapper(self, other):
try:
other = _validate_comparison_value(self, other)
except InvalidComparison:
return invalid_comparison(self, other, op)
dtype = getattr(other, ""dtype"", None)
if is_object_dtype(dtype):
# We have to use comp_method_OBJECT_ARRAY instead of numpy
#  comparison otherwise it would fail to raise when
#  comparing tz-aware and tz-naive
with np.errstate(all=""ignore""):
result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)
return result
if isinstance(other, self._scalar_type) or other is NaT:
other_i8 = self._unbox_scalar(other)
else:
# Then type(other) == type(self)
other_i8 = other.asi8
result = op(self.asi8, other_i8)
o_mask = isna(other)
if self._hasnans | np.any(o_mask):
result[self._isnan | o_mask] = nat_result
return result
return set_function_name(wrapper, opname, cls)
_data: np.ndarray
@classmethod
def _simple_new(cls, values: np.ndarray, **kwargs):
raise AbstractMethodError(cls)
def _scalar_type(self) -> Type[DatetimeLikeScalar]:
""""""
The scalar associated with this datelike
* PeriodArray : Period
* DatetimeArray : Timestamp
* TimedeltaArray : Timedelta
raise AbstractMethodError(self)
def _scalar_from_string(
self, value: str
) -> Union[Period, Timestamp, Timedelta, NaTType]:
Construct a scalar type from a string.
Parameters
----------
value : str
Returns
-------
Period, Timestamp, or Timedelta, or NaT
Whatever the type of ``self._scalar_type`` is.
Notes
-----
This should call ``self._check_compatible_with`` before
unboxing the result.
raise AbstractMethodError(self)
def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:
Unbox the integer value of a scalar `value`.
Parameters
----------
value : Union[Period, Timestamp, Timedelta]
Returns
-------
int
Examples
--------
>>> self._unbox_scalar(Timedelta(""10s""))  # doctest: +SKIP
10000000000
""""""
raise AbstractMethodError(self)
def _check_compatible_with(
self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False
) -> None:
Verify that `self` and `other` are compatible.
* DatetimeArray verifies that the timezones (if any) match
* PeriodArray verifies that the freq matches
* Timedelta has no verification
In each case, NaT is considered compatible.
Parameters
----------
other
setitem : bool, default False
For __setitem__ we may have stricter compatibility resrictions than
for comparisons.
Raises
------
Exception
raise AbstractMethodError(self)
""""""
Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.
""""""
@Substitution(
URL=""https://docs.python.org/3/library/datetime.html""
""#strftime-and-strptime-behavior""
)
def strftime(self, date_format):
""""""
Convert to Index using specified date_format.
Return an Index of formatted strings specified by date_format, which
supports the same string format as the python standard library. Details
of the string format can be found in `python string format
doc <%(URL)s>`__.
Parameters
----------
date_format : str
Date format string (e.g. ""%%Y-%%m-%%d"").
Returns
-------
ndarray
NumPy ndarray of formatted strings.
See Also
--------
to_datetime : Convert the given argument to datetime.
DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.
DatetimeIndex.round : Round the DatetimeIndex to the specified freq.
DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.
Examples
--------
>>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),
...                     periods=3, freq='s')
>>> rng.strftime('%%B %%d, %%Y, %%r')
Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',
'March 10, 2018, 09:00:02 AM'],
dtype='object')
""""""
result = self._format_native_types(date_format=date_format, na_rep=np.nan)
return result.astype(object)
""""""
Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.
""""""
_round_doc = """"""
Perform {op} operation on the data to the specified `freq`.
Parameters
----------
freq : str or Offset
The frequency level to {op} the index to. Must be a fixed
frequency like 'S' (second) not 'ME' (month end). See
:ref:`frequency aliases <timeseries.offset_aliases>` for
a list of possible `freq` values.
ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
Only relevant for DatetimeIndex:
- 'infer' will attempt to infer fall dst-transition hours based on
order
- bool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)
- 'NaT' will return NaT where there are ambiguous times
- 'raise' will raise an AmbiguousTimeError if there are ambiguous
times.
.. versionadded:: 0.24.0
nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \
A nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.
- 'shift_forward' will shift the nonexistent time forward to the
closest existing time
- 'shift_backward' will shift the nonexistent time backward to the
closest existing time
- 'NaT' will return NaT where there are nonexistent times
- timedelta objects will shift nonexistent times by the timedelta
- 'raise' will raise an NonExistentTimeError if there are
nonexistent times.
.. versionadded:: 0.24.0
Returns
-------
DatetimeIndex, TimedeltaIndex, or Series
Index of the same type for a DatetimeIndex or TimedeltaIndex,
or a Series with the same index for a Series.
Raises
------
ValueError if the `freq` cannot be converted.
Examples
**DatetimeIndex**
>>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')
>>> rng
DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',
'2018-01-01 12:01:00'],
dtype='datetime64[ns]', freq='T')
_round_example = """""">>> rng.round('H')
DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
'2018-01-01 12:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.round(""H"")
0   2018-01-01 12:00:00
1   2018-01-01 12:00:00
2   2018-01-01 12:00:00
dtype: datetime64[ns]
_floor_example = """""">>> rng.floor('H')
DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',
'2018-01-01 12:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.floor(""H"")
0   2018-01-01 11:00:00
1   2018-01-01 12:00:00
2   2018-01-01 12:00:00
dtype: datetime64[ns]
_ceil_example = """""">>> rng.ceil('H')
DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
'2018-01-01 13:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.ceil(""H"")
0   2018-01-01 12:00:00
1   2018-01-01 12:00:00
2   2018-01-01 13:00:00
dtype: datetime64[ns]
def _round(self, freq, mode, ambiguous, nonexistent):
# round the local times
if is_datetime64tz_dtype(self):
# operate on naive timestamps, then convert back to aware
naive = self.tz_localize(None)
result = naive._round(freq, mode, ambiguous, nonexistent)
aware = result.tz_localize(
self.tz, ambiguous=ambiguous, nonexistent=nonexistent
)
return aware
values = self.view(""i8"")
result = round_nsint64(values, mode, freq)
result = self._maybe_mask_results(result, fill_value=NaT)
return self._simple_new(result, dtype=self.dtype)
@Appender((_round_doc + _round_example).format(op=""round""))
def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
@Appender((_round_doc + _floor_example).format(op=""floor""))
def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
@Appender((_round_doc + _ceil_example).format(op=""ceil""))
def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
def _with_freq(self, freq):
Helper to set our freq in-place, returning self to allow method chaining.
Parameters
----------
freq : DateOffset, None, or ""infer""
Returns
-------
self
# GH#29843
if freq is None:
# Always valid
pass
elif len(self) == 0 and isinstance(freq, DateOffset):
# Always valid.  In the TimedeltaArray case, we assume this
#  is a Tick offset.
pass
else:
# As an internal method, we can ensure this assertion always holds
assert freq == ""infer""
freq = frequencies.to_offset(self.inferred_freq)
self._freq = freq
return self
ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray
""""""
Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
Assumes that __new__/__init__ defines:
_data
_freq
and that the inheriting class has methods:
_generate_range
""""""
# ------------------------------------------------------------------
# NDArrayBackedExtensionArray compat
@property
def _ndarray(self) -> np.ndarray:
# NB: A bunch of Interval tests fail if we use ._data
return self.asi8
def _from_backing_data(self: _T, arr: np.ndarray) -> _T:
# Note: we do not retain `freq`
return type(self)(arr, dtype=self.dtype)  # type: ignore
# ------------------------------------------------------------------
def _box_func(self):
""""""
box function to get object from internal representation
""""""
def _box_values(self, values):
apply box func to passed values
return lib.map_infer(values, self._box_func)
def __iter__(self):
return (self._box_func(v) for v in self.asi8)
@property
def asi8(self) -> np.ndarray:
""""""
Integer representation of the values.
Returns
-------
ndarray
An ndarray with int64 dtype.
""""""
# do not cache or you'll create a memory leak
return self._data.view(""i8"")
# ----------------------------------------------------------------
# Rendering Methods
def _format_native_types(self, na_rep=""NaT"", date_format=None):
Helper method for astype when converting to strings.
ndarray[str]
raise AbstractMethodError(self)
def _formatter(self, boxed=False):
# TODO: Remove Datetime & DatetimeTZ formatters.
return ""'{}'"".format
# ----------------------------------------------------------------
# Array-Like / EA-Interface Methods
def __array__(self, dtype=None) -> np.ndarray:
# used for Timedelta/DatetimeArray, overwritten by PeriodArray
if is_object_dtype(dtype):
return np.array(list(self), dtype=object)
return self._data
def __getitem__(self, key):
""""""
This getitem defers to the underlying array, which by-definition can
only handle list-likes, slices, and integer scalars
""""""
if com.is_bool_indexer(key):
# first convert to boolean, because check_array_indexer doesn't
# allow object dtype
if is_object_dtype(key):
key = np.asarray(key, dtype=bool)
key = check_array_indexer(self, key)
key = lib.maybe_booleans_to_slice(key.view(np.uint8))
elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):
# see https://github.com/pandas-dev/pandas/issues/31299, need to allow
# this for now (would otherwise raise in check_array_indexer)
pass
else:
key = check_array_indexer(self, key)
freq = self._get_getitem_freq(key)
result = self._data[key]
if lib.is_scalar(result):
return self._box_func(result)
return self._simple_new(result, dtype=self.dtype, freq=freq)
def _get_getitem_freq(self, key):
""""""
Find the `freq` attribute to assign to the result of a __getitem__ lookup.
""""""
is_period = is_period_dtype(self.dtype)
if is_period:
freq = self.freq
else:
freq = None
if isinstance(key, slice):
if self.freq is not None and key.step is not None:
freq = key.step * self.freq
else:
freq = self.freq
elif key is Ellipsis:
# GH#21282 indexing with Ellipsis is similar to a full slice,
#  should preserve `freq` attribute
freq = self.freq
return freq
def __setitem__(
self,
key: Union[int, Sequence[int], Sequence[bool], slice],
value: Union[NaTType, Any, Sequence[Any]],
) -> None:
# I'm fudging the types a bit here. ""Any"" above really depends
# on type(self). For PeriodArray, it's Period (or stuff coercible
# to a period in from_sequence). For DatetimeArray, it's Timestamp...
# I don't know if mypy can do that, possibly with Generics.
# https://mypy.readthedocs.io/en/latest/generics.html
if is_list_like(value):
is_slice = isinstance(key, slice)
if lib.is_scalar(key):
raise ValueError(""setting an array element with a sequence."")
if not is_slice:
key = cast(Sequence, key)
if len(key) != len(value) and not com.is_bool_indexer(key):
msg = (
f""shape mismatch: value array of length '{len(key)}' ""
""does not match indexing result of length ""
f""'{len(value)}'.""
)
raise ValueError(msg)
elif not len(key):
return
value = self._validate_setitem_value(value)
key = check_array_indexer(self, key)
self._data[key] = value
self._maybe_clear_freq()
def _maybe_clear_freq(self):
# inplace operations like __setitem__ may invalidate the freq of
# DatetimeArray and TimedeltaArray
pass
def astype(self, dtype, copy=True):
# Some notes on cases we don't have to handle here in the base class:
#   1. PeriodArray.astype handles period -> period
#   2. DatetimeArray.astype handles conversion between tz.
#   3. DatetimeArray.astype handles datetime -> period
dtype = pandas_dtype(dtype)
if is_object_dtype(dtype):
return self._box_values(self.asi8.ravel()).reshape(self.shape)
elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):
return self._format_native_types()
elif is_integer_dtype(dtype):
# we deliberately ignore int32 vs. int64 here.
# See https://github.com/pandas-dev/pandas/issues/24381 for more.
values = self.asi8
if is_unsigned_integer_dtype(dtype):
# Again, we ignore int32 vs. int64
values = values.view(""uint64"")
if copy:
values = values.copy()
return values
elif (
is_datetime_or_timedelta_dtype(dtype)
and not is_dtype_equal(self.dtype, dtype)
) or is_float_dtype(dtype):
# disallow conversion between datetime/timedelta,
# and conversions for any datetimelike to float
msg = f""Cannot cast {type(self).__name__} to dtype {dtype}""
raise TypeError(msg)
elif is_categorical_dtype(dtype):
arr_cls = dtype.construct_array_type()
return arr_cls(self, dtype=dtype)
return np.asarray(self, dtype=dtype)
def view(self, dtype=None):
if dtype is None or dtype is self.dtype:
return type(self)(self._data, dtype=self.dtype)
return self._data.view(dtype=dtype)
# ------------------------------------------------------------------
# ExtensionArray Interface
@classmethod
def _concat_same_type(cls, to_concat, axis: int = 0):
# do not pass tz to set because tzlocal cannot be hashed
dtypes = {str(x.dtype) for x in to_concat}
if len(dtypes) != 1:
raise ValueError(""to_concat must have the same dtype (tz)"", dtypes)
obj = to_concat[0]
dtype = obj.dtype
i8values = [x.asi8 for x in to_concat]
values = np.concatenate(i8values, axis=axis)
new_freq = None
if is_period_dtype(dtype):
new_freq = obj.freq
elif axis == 0:
# GH 3232: If the concat result is evenly spaced, we can retain the
# original frequency
to_concat = [x for x in to_concat if len(x)]
if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):
pairs = zip(to_concat[:-1], to_concat[1:])
if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):
new_freq = obj.freq
return cls._simple_new(values, dtype=dtype, freq=new_freq)
def copy(self):
values = self.asi8.copy()
return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)
def _values_for_factorize(self):
return self.asi8, iNaT
@classmethod
def _from_factorized(cls, values, original):
return cls(values, dtype=original.dtype)
def _values_for_argsort(self):
return self._data
@Appender(ExtensionArray.shift.__doc__)
def shift(self, periods=1, fill_value=None, axis=0):
if not self.size or periods == 0:
return self.copy()
fill_value = self._validate_shift_value(fill_value)
new_values = shift(self._data, periods, axis, fill_value)
return type(self)._simple_new(new_values, dtype=self.dtype)
# ------------------------------------------------------------------
# Validation Methods
# TODO: try to de-duplicate these, ensure identical behavior
def _validate_fill_value(self, fill_value):
""""""
If a fill_value is passed to `take` convert it to an i8 representation,
raising ValueError if this is not possible.
fill_value : object
fill_value : np.int64
Raises
------
ValueError
if is_valid_nat_for_dtype(fill_value, self.dtype):
fill_value = iNaT
elif isinstance(fill_value, self._recognized_scalars):
self._check_compatible_with(fill_value)
fill_value = self._scalar_type(fill_value)
fill_value = self._unbox_scalar(fill_value)
else:
raise ValueError(
f""'fill_value' should be a {self._scalar_type}. ""
f""Got '{str(fill_value)}'.""
)
return fill_value
def _validate_shift_value(self, fill_value):
# TODO(2.0): once this deprecation is enforced, used _validate_fill_value
if is_valid_nat_for_dtype(fill_value, self.dtype):
fill_value = NaT
elif not isinstance(fill_value, self._recognized_scalars):
# only warn if we're not going to raise
if self._scalar_type is Period and lib.is_integer(fill_value):
# kludge for #31971 since Period(integer) tries to cast to str
new_fill = Period._from_ordinal(fill_value, freq=self.freq)
else:
new_fill = self._scalar_type(fill_value)
# stacklevel here is chosen to be correct when called from
#  DataFrame.shift or Series.shift
warnings.warn(
f""Passing {type(fill_value)} to shift is deprecated and ""
""will raise in a future version, pass ""
f""{self._scalar_type.__name__} instead."",
FutureWarning,
stacklevel=10,
)
fill_value = new_fill
fill_value = self._unbox_scalar(fill_value)
return fill_value
def _validate_searchsorted_value(self, value):
if isinstance(value, str):
value = self._scalar_from_string(value)
except ValueError as err:
raise TypeError(
""searchsorted requires compatible dtype or scalar""
) from err
elif is_valid_nat_for_dtype(value, self.dtype):
value = NaT
elif isinstance(value, self._recognized_scalars):
value = self._scalar_type(value)
elif is_list_like(value) and not isinstance(value, type(self)):
value = array(value)
if not type(self)._is_recognized_dtype(value):
raise TypeError(
""searchsorted requires compatible dtype or scalar, ""
f""not {type(value).__name__}""
)
if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):
raise TypeError(f""Unexpected type for 'value': {type(value)}"")
if isinstance(value, type(self)):
self._check_compatible_with(value)
value = value.asi8
else:
value = self._unbox_scalar(value)
return value
def _validate_setitem_value(self, value):
if lib.is_scalar(value) and not isna(value):
value = com.maybe_box_datetimelike(value)
if is_list_like(value):
value = type(self)._from_sequence(value, dtype=self.dtype)
self._check_compatible_with(value, setitem=True)
value = value.asi8
elif isinstance(value, self._scalar_type):
self._check_compatible_with(value, setitem=True)
value = self._unbox_scalar(value)
elif is_valid_nat_for_dtype(value, self.dtype):
value = iNaT
else:
msg = (
f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
f""or array of those. Got '{type(value).__name__}' instead.""
)
raise TypeError(msg)
return value
def _validate_insert_value(self, value):
if isinstance(value, self._recognized_scalars):
value = self._scalar_type(value)
self._check_compatible_with(value, setitem=True)
# TODO: if we dont have compat, should we raise or astype(object)?
#  PeriodIndex does astype(object)
elif is_valid_nat_for_dtype(value, self.dtype):
# GH#18295
value = NaT
else:
raise TypeError(
f""cannot insert {type(self).__name__} with incompatible label""
)
return value
def _validate_where_value(self, other):
if is_valid_nat_for_dtype(other, self.dtype):
other = NaT
elif isinstance(other, self._recognized_scalars):
other = self._scalar_type(other)
self._check_compatible_with(other, setitem=True)
elif not is_list_like(other):
raise TypeError(f""Where requires matching dtype, not {type(other)}"")
else:
# Do type inference if necessary up front
# e.g. we passed PeriodIndex.values and got an ndarray of Periods
other = array(other)
other = extract_array(other, extract_numpy=True)
if is_categorical_dtype(other.dtype):
# e.g. we have a Categorical holding self.dtype
if is_dtype_equal(other.categories.dtype, self.dtype):
other = other._internal_get_values()
if not type(self)._is_recognized_dtype(other.dtype):
raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
self._check_compatible_with(other, setitem=True)
if lib.is_scalar(other):
other = self._unbox_scalar(other)
else:
other = other.view(""i8"")
return other
# ------------------------------------------------------------------
# Additional array methods
#  These are not part of the EA API, but we implement them because
#  pandas assumes they're there.
def searchsorted(self, value, side=""left"", sorter=None):
Find indices where elements should be inserted to maintain order.
Find the indices into a sorted array `self` such that, if the
corresponding elements in `value` were inserted before the indices,
the order of `self` would be preserved.
value : array_like
Values to insert into `self`.
side : {'left', 'right'}, optional
If 'left', the index of the first suitable location found is given.
If 'right', return the last such index.  If there is no suitable
index, return either 0 or N (where N is the length of `self`).
sorter : 1-D array_like, optional
Optional array of integer indices that sort `self` into ascending
order. They are typically the result of ``np.argsort``.
indices : array of ints
Array of insertion points with the same shape as `value`.
value = self._validate_searchsorted_value(value)
# TODO: Use datetime64 semantics for sorting, xref GH#29844
return self.asi8.searchsorted(value, side=side, sorter=sorter)
def value_counts(self, dropna=False):
Return a Series containing counts of unique values.
dropna : bool, default True
Don't include counts of NaT values.
Returns
-------
Series
""""""
from pandas import Series, Index
if dropna:
values = self[~self.isna()]._data
else:
values = self._data
cls = type(self)
result = value_counts(values, sort=False, dropna=dropna)
index = Index(
cls(result.index.view(""i8""), dtype=self.dtype), name=result.index.name
)
return Series(result._values, index=index, name=result.name)
def map(self, mapper):
# TODO(GH-23179): Add ExtensionArray.map
# Need to figure out if we want ExtensionArray.map first.
# If so, then we can refactor IndexOpsMixin._map_values to
# a standalone function and call from here..
# Else, just rewrite _map_infer_values to do the right thing.
from pandas import Index
return Index(self).map(mapper).array
# ------------------------------------------------------------------
# Null Handling
def isna(self):
return self._isnan
@property  # NB: override with cache_readonly in immutable subclasses
def _isnan(self):
""""""
return if each value is nan
""""""
return self.asi8 == iNaT
@property  # NB: override with cache_readonly in immutable subclasses
def _hasnans(self):
""""""
return if I have any nans; enables various perf speedups
""""""
return bool(self._isnan.any())
def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):
""""""
Parameters
----------
result : a ndarray
fill_value : object, default iNaT
convert : str, dtype or None
Returns
-------
result : ndarray with values replace by the fill_value
mask the result if needed, convert to the provided dtype if its not
None
This is an internal routine.
""""""
if self._hasnans:
if convert:
result = result.astype(convert)
if fill_value is None:
fill_value = np.nan
result[self._isnan] = fill_value
return result
def fillna(self, value=None, method=None, limit=None):
# TODO(GH-20300): remove this
# Just overriding to ensure that we avoid an astype(object).
# Either 20300 or a `_values_for_fillna` would avoid this duplication.
if isinstance(value, ABCSeries):
value = value.array
value, method = validate_fillna_kwargs(value, method)
mask = self.isna()
if is_array_like(value):
if len(value) != len(self):
raise ValueError(
f""Length of 'value' does not match. Got ({len(value)}) ""
f"" expected {len(self)}""
)
value = value[mask]
if mask.any():
if method is not None:
if method == ""pad"":
func = missing.pad_1d
else:
func = missing.backfill_1d
values = self._data
if not is_period_dtype(self):
# For PeriodArray self._data is i8, which gets copied
#  by `func`.  Otherwise we need to make a copy manually
# to avoid modifying `self` in-place.
values = values.copy()
new_values = func(values, limit=limit, mask=mask)
if is_datetime64tz_dtype(self):
# we need to pass int64 values to the constructor to avoid
#  re-localizing incorrectly
new_values = new_values.view(""i8"")
new_values = type(self)(new_values, dtype=self.dtype)
else:
# fill with value
new_values = self.copy()
new_values[mask] = value
else:
new_values = self.copy()
return new_values
# ------------------------------------------------------------------
# Frequency Properties/Methods
Return the frequency object if it is set, otherwise None.
return self._freq
@freq.setter
def freq(self, value):
if value is not None:
value = frequencies.to_offset(value)
self._validate_frequency(self, value)
self._freq = value
@property  # NB: override with cache_readonly in immutable subclasses
def inferred_freq(self):
""""""
Tryies to return a string representing a frequency guess,
generated by infer_freq.  Returns None if it can't autodetect the
frequency.
""""""
if self.ndim != 1:
return None
try:
return frequencies.infer_freq(self)
except ValueError:
return None
@property  # NB: override with cache_readonly in immutable subclasses
def _resolution(self):
return frequencies.Resolution.get_reso_from_freq(self.freqstr)
@property  # NB: override with cache_readonly in immutable subclasses
def resolution(self):
""""""
Returns day, hour, minute, second, millisecond or microsecond
""""""
return frequencies.Resolution.get_str(self._resolution)
@classmethod
def _validate_frequency(cls, index, freq, **kwargs):
""""""
Validate that a frequency is compatible with the values of a given
Datetime Array/Index or Timedelta Array/Index
Parameters
----------
index : DatetimeIndex or TimedeltaIndex
The index on which to determine if the given frequency is valid
freq : DateOffset
The frequency to validate
""""""
if is_period_dtype(cls):
# Frequency validation is not meaningful for Period Array/Index
return None
inferred = index.inferred_freq
if index.size == 0 or inferred == freq.freqstr:
return None
try:
on_freq = cls._generate_range(
start=index[0], end=None, periods=len(index), freq=freq, **kwargs
)
if not np.array_equal(index.asi8, on_freq.asi8):
raise ValueError
except ValueError as e:
if ""non-fixed"" in str(e):
# non-fixed frequencies are not meaningful for timedelta64;
#  we retain that error message
raise e
# GH#11587 the main way this is reached is if the `np.array_equal`
#  check above is False.  This can also be reached if index[0]
#  is `NaT`, in which case the call to `cls._generate_range` will
#  raise a ValueError, which we re-raise with a more targeted
#  message.
raise ValueError(
f""Inferred frequency {inferred} from passed values ""
f""does not conform to passed frequency {freq.freqstr}""
) from e
# monotonicity/uniqueness properties are called via frequencies.infer_freq,
#  see GH#23789
@property
def _is_monotonic_increasing(self):
return algos.is_monotonic(self.asi8, timelike=True)[0]
@property
def _is_monotonic_decreasing(self):
return algos.is_monotonic(self.asi8, timelike=True)[1]
@property
def _is_unique(self):
return len(unique1d(self.asi8)) == len(self)
# ------------------------------------------------------------------
# Arithmetic Methods
_create_comparison_method = classmethod(_datetimelike_array_cmp)
# pow is invalid for all three subclasses; TimedeltaArray will override
#  the multiplication and division ops
__pow__ = make_invalid_op(""__pow__"")
__rpow__ = make_invalid_op(""__rpow__"")
__mul__ = make_invalid_op(""__mul__"")
__rmul__ = make_invalid_op(""__rmul__"")
__truediv__ = make_invalid_op(""__truediv__"")
__rtruediv__ = make_invalid_op(""__rtruediv__"")
__floordiv__ = make_invalid_op(""__floordiv__"")
__rfloordiv__ = make_invalid_op(""__rfloordiv__"")
__mod__ = make_invalid_op(""__mod__"")
__rmod__ = make_invalid_op(""__rmod__"")
__divmod__ = make_invalid_op(""__divmod__"")
__rdivmod__ = make_invalid_op(""__rdivmod__"")
def _add_datetimelike_scalar(self, other):
# Overridden by TimedeltaArray
raise TypeError(f""cannot add {type(self).__name__} and {type(other).__name__}"")
_add_datetime_arraylike = _add_datetimelike_scalar
def _sub_datetimelike_scalar(self, other):
# Overridden by DatetimeArray
assert other is not NaT
raise TypeError(f""cannot subtract a datelike from a {type(self).__name__}"")
_sub_datetime_arraylike = _sub_datetimelike_scalar
def _sub_period(self, other):
# Overridden by PeriodArray
raise TypeError(f""cannot subtract Period from a {type(self).__name__}"")
def _add_offset(self, offset):
raise AbstractMethodError(self)
def _add_timedeltalike_scalar(self, other):
""""""
Add a delta of a timedeltalike
Returns
-------
Same type as self
""""""
if isna(other):
# i.e np.timedelta64(""NaT""), not recognized by delta_to_nanoseconds
new_values = np.empty(self.shape, dtype=""i8"")
new_values[:] = iNaT
return type(self)(new_values, dtype=self.dtype)
inc = delta_to_nanoseconds(other)
new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(
""i8""
)
new_values = self._maybe_mask_results(new_values)
new_freq = None
if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):
# adding a scalar preserves freq
new_freq = self.freq
return type(self)(new_values, dtype=self.dtype, freq=new_freq)
def _add_timedelta_arraylike(self, other):
""""""
Add a delta of a TimedeltaIndex
Returns
-------
Same type as self
""""""
# overridden by PeriodArray
if len(self) != len(other):
raise ValueError(""cannot add indices of unequal length"")
if isinstance(other, np.ndarray):
# ndarray[timedelta64]; wrap in TimedeltaIndex for op
from pandas.core.arrays import TimedeltaArray
other = TimedeltaArray._from_sequence(other)
self_i8 = self.asi8
other_i8 = other.asi8
new_values = checked_add_with_arr(
self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan
)
if self._hasnans or other._hasnans:
mask = (self._isnan) | (other._isnan)
new_values[mask] = iNaT
return type(self)(new_values, dtype=self.dtype)
def _add_nat(self):
Add pd.NaT to self
""""""
if is_period_dtype(self):
raise TypeError(
f""Cannot add {type(self).__name__} and {type(NaT).__name__}""
)
# GH#19124 pd.NaT is treated like a timedelta for both timedelta
# and datetime dtypes
result = np.zeros(self.shape, dtype=np.int64)
result.fill(iNaT)
return type(self)(result, dtype=self.dtype, freq=None)
def _sub_nat(self):
""""""
Subtract pd.NaT from self
""""""
# GH#19124 Timedelta - datetime is not in general well-defined.
# We make an exception for pd.NaT, which in this case quacks
# like a timedelta.
# For datetime64 dtypes by convention we treat NaT as a datetime, so
# this subtraction returns a timedelta64 dtype.
# For period dtype, timedelta64 is a close-enough return dtype.
result = np.zeros(self.shape, dtype=np.int64)
result.fill(iNaT)
return result.view(""timedelta64[ns]"")
def _sub_period_array(self, other):
""""""
Subtract a Period Array/Index from self.  This is only valid if self
is itself a Period Array/Index, raises otherwise.  Both objects must
have the same frequency.
other : PeriodIndex or PeriodArray
Returns
-------
result : np.ndarray[object]
Array of DateOffset objects; nulls represented by NaT.
""""""
if not is_period_dtype(self):
raise TypeError(
f""cannot subtract {other.dtype}-dtype from {type(self).__name__}""
)
if self.freq != other.freq:
msg = DIFFERENT_FREQ.format(
cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr
)
raise IncompatibleFrequency(msg)
new_values = checked_add_with_arr(
self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan
)
new_values = np.array([self.freq.base * x for x in new_values])
if self._hasnans or other._hasnans:
mask = (self._isnan) | (other._isnan)
new_values[mask] = NaT
return new_values
def _addsub_object_array(self, other: np.ndarray, op):
""""""
Add or subtract array-like of DateOffset objects
Parameters
----------
other : np.ndarray[object]
op : {operator.add, operator.sub}
result : same class as self
assert op in [operator.add, operator.sub]
if len(other) == 1:
return op(self, other[0])
warnings.warn(
""Adding/subtracting array of DateOffsets to ""
f""{type(self).__name__} not vectorized"",
PerformanceWarning,
)
# Caller is responsible for broadcasting if necessary
assert self.shape == other.shape, (self.shape, other.shape)
res_values = op(self.astype(""O""), np.array(other))
result = array(res_values.ravel())
result = extract_array(result, extract_numpy=True).reshape(self.shape)
return result
def _time_shift(self, periods, freq=None):
""""""
Shift each value by `periods`.
Note this is different from ExtensionArray.shift, which
shifts the *position* of each element, padding the end with
missing values.
Parameters
----------
periods : int
Number of periods to shift by.
freq : pandas.DateOffset, pandas.Timedelta, or str
Frequency increment to shift by.
""""""
if freq is not None and freq != self.freq:
if isinstance(freq, str):
freq = frequencies.to_offset(freq)
offset = periods * freq
result = self + offset
if periods == 0:
# immutable so OK
return self.copy()
if self.freq is None:
raise NullFrequencyError(""Cannot shift with no freq"")
start = self[0] + periods * self.freq
end = self[-1] + periods * self.freq
# Note: in the DatetimeTZ case, _generate_range will infer the
#  appropriate timezone from `start` and `end`, so tz does not need
#  to be passed explicitly.
return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
@unpack_zerodim_and_defer(""__add__"")
def __add__(self, other):
# scalar others
if other is NaT:
result = self._add_nat()
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
result = self._add_timedeltalike_scalar(other)
elif isinstance(other, DateOffset):
# specifically _not_ a Tick
result = self._add_offset(other)
elif isinstance(other, (datetime, np.datetime64)):
result = self._add_datetimelike_scalar(other)
elif lib.is_integer(other):
# This check must come after the check for np.timedelta64
# as is_integer returns True for these
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._time_shift(other)
# array-like others
elif is_timedelta64_dtype(other):
# TimedeltaIndex, ndarray[timedelta64]
result = self._add_timedelta_arraylike(other)
elif is_object_dtype(other):
# e.g. Array/Index of DateOffset objects
result = self._addsub_object_array(other, operator.add)
elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
# DatetimeIndex, ndarray[datetime64]
return self._add_datetime_arraylike(other)
elif is_integer_dtype(other):
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._addsub_int_array(other, operator.add)
else:
# Includes Categorical, other ExtensionArrays
# For PeriodDtype, if self is a TimedeltaArray and other is a
#  PeriodArray with  a timedelta-like (i.e. Tick) freq, this
#  operation is valid.  Defer to the PeriodArray implementation.
#  In remaining cases, this will end up raising TypeError.
return NotImplemented
if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
from pandas.core.arrays import TimedeltaArray
return TimedeltaArray(result)
return result
def __radd__(self, other):
# alias for __add__
return self.__add__(other)
@unpack_zerodim_and_defer(""__sub__"")
def __sub__(self, other):
# scalar others
if other is NaT:
result = self._sub_nat()
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
result = self._add_timedeltalike_scalar(-other)
elif isinstance(other, DateOffset):
# specifically _not_ a Tick
result = self._add_offset(-other)
elif isinstance(other, (datetime, np.datetime64)):
result = self._sub_datetimelike_scalar(other)
elif lib.is_integer(other):
# This check must come after the check for np.timedelta64
# as is_integer returns True for these
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._time_shift(-other)
elif isinstance(other, Period):
result = self._sub_period(other)
# array-like others
elif is_timedelta64_dtype(other):
# TimedeltaIndex, ndarray[timedelta64]
result = self._add_timedelta_arraylike(-other)
elif is_object_dtype(other):
# e.g. Array/Index of DateOffset objects
result = self._addsub_object_array(other, operator.sub)
elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
# DatetimeIndex, ndarray[datetime64]
result = self._sub_datetime_arraylike(other)
elif is_period_dtype(other):
# PeriodIndex
result = self._sub_period_array(other)
elif is_integer_dtype(other):
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._addsub_int_array(other, operator.sub)
# Includes ExtensionArrays, float_dtype
return NotImplemented
if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
from pandas.core.arrays import TimedeltaArray
return TimedeltaArray(result)
return result
def __rsub__(self, other):
if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
# ndarray[datetime64] cannot be subtracted from self, so
# we need to wrap in DatetimeArray/Index and flip the operation
if lib.is_scalar(other):
# i.e. np.datetime64 object
return Timestamp(other) - self
if not isinstance(other, DatetimeLikeArrayMixin):
# Avoid down-casting DatetimeIndex
from pandas.core.arrays import DatetimeArray
other = DatetimeArray(other)
return other - self
elif (
is_datetime64_any_dtype(self.dtype)
and hasattr(other, ""dtype"")
and not is_datetime64_any_dtype(other.dtype)
):
# GH#19959 datetime - datetime is well-defined as timedelta,
# but any other type - datetime is not well-defined.
raise TypeError(
f""cannot subtract {type(self).__name__} from {type(other).__name__}""
)
elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):
# TODO: Can we simplify/generalize these cases at all?
raise TypeError(f""cannot subtract {type(self).__name__} from {other.dtype}"")
elif is_timedelta64_dtype(self.dtype):
if lib.is_integer(other) or is_integer_dtype(other):
# need to subtract before negating, since that flips freq
# -self flips self.freq, messing up results
return -(self - other)
return (-self) + other
return -(self - other)
def __iadd__(self, other):
result = self + other
self[:] = result[:]
if not is_period_dtype(self):
# restore freq, which is invalidated by setitem
self._freq = result._freq
return self
def __isub__(self, other):
result = self - other
self[:] = result[:]
if not is_period_dtype(self):
# restore freq, which is invalidated by setitem
self._freq = result._freq
return self
# --------------------------------------------------------------
# Reductions
def _reduce(self, name, axis=0, skipna=True, **kwargs):
op = getattr(self, name, None)
if op:
return op(skipna=skipna, **kwargs)
return super()._reduce(name, skipna, **kwargs)
def min(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the minimum value of the Array or minimum along
an axis.
See Also
--------
numpy.ndarray.min
Index.min : Return the minimum value in an Index.
Series.min : Return the minimum value in a Series.
nv.validate_min(args, kwargs)
nv.validate_minmax_axis(axis)
result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())
if isna(result):
# Period._from_ordinal does not handle np.nan gracefully
return NaT
return self._box_func(result)
def max(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the maximum value of the Array or maximum along
an axis.
See Also
--------
numpy.ndarray.max
Index.max : Return the maximum value in an Index.
Series.max : Return the maximum value in a Series.
""""""
# TODO: skipna is broken with max.
# See https://github.com/pandas-dev/pandas/issues/24265
nv.validate_max(args, kwargs)
nv.validate_minmax_axis(axis)
mask = self.isna()
if skipna:
values = self[~mask].asi8
elif mask.any():
return NaT
else:
values = self.asi8
if not len(values):
# short-circuit for empty max / min
return NaT
result = nanops.nanmax(values, skipna=skipna)
# Don't have to worry about NA `result`, since no NA went in.
return self._box_func(result)
def mean(self, skipna=True):
Return the mean value of the Array.
.. versionadded:: 0.25.0
skipna : bool, default True
Whether to ignore any NaT elements.
scalar
Timestamp or Timedelta.
See Also
--------
numpy.ndarray.mean : Returns the average of array elements along a given axis.
Series.mean : Return the mean value in a Series.
Notes
-----
mean is only defined for Datetime and Timedelta dtypes, not for Period.
""""""
if is_period_dtype(self):
# See discussion in GH#24757
raise TypeError(
f""mean is not implemented for {type(self).__name__} since the ""
""meaning is ambiguous.  An alternative is ""
""obj.to_timestamp(how='start').mean()""
)
mask = self.isna()
if skipna:
values = self[~mask]
elif mask.any():
return NaT
else:
values = self
if not len(values):
# short-circuit for empty max / min
return NaT
result = nanops.nanmean(values.view(""i8""), skipna=skipna)
# Don't have to worry about NA `result`, since no NA went in.
return self._box_func(result)
""""""
If a `periods` argument is passed to the Datetime/Timedelta Array/Index
constructor, cast it to an integer.
Parameters
----------
periods : None, float, int
Returns
-------
periods : None or int
Raises
------
TypeError
if periods is None, float, or int
""""""
if periods is not None:
if lib.is_float(periods):
periods = int(periods)
elif not lib.is_integer(periods):
raise TypeError(f""periods must be a number, got {periods}"")
return periods
""""""
Check that the `closed` argument is among [None, ""left"", ""right""]
Parameters
----------
closed : {None, ""left"", ""right""}
Returns
-------
left_closed : bool
right_closed : bool
Raises
------
ValueError : if argument is not among valid values
""""""
left_closed = False
right_closed = False
if closed is None:
left_closed = True
right_closed = True
elif closed == ""left"":
left_closed = True
elif closed == ""right"":
right_closed = True
else:
raise ValueError(""Closed has to be either 'left', 'right' or None"")
return left_closed, right_closed
""""""
If the user passes a freq and another freq is inferred from passed data,
require that they match.
Parameters
----------
freq : DateOffset or None
inferred_freq : DateOffset or None
freq_infer : bool
Returns
-------
freq : DateOffset or None
freq_infer : bool
Notes
-----
We assume at this point that `maybe_infer_freq` has been called, so
`freq` is either a DateOffset object or None.
""""""
if inferred_freq is not None:
if freq is not None and freq != inferred_freq:
raise ValueError(
f""Inferred frequency {inferred_freq} from passed ""
""values does not conform to passed frequency ""
f""{freq.freqstr}""
)
elif freq is None:
freq = inferred_freq
freq_infer = False
return freq, freq_infer
""""""
Comparing a DateOffset to the string ""infer"" raises, so we need to
be careful about comparisons.  Make a dummy variable `freq_infer` to
signify the case where the given freq is ""infer"" and set freq to None
to avoid comparison trouble later on.
Parameters
----------
freq : {DateOffset, None, str}
Returns
-------
freq : {DateOffset, None}
freq_infer : bool
Whether we should inherit the freq of passed data.
""""""
freq_infer = False
if not isinstance(freq, DateOffset):
# if a passed freq is None, don't infer automatically
if freq != ""infer"":
freq = frequencies.to_offset(freq)
else:
freq_infer = True
freq = None
return freq, freq_infer","ensure_int64,
is_bool_dtype,
is_integer,
is_scalar,
ExtensionIndex,
inherit_names,
make_wrapped_arith_op,
""""""
Create the join wrapper methods.
""""""
@staticmethod  # type: ignore
def wrapper(left, right):
if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
left = left.view(""i8"")
if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
right = right.view(""i8"")
results = joinf(left, right)
if with_indexers:
# dtype should be timedelta64[ns] for TimedeltaIndex
#  and datetime64[ns] for DatetimeIndex
dtype = left.dtype.base
join_index, left_indexer, right_indexer = results
join_index = join_index.view(dtype)
return join_index, left_indexer, right_indexer
return results
return wrapper
""""""
Dispatch the operation to the underlying ExtensionArray, and infer
the appropriate frequency for the result.
""""""
meth = make_wrapped_arith_op(opname)
def wrapped(self, other):
result = meth(self, other)
if result is NotImplemented:
return NotImplemented
new_freq = self._get_addsub_freq(other, result)
result._freq = new_freq
return result
wrapped.__name__ = opname
return wrapped
[""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
DatetimeLikeArrayMixin,
cache=True,
[""mean"", ""asi8"", ""_box_func""], DatetimeLikeArrayMixin,
""""""
Common ops mixin to support a unified interface datetimelike Index.
""""""
_data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
freq: Optional[DateOffset]
freqstr: Optional[str]
_resolution: int
_bool_ops: List[str] = []
_field_ops: List[str] = []
hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
_hasnans = hasnans  # for index / array -agnostic code
@property
def is_all_dates(self) -> bool:
return True
# ------------------------------------------------------------------------
# Abstract data attributes
def values(self):
# Note: PeriodArray overrides this to return an ndarray of objects.
return self._data._data
def __array_wrap__(self, result, context=None):
Gets called after a ufunc.
result = lib.item_from_zerodim(result)
if is_bool_dtype(result) or lib.is_scalar(result):
return result
attrs = self._get_attributes_dict()
if not is_period_dtype(self) and attrs[""freq""]:
# no need to infer if freq is None
attrs[""freq""] = ""infer""
return Index(result, **attrs)
# ------------------------------------------------------------------------
def equals(self, other) -> bool:
Determines if two Index objects contain the same elements.
if self.is_(other):
return True
if not isinstance(other, ABCIndexClass):
return False
elif not isinstance(other, type(self)):
try:
other = type(self)(other)
except (ValueError, TypeError, OverflowError):
# e.g.
#  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
#  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
#  OverflowError -> Index([very_large_timedeltas])
return False
if not is_dtype_equal(self.dtype, other.dtype):
# have different timezone
return False
return np.array_equal(self.asi8, other.asi8)
@Appender(Index.__contains__.__doc__)
def __contains__(self, key: Any) -> bool:
hash(key)
try:
res = self.get_loc(key)
except (KeyError, TypeError, ValueError):
return False
return bool(
is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))
)
def sort_values(self, return_indexer=False, ascending=True):
Return sorted copy of Index.
if return_indexer:
_as = self.argsort()
if not ascending:
_as = _as[::-1]
sorted_index = self.take(_as)
return sorted_index, _as
else:
# NB: using asi8 instead of _data matters in numpy 1.18
#  because the treatment of NaT has been changed to put NaT last
#  instead of first.
sorted_values = np.sort(self.asi8)
freq = self.freq
if freq is not None and not is_period_dtype(self):
if freq.n > 0 and not ascending:
freq = freq * -1
elif freq.n < 0 and ascending:
freq = freq * -1
if not ascending:
sorted_values = sorted_values[::-1]
arr = type(self._data)._simple_new(
sorted_values, dtype=self.dtype, freq=freq
)
return type(self)._simple_new(arr, name=self.name)
@Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
nv.validate_take(tuple(), kwargs)
indices = ensure_int64(indices)
maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
if isinstance(maybe_slice, slice):
return self[maybe_slice]
return ExtensionIndex.take(
self, indices, axis, allow_fill, fill_value, **kwargs
)
@doc(IndexOpsMixin.searchsorted, klass=""Datetime-like Index"")
def searchsorted(self, value, side=""left"", sorter=None):
if isinstance(value, str):
raise TypeError(
""searchsorted requires compatible dtype or scalar, ""
f""not {type(value).__name__}""
)
if isinstance(value, Index):
value = value._data
return self._data.searchsorted(value, side=side, sorter=sorter)
_can_hold_na = True
_na_value = NaT
""""""The expected NA value to use with this index.""""""
def _convert_tolerance(self, tolerance, target):
tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
if target.size != tolerance.size and tolerance.size > 1:
raise ValueError(""list-like tolerance size must match target index size"")
return tolerance
def tolist(self) -> List:
""""""
Return a list of the underlying data.
""""""
return list(self.astype(object))
def min(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the minimum value of the Index or minimum along
an axis.
See Also
numpy.ndarray.min
Series.min : Return the minimum value in a Series.
nv.validate_min(args, kwargs)
nv.validate_minmax_axis(axis)
if not len(self):
return self._na_value
i8 = self.asi8
try:
# quick check
if len(i8) and self.is_monotonic:
if i8[0] != iNaT:
return self._box_func(i8[0])
if self.hasnans:
if skipna:
min_stamp = self[~self._isnan].asi8.min()
else:
return self._na_value
else:
min_stamp = i8.min()
return self._box_func(min_stamp)
except ValueError:
return self._na_value
def argmin(self, axis=None, skipna=True, *args, **kwargs):
Returns the indices of the minimum values along an axis.
See `numpy.ndarray.argmin` for more information on the
`axis` parameter.
See Also
--------
numpy.ndarray.argmin
nv.validate_argmin(args, kwargs)
nv.validate_minmax_axis(axis)
i8 = self.asi8
if self.hasnans:
mask = self._isnan
if mask.all() or not skipna:
return -1
i8 = i8.copy()
i8[mask] = np.iinfo(""int64"").max
return i8.argmin()
def max(self, axis=None, skipna=True, *args, **kwargs):
Return the maximum value of the Index or maximum along
an axis.
See Also
--------
numpy.ndarray.max
Series.max : Return the maximum value in a Series.
""""""
nv.validate_max(args, kwargs)
nv.validate_minmax_axis(axis)
if not len(self):
return self._na_value
i8 = self.asi8
try:
# quick check
if len(i8) and self.is_monotonic:
if i8[-1] != iNaT:
return self._box_func(i8[-1])
if self.hasnans:
if skipna:
max_stamp = self[~self._isnan].asi8.max()
else:
return self._na_value
else:
max_stamp = i8.max()
return self._box_func(max_stamp)
except ValueError:
return self._na_value
def argmax(self, axis=None, skipna=True, *args, **kwargs):
Returns the indices of the maximum values along an axis.
See `numpy.ndarray.argmax` for more information on the
`axis` parameter.
See Also
--------
numpy.ndarray.argmax
nv.validate_argmax(args, kwargs)
nv.validate_minmax_axis(axis)
i8 = self.asi8
if self.hasnans:
mask = self._isnan
if mask.all() or not skipna:
return -1
i8 = i8.copy()
i8[mask] = 0
return i8.argmax()
# --------------------------------------------------------------------
# Rendering Methods
def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
return header + list(self._format_native_types(na_rep, **kwargs))
def _formatter_func(self):
def _format_attrs(self):
Return a list of tuples of the (attr,formatted_value).
attrs = super()._format_attrs()
for attrib in self._attributes:
if attrib == ""freq"":
freq = self.freqstr
if freq is not None:
freq = repr(freq)
attrs.append((""freq"", freq))
return attrs
# --------------------------------------------------------------------
# Indexing Methods
def _validate_partial_date_slice(self, reso: str):
raise NotImplementedError
def _parsed_string_to_bounds(self, reso: str, parsed: datetime):
raise NotImplementedError
def _partial_date_slice(
self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True
):
Parameters
----------
reso : str
parsed : datetime
use_lhs : bool, default True
use_rhs : bool, default True
slice or ndarray[intp]
self._validate_partial_date_slice(reso)
t1, t2 = self._parsed_string_to_bounds(reso, parsed)
i8vals = self.asi8
unbox = self._data._unbox_scalar
if self.is_monotonic:
if len(self) and (
(use_lhs and t1 < self[0] and t2 < self[0])
or ((use_rhs and t1 > self[-1] and t2 > self[-1]))
):
# we are out of range
raise KeyError
# TODO: does this depend on being monotonic _increasing_?
# a monotonic (sorted) series can be sliced
# Use asi8.searchsorted to avoid re-validating Periods/Timestamps
left = i8vals.searchsorted(unbox(t1), side=""left"") if use_lhs else None
right = i8vals.searchsorted(unbox(t2), side=""right"") if use_rhs else None
return slice(left, right)
lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True
rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True
# try to find the dates
return (lhs_mask & rhs_mask).nonzero()[0]
# --------------------------------------------------------------------
# Arithmetic Methods
def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:
""""""
Find the freq we expect the result of an addition/subtraction operation
to have.
""""""
if is_period_dtype(self.dtype):
if is_period_dtype(result.dtype):
# Only used for ops that stay PeriodDtype
return self.freq
return None
elif self.freq is None:
return None
elif lib.is_scalar(other) and isna(other):
return None
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
new_freq = None
if isinstance(self.freq, Tick):
new_freq = self.freq
return new_freq
elif isinstance(other, DateOffset):
# otherwise just DatetimeArray
return None  # TODO: Should we infer if it matches self.freq * n?
elif isinstance(other, (datetime, np.datetime64)):
return self.freq
elif is_timedelta64_dtype(other):
return None  # TODO: shouldnt we be able to do self.freq + other.freq?
elif is_object_dtype(other):
return None  # TODO: is this quite right?  sometimes we unpack singletons
elif is_datetime64_any_dtype(other):
return None  # TODO: shouldnt we be able to do self.freq + other.freq?
else:
raise NotImplementedError
__add__ = _make_wrapped_arith_op_with_freq(""__add__"")
__sub__ = _make_wrapped_arith_op_with_freq(""__sub__"")
__radd__ = make_wrapped_arith_op(""__radd__"")
__rsub__ = make_wrapped_arith_op(""__rsub__"")
__pow__ = make_wrapped_arith_op(""__pow__"")
__rpow__ = make_wrapped_arith_op(""__rpow__"")
__mul__ = make_wrapped_arith_op(""__mul__"")
__rmul__ = make_wrapped_arith_op(""__rmul__"")
__floordiv__ = make_wrapped_arith_op(""__floordiv__"")
__rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
__mod__ = make_wrapped_arith_op(""__mod__"")
__rmod__ = make_wrapped_arith_op(""__rmod__"")
__divmod__ = make_wrapped_arith_op(""__divmod__"")
__rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
__truediv__ = make_wrapped_arith_op(""__truediv__"")
__rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
def isin(self, values, level=None):
""""""
Compute boolean array of whether each index value is found in the
passed set of values.
values : set or sequence of values
is_contained : ndarray (boolean dtype)
if level is not None:
self._validate_index_level(level)
if not isinstance(values, type(self)):
values = type(self)(values)
except ValueError:
return self.astype(object).isin(values)
return algorithms.isin(self.asi8, values.asi8)
@Appender(Index.where.__doc__)
def where(self, cond, other=None):
values = self.view(""i8"")
try:
other = self._data._validate_where_value(other)
except (TypeError, ValueError) as err:
# Includes tzawareness mismatch and IncompatibleFrequencyError
oth = getattr(other, ""dtype"", other)
raise TypeError(f""Where requires matching dtype, not {oth}"") from err
result = np.where(cond, values, other).astype(""i8"")
arr = type(self._data)._simple_new(result, dtype=self.dtype)
return type(self)._simple_new(arr, name=self.name)
def _summary(self, name=None) -> str:
Return a summarized representation.
name : str
Name to use in the summary representation.
str
Summarized representation of the index.
formatter = self._formatter_func
if len(self) > 0:
index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
else:
index_summary = """"
if name is None:
name = type(self).__name__
result = f""{name}: {len(self)} entries{index_summary}""
if self.freq:
result += f""\nFreq: {self.freqstr}""
# display as values, not quoted
result = result.replace(""'"", """")
return result
def shift(self, periods=1, freq=None):
Shift index by desired number of time frequency increments.
This method is for shifting the values of datetime-like indexes
by a specified time increment a given number of times.
periods : int, default 1
Number of periods (or increments) to shift by,
can be positive or negative.
.. versionchanged:: 0.24.0
freq : pandas.DateOffset, pandas.Timedelta or string, optional
Frequency increment to shift by.
If None, the index is shifted by its own `freq` attribute.
Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.
Returns
-------
pandas.DatetimeIndex
Shifted index.
See Also
--------
Index.shift : Shift values of Index.
PeriodIndex.shift : Shift values of PeriodIndex.
""""""
arr = self._data.view()
arr._freq = self.freq
result = arr._time_shift(periods, freq=freq)
return type(self)(result, name=self.name)
# --------------------------------------------------------------------
# List-like Methods
def delete(self, loc):
new_i8s = np.delete(self.asi8, loc)
freq = None
if is_period_dtype(self):
freq = self.freq
elif is_integer(loc):
if loc in (0, -len(self), -1, len(self) - 1):
freq = self.freq
else:
if is_list_like(loc):
loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
if isinstance(loc, slice) and loc.step in (1, None):
if loc.start in (0, None) or loc.stop in (len(self), None):
freq = self.freq
arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
return type(self)._simple_new(arr, name=self.name)
# --------------------------------------------------------------------
# Join/Set Methods
def _wrap_joined_index(self, joined: np.ndarray, other):
assert other.dtype == self.dtype, (other.dtype, self.dtype)
name = get_op_result_name(self, other)
if is_period_dtype(self.dtype):
freq = self.freq
else:
self = cast(DatetimeTimedeltaMixin, self)
freq = self.freq if self._can_fast_union(other) else None
new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)
return type(self)._simple_new(new_data, name=name)
""""""
Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
but not PeriodIndex
""""""
# Compat for frequency inference, see GH#23789
_is_monotonic_increasing = Index.is_monotonic_increasing
_is_monotonic_decreasing = Index.is_monotonic_decreasing
_is_unique = Index.is_unique
_freq = lib.no_default
In limited circumstances, our freq may differ from that of our _data.
if self._freq is not lib.no_default:
return self._freq
return self._data.freq
def _with_freq(self, freq):
arr = self._data._with_freq(freq)
return type(self)._simple_new(arr, name=self.name)
def _shallow_copy(self, values=None, name: Label = lib.no_default):
name = self.name if name is lib.no_default else name
cache = self._cache.copy() if values is None else {}
if values is None:
values = self._data
if isinstance(values, np.ndarray):
# TODO: We would rather not get here
values = type(self._data)(values, dtype=self.dtype)
result = type(self)._simple_new(values, name=name)
result._cache = cache
return result
# --------------------------------------------------------------------
# Set Operation Methods
@Appender(Index.difference.__doc__)
def difference(self, other, sort=None):
new_idx = super().difference(other, sort=sort)._with_freq(None)
return new_idx
def intersection(self, other, sort=False):
Specialized intersection for DatetimeIndex/TimedeltaIndex.
May be much faster than Index.intersection
other : Same type as self or array-like
sort : False or None, default False
Sort the resulting index if possible.
.. versionadded:: 0.24.0
.. versionchanged:: 0.24.1
Changed the default to ``False`` to match the behaviour
from before 0.24.0.
.. versionchanged:: 0.25.0
The `sort` keyword is added
y : Index or same type as self
self._validate_sort_keyword(sort)
self._assert_can_do_setop(other)
if self.equals(other):
return self._get_reconciled_name_object(other)
if len(self) == 0:
return self.copy()
if len(other) == 0:
return other.copy()
if not isinstance(other, type(self)):
result = Index.intersection(self, other, sort=sort)
if isinstance(result, type(self)):
if result.freq is None:
result = result._with_freq(""infer"")
return result
elif (
other.freq is None
or self.freq is None
or other.freq != self.freq
or not other.freq.is_anchored()
or (not self.is_monotonic or not other.is_monotonic)
):
result = Index.intersection(self, other, sort=sort)
result = result._with_freq(""infer"")
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
else:
left, right = other, self
# after sorting, the intersection always starts with the right index
# and ends with the index of which the last elements is smallest
end = min(left[-1], right[-1])
start = right[0]
if end < start:
return type(self)(data=[], dtype=self.dtype, freq=self.freq)
else:
lslice = slice(*left.slice_locs(start, end))
left_chunk = left._values[lslice]
return self._shallow_copy(left_chunk)
def _can_fast_union(self, other) -> bool:
if not isinstance(other, type(self)):
return False
freq = self.freq
if freq is None or freq != other.freq:
return False
if not self.is_monotonic or not other.is_monotonic:
return False
if len(self) == 0 or len(other) == 0:
return True
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
else:
left, right = other, self
right_start = right[0]
left_end = left[-1]
# Only need to ""adjoin"", not overlap
try:
return (right_start == left_end + freq) or right_start in left
except ValueError:
# if we are comparing a freq that does not propagate timezones
# this will raise
return False
def _fast_union(self, other, sort=None):
if len(other) == 0:
return self.view(type(self))
if len(self) == 0:
return other.view(type(self))
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
elif sort is False:
# TDIs are not in the ""correct"" order and we don't want
#  to sort but want to remove overlaps
left, right = self, other
left_start = left[0]
loc = right.searchsorted(left_start, side=""left"")
right_chunk = right._values[:loc]
dates = concat_compat((left._values, right_chunk))
# TODO: can we infer that it has self.freq?
result = self._shallow_copy(dates)._with_freq(""infer"")
return result
left, right = other, self
left_end = left[-1]
right_end = right[-1]
# concatenate
if left_end < right_end:
loc = right.searchsorted(left_end, side=""right"")
right_chunk = right._values[loc:]
dates = concat_compat([left._values, right_chunk])
# TODO: can we infer that it has self.freq?
result = self._shallow_copy(dates)._with_freq(""infer"")
return result
else:
return left
def _union(self, other, sort):
if not len(other) or self.equals(other) or not len(self):
return super()._union(other, sort=sort)
# We are called by `union`, which is responsible for this validation
assert isinstance(other, type(self))
this, other = self._maybe_utc_convert(other)
if this._can_fast_union(other):
result = this._fast_union(other, sort=sort)
if result.freq is None:
result = result._with_freq(""infer"")
return result
i8self = Int64Index._simple_new(self.asi8, name=self.name)
i8other = Int64Index._simple_new(other.asi8, name=other.name)
i8result = i8self._union(i8other, sort=sort)
result = type(self)(i8result, dtype=self.dtype, freq=""infer"")
return result
# --------------------------------------------------------------------
# Join Methods
_join_precedence = 10
_inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
_outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
_left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
_left_indexer_unique = _join_i8_wrapper(
libjoin.left_join_indexer_unique, with_indexers=False
)
def join(
self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
):
See Index.join
""""""
if self._is_convertible_to_index_for_join(other):
try:
other = type(self)(other)
except (TypeError, ValueError):
pass
this, other = self._maybe_utc_convert(other)
return Index.join(
this,
other,
how=how,
level=level,
return_indexers=return_indexers,
sort=sort,
)
def _maybe_utc_convert(self, other):
this = self
if not hasattr(self, ""tz""):
return this, other
if isinstance(other, type(self)):
if self.tz is not None:
if other.tz is None:
raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
elif other.tz is not None:
raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
if not timezones.tz_compare(self.tz, other.tz):
this = self.tz_convert(""UTC"")
other = other.tz_convert(""UTC"")
return this, other
@classmethod
def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
""""""
return a boolean whether I can attempt conversion to a
DatetimeIndex/TimedeltaIndex
""""""
if isinstance(other, cls):
return False
elif len(other) > 0 and other.inferred_type not in (
""floating"",
""mixed-integer"",
""integer"",
""integer-na"",
""mixed-integer-float"",
""mixed"",
):
return True
return False
# --------------------------------------------------------------------
# List-Like Methods
def insert(self, loc, item):
Make new Index inserting new item at location
loc : int
item : object
if not either a Python datetime or a numpy integer-like, returned
Index dtype will be object rather than datetime.
new_index : Index
""""""
if isinstance(item, str):
# TODO: Why are strings special?
# TODO: Should we attempt _scalar_from_string?
return self.astype(object).insert(loc, item)
item = self._data._validate_insert_value(item)
freq = None
# check freq can be preserved on edge cases
if self.freq is not None:
if self.size:
if item is NaT:
pass
elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:
freq = self.freq
elif (loc == len(self)) and item - self.freq == self[-1]:
freq = self.freq
else:
# Adding a single item to an empty index may preserve freq
if self.freq.is_on_offset(item):
freq = self.freq
item = self._data._unbox_scalar(item)
new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])
arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
return type(self)._simple_new(arr, name=self.name)
def test_pickle_after_set_freq(self):
tdi = timedelta_range(""1 day"", periods=4, freq=""s"")
tdi = tdi._with_freq(None)
res = tm.round_trip_pickle(tdi)
tm.assert_index_equal(res, tdi)"
pandas,150,if left_value != right_value:,if np.any(left_value != right_value):
pandas,151,"values = self._ndarray
t = np.result_type(value, values)
if t != self._ndarray.dtype:
values = values.astype(t, casting=""safe"")
values[key] = value
self._dtype = PandasDtype(t)
self._ndarray = values
else:
self._ndarray[key] = value","value = np.asarray(value, dtype=self._ndarray.dtype)
self._ndarray[key] = value"
pandas,152,to_concat = [self] + to_append,"to_concat = [self]
to_concat.extend(to_append)"
pandas,153,values = values.astype(str),"itemsize = writers.word_len(na_rep)
values = values.astype(""<U{size}"".format(size=itemsize))"
pandas,154,"result_sz = len(obj.values)
cython_dtype = obj.values.dtype
vals = obj.values
mask = isna(obj.values).view(np.uint8)
result = algorithms.take_nd(obj.values, result)","values = obj._data._values
result_sz = len(values)
cython_dtype = values.dtype
vals = values
mask = isna(values).view(np.uint8)
result = algorithms.take_nd(values, result)"
pandas,155,return self.obj.index,"if self.axis == 0:
return self.obj.index
elif self.axis == 1:
return self.obj.columns"
pandas,156,"for col, series in this.items():
new_data[col] = func(series.values, other.values)
new_data[col] = func(left[col], float(right[col]))","for col in this.columns:
new_data[col] = func(this[col], other)
new_data[col] = func(left[col], right[col])"
pandas,157,"is_datetime64_dtype,
if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):",if is_datetimelike(lt):
pandas,158,"non_mapping = is_scalar(index) or (
is_list_like(index) and not is_dict_like(index)
)
if non_mapping:
return super().rename(index=index, **kwargs)","if callable(index) or is_dict_like(index):
return super().rename(index=index, **kwargs)
else:"
pandas,159,"result = _arith_op(this.values, other.values)","with np.errstate(all=""ignore""):
result = _arith_op(this.values, other.values)
result = dispatch_fill_zeros(func, this.values, other.values, result)"
pandas,16,"new_freq = self._get_addsub_freq(other)
def _get_addsub_freq(self, other) -> Optional[DateOffset]:
# Only used for ops that stay PeriodDtype
return self.freq","new_freq = self._get_addsub_freq(other, result)
def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:
if is_period_dtype(result.dtype):
# Only used for ops that stay PeriodDtype
return self.freq
return None"
pandas,160,"if hasattr(o, ""dtypes""):
elif isinstance(o, np.ndarray):","# Series implements dtypes, check for dimension count as well
if hasattr(o, ""dtypes"") and o.ndim > 1:
# ndarray and Series Case
elif hasattr(o, ""dtype""):"
pandas,161,"indexer = np.where(values_codes != -1)
codes[indexer] = values_codes[values_codes != -1]","indexer = np.where(codes == -1)
codes[indexer] = values_codes[indexer]"
pandas,162,"column_margin = table.loc[:, margins_name].drop(margins_name)
index_margin = table.loc[margins_name, :].drop(margins_name)
table = table.drop(margins_name, axis=1).drop(margins_name)
# to keep index and columns names
table_index_names = table.index.names
table_columns_names = table.columns.names
table.index.names = table_index_names
table.columns.names = table_columns_names","# keep index and column of pivoted table
table_index = table.index
table_columns = table.columns
# check if margin name is in (for MI cases) or equal to last
# index/column and save the column and index margin
if (margins_name not in table.iloc[-1, :].name) | (
margins_name != table.iloc[:, -1].name
):
raise ValueError(""{} not in pivoted DataFrame"".format(margins_name))
column_margin = table.iloc[:-1, -1]
index_margin = table.iloc[-1, :-1]
# keep the core table
table = table.iloc[:-1, :-1]
table.columns = table_columns
table.index = table_index
table.index = table_index
table.columns = table_columns"
pandas,163,"# Always convert inf to nan
values[np.isinf(values)] = np.NaN","# Convert inf to nan for C funcs
inf = np.isinf(values)
if inf.any():
values = np.where(inf, np.nan, values)"
pandas,164,,
pandas,165,"if isinstance(other, (ABCSeries, ABCDataFrame)):
if isinstance(other, (ABCSeries, ABCDataFrame)):
if is_datetime64_dtype(other) and is_timedelta64_dtype(self):","if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):
if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):
if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):
# ---------------------------------------------------------------
# Unsorted
def test_parr_add_sub_index(self):
# Check that PeriodArray defers to Index on arithmetic ops
pi = pd.period_range(""2000-12-31"", periods=3)
parr = pi.array
result = parr - pi
expected = pi - pi
tm.assert_index_equal(result, expected)
def test_tda_add_sub_index(self):
# Check that TimedeltaArray defers to Index on arithmetic ops
tdi = TimedeltaIndex([""1 days"", pd.NaT, ""2 days""])
tda = tdi.array
dti = pd.date_range(""1999-12-31"", periods=3, freq=""D"")
result = tda + dti
expected = tdi + dti
tm.assert_index_equal(result, expected)
result = tda + tdi
expected = tdi + tdi
tm.assert_index_equal(result, expected)
result = tda - tdi
expected = tdi - tdi
tm.assert_index_equal(result, expected)"
pandas,166,"res = concat(frames, axis=1, join=""outer"", verify_integrity=True)
return concat(frames, axis=1, join=how, verify_integrity=True)","res = concat(
frames, axis=1, join=""outer"", verify_integrity=True, sort=sort
)
return concat(
frames, axis=1, join=how, verify_integrity=True, sort=sort
)"
pandas,167,"if isinstance(key, str) and labels.levels[0].is_all_dates:
if isinstance(component, str) and labels.levels[i].is_all_dates:
if idx.is_all_dates:","# whether we support partial string indexing. Overridden
# in DatetimeIndex and PeriodIndex
_supports_partial_string_indexing = False
_supports_partial_string_indexing = True
_supports_partial_string_indexing = True
if isinstance(k, str) and ax._supports_partial_string_indexing:
# partial string indexing, df.loc['2000', 'A']
# should not be considered scalar
return False
if (
isinstance(key, str)
and labels.levels[0]._supports_partial_string_indexing
):
if (
isinstance(component, str)
and labels.levels[i]._supports_partial_string_indexing
):
if idx._supports_partial_string_indexing:"
pandas,168,"obj._check_label_or_level_ambiguity(gpr)
elif obj._is_level_reference(gpr):","obj._check_label_or_level_ambiguity(gpr, axis=axis)
elif obj._is_level_reference(gpr, axis=axis):"
pandas,169,,
pandas,17,"is_categorical_dtype,
is_datetime64_dtype,
is_datetime64tz_dtype,
is_datetime_or_timedelta_dtype,
is_float_dtype,
is_integer_dtype,
is_string_dtype,
is_unsigned_integer_dtype,
pandas_dtype,
""""""
Wrap comparison operations to convert Timestamp/Timedelta/Period-like to
boxed scalars/arrays.
""""""
opname = f""__{op.__name__}__""
nat_result = opname == ""__ne__""
class InvalidComparison(Exception):
pass
def _validate_comparison_value(self, other):
if isinstance(other, str):
try:
# GH#18435 strings get a pass from tzawareness compat
other = self._scalar_from_string(other)
except ValueError:
# failed to parse as Timestamp/Timedelta/Period
raise InvalidComparison(other)
if isinstance(other, self._recognized_scalars) or other is NaT:
other = self._scalar_type(other)
self._check_compatible_with(other)
elif not is_list_like(other):
raise InvalidComparison(other)
elif len(other) != len(self):
raise ValueError(""Lengths must match"")
else:
if isinstance(other, list):
# TODO: could use pd.Index to do inference?
other = np.array(other)
if not isinstance(other, (np.ndarray, type(self))):
raise InvalidComparison(other)
elif is_object_dtype(other.dtype):
pass
elif not type(self)._is_recognized_dtype(other.dtype):
raise InvalidComparison(other)
else:
# For PeriodDType this casting is unnecessary
# TODO: use Index to do inference?
other = type(self)._from_sequence(other)
self._check_compatible_with(other)
return other
@unpack_zerodim_and_defer(opname)
def wrapper(self, other):
try:
other = _validate_comparison_value(self, other)
except InvalidComparison:
return invalid_comparison(self, other, op)
dtype = getattr(other, ""dtype"", None)
if is_object_dtype(dtype):
# We have to use comp_method_OBJECT_ARRAY instead of numpy
#  comparison otherwise it would fail to raise when
#  comparing tz-aware and tz-naive
with np.errstate(all=""ignore""):
result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)
return result
if isinstance(other, self._scalar_type) or other is NaT:
other_i8 = self._unbox_scalar(other)
else:
# Then type(other) == type(self)
other_i8 = other.asi8
result = op(self.asi8, other_i8)
o_mask = isna(other)
if self._hasnans | np.any(o_mask):
result[self._isnan | o_mask] = nat_result
return set_function_name(wrapper, opname, cls)
_data: np.ndarray
@classmethod
def _simple_new(cls, values: np.ndarray, **kwargs):
raise AbstractMethodError(cls)
@property
def _scalar_type(self) -> Type[DatetimeLikeScalar]:
""""""
The scalar associated with this datelike
* PeriodArray : Period
* DatetimeArray : Timestamp
* TimedeltaArray : Timedelta
""""""
raise AbstractMethodError(self)
def _scalar_from_string(
self, value: str
) -> Union[Period, Timestamp, Timedelta, NaTType]:
""""""
Construct a scalar type from a string.
Parameters
----------
value : str
Returns
-------
Period, Timestamp, or Timedelta, or NaT
Whatever the type of ``self._scalar_type`` is.
Notes
-----
This should call ``self._check_compatible_with`` before
unboxing the result.
raise AbstractMethodError(self)
def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:
Unbox the integer value of a scalar `value`.
Parameters
----------
value : Union[Period, Timestamp, Timedelta]
Returns
-------
int
Examples
--------
>>> self._unbox_scalar(Timedelta(""10s""))  # doctest: +SKIP
10000000000
raise AbstractMethodError(self)
def _check_compatible_with(
self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False
) -> None:
Verify that `self` and `other` are compatible.
* DatetimeArray verifies that the timezones (if any) match
* PeriodArray verifies that the freq matches
* Timedelta has no verification
In each case, NaT is considered compatible.
Parameters
----------
other
setitem : bool, default False
For __setitem__ we may have stricter compatibility resrictions than
for comparisons.
Raises
------
Exception
raise AbstractMethodError(self)
""""""
Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.
""""""
@Substitution(
URL=""https://docs.python.org/3/library/datetime.html""
""#strftime-and-strptime-behavior""
)
def strftime(self, date_format):
Convert to Index using specified date_format.
Return an Index of formatted strings specified by date_format, which
supports the same string format as the python standard library. Details
of the string format can be found in `python string format
doc <%(URL)s>`__.
Parameters
----------
date_format : str
Date format string (e.g. ""%%Y-%%m-%%d"").
Returns
-------
ndarray
NumPy ndarray of formatted strings.
See Also
--------
to_datetime : Convert the given argument to datetime.
DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.
DatetimeIndex.round : Round the DatetimeIndex to the specified freq.
DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.
Examples
--------
>>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),
...                     periods=3, freq='s')
>>> rng.strftime('%%B %%d, %%Y, %%r')
Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',
'March 10, 2018, 09:00:02 AM'],
dtype='object')
""""""
result = self._format_native_types(date_format=date_format, na_rep=np.nan)
return result.astype(object)
""""""
Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.
""""""
_round_doc = """"""
Perform {op} operation on the data to the specified `freq`.
Parameters
----------
freq : str or Offset
The frequency level to {op} the index to. Must be a fixed
frequency like 'S' (second) not 'ME' (month end). See
:ref:`frequency aliases <timeseries.offset_aliases>` for
a list of possible `freq` values.
ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
Only relevant for DatetimeIndex:
- 'infer' will attempt to infer fall dst-transition hours based on
order
- bool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)
- 'NaT' will return NaT where there are ambiguous times
- 'raise' will raise an AmbiguousTimeError if there are ambiguous
times.
.. versionadded:: 0.24.0
nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \
A nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.
- 'shift_forward' will shift the nonexistent time forward to the
closest existing time
- 'shift_backward' will shift the nonexistent time backward to the
closest existing time
- 'NaT' will return NaT where there are nonexistent times
- timedelta objects will shift nonexistent times by the timedelta
- 'raise' will raise an NonExistentTimeError if there are
nonexistent times.
.. versionadded:: 0.24.0
Returns
-------
DatetimeIndex, TimedeltaIndex, or Series
Index of the same type for a DatetimeIndex or TimedeltaIndex,
or a Series with the same index for a Series.
Raises
------
ValueError if the `freq` cannot be converted.
Examples
**DatetimeIndex**
>>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')
>>> rng
DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',
'2018-01-01 12:01:00'],
dtype='datetime64[ns]', freq='T')
_round_example = """""">>> rng.round('H')
DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
'2018-01-01 12:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.round(""H"")
0   2018-01-01 12:00:00
1   2018-01-01 12:00:00
2   2018-01-01 12:00:00
dtype: datetime64[ns]
_floor_example = """""">>> rng.floor('H')
DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',
'2018-01-01 12:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.floor(""H"")
0   2018-01-01 11:00:00
1   2018-01-01 12:00:00
2   2018-01-01 12:00:00
dtype: datetime64[ns]
_ceil_example = """""">>> rng.ceil('H')
DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
'2018-01-01 13:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.ceil(""H"")
0   2018-01-01 12:00:00
1   2018-01-01 12:00:00
2   2018-01-01 13:00:00
dtype: datetime64[ns]
def _round(self, freq, mode, ambiguous, nonexistent):
# round the local times
if is_datetime64tz_dtype(self):
# operate on naive timestamps, then convert back to aware
naive = self.tz_localize(None)
result = naive._round(freq, mode, ambiguous, nonexistent)
aware = result.tz_localize(
self.tz, ambiguous=ambiguous, nonexistent=nonexistent
)
return aware
values = self.view(""i8"")
result = round_nsint64(values, mode, freq)
result = self._maybe_mask_results(result, fill_value=NaT)
return self._simple_new(result, dtype=self.dtype)
@Appender((_round_doc + _round_example).format(op=""round""))
def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
@Appender((_round_doc + _floor_example).format(op=""floor""))
def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
@Appender((_round_doc + _ceil_example).format(op=""ceil""))
def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
def _with_freq(self, freq):
Helper to set our freq in-place, returning self to allow method chaining.
Parameters
----------
freq : DateOffset, None, or ""infer""
Returns
-------
self
# GH#29843
if freq is None:
# Always valid
pass
elif len(self) == 0 and isinstance(freq, DateOffset):
# Always valid.  In the TimedeltaArray case, we assume this
#  is a Tick offset.
pass
else:
# As an internal method, we can ensure this assertion always holds
assert freq == ""infer""
freq = frequencies.to_offset(self.inferred_freq)
self._freq = freq
return self
ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray
""""""
Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
Assumes that __new__/__init__ defines:
_data
_freq
and that the inheriting class has methods:
_generate_range
""""""
# ------------------------------------------------------------------
# NDArrayBackedExtensionArray compat
@property
def _ndarray(self) -> np.ndarray:
# NB: A bunch of Interval tests fail if we use ._data
return self.asi8
def _from_backing_data(self: _T, arr: np.ndarray) -> _T:
# Note: we do not retain `freq`
return type(self)(arr, dtype=self.dtype)  # type: ignore
# ------------------------------------------------------------------
@property
def ndim(self) -> int:
return self._data.ndim
@property
def shape(self):
return self._data.shape
def reshape(self, *args, **kwargs):
# Note: we drop any freq
data = self._data.reshape(*args, **kwargs)
return type(self)(data, dtype=self.dtype)
def ravel(self, *args, **kwargs):
# Note: we drop any freq
data = self._data.ravel(*args, **kwargs)
return type(self)(data, dtype=self.dtype)
def _box_func(self):
""""""
box function to get object from internal representation
""""""
def _box_values(self, values):
apply box func to passed values
return lib.map_infer(values, self._box_func)
def __iter__(self):
return (self._box_func(v) for v in self.asi8)
@property
def asi8(self) -> np.ndarray:
""""""
Integer representation of the values.
Returns
-------
ndarray
An ndarray with int64 dtype.
""""""
# do not cache or you'll create a memory leak
return self._data.view(""i8"")
# ----------------------------------------------------------------
# Rendering Methods
def _format_native_types(self, na_rep=""NaT"", date_format=None):
Helper method for astype when converting to strings.
ndarray[str]
raise AbstractMethodError(self)
def _formatter(self, boxed=False):
# TODO: Remove Datetime & DatetimeTZ formatters.
return ""'{}'"".format
# ----------------------------------------------------------------
# Array-Like / EA-Interface Methods
@property
def nbytes(self):
return self._data.nbytes
def __array__(self, dtype=None) -> np.ndarray:
# used for Timedelta/DatetimeArray, overwritten by PeriodArray
if is_object_dtype(dtype):
return np.array(list(self), dtype=object)
return self._data
@property
def size(self) -> int:
""""""The number of elements in this array.""""""
return np.prod(self.shape)
def __len__(self) -> int:
return len(self._data)
def __getitem__(self, key):
""""""
This getitem defers to the underlying array, which by-definition can
only handle list-likes, slices, and integer scalars
""""""
if com.is_bool_indexer(key):
# first convert to boolean, because check_array_indexer doesn't
# allow object dtype
if is_object_dtype(key):
key = np.asarray(key, dtype=bool)
key = check_array_indexer(self, key)
key = lib.maybe_booleans_to_slice(key.view(np.uint8))
elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):
# see https://github.com/pandas-dev/pandas/issues/31299, need to allow
# this for now (would otherwise raise in check_array_indexer)
pass
key = check_array_indexer(self, key)
freq = self._get_getitem_freq(key)
result = self._data[key]
if lib.is_scalar(result):
return self._box_func(result)
return self._simple_new(result, dtype=self.dtype, freq=freq)
def _get_getitem_freq(self, key):
Find the `freq` attribute to assign to the result of a __getitem__ lookup.
is_period = is_period_dtype(self.dtype)
if is_period:
freq = self.freq
else:
freq = None
if isinstance(key, slice):
if self.freq is not None and key.step is not None:
freq = key.step * self.freq
else:
freq = self.freq
elif key is Ellipsis:
# GH#21282 indexing with Ellipsis is similar to a full slice,
#  should preserve `freq` attribute
freq = self.freq
return freq
def __setitem__(
self,
key: Union[int, Sequence[int], Sequence[bool], slice],
value: Union[NaTType, Any, Sequence[Any]],
) -> None:
# I'm fudging the types a bit here. ""Any"" above really depends
# on type(self). For PeriodArray, it's Period (or stuff coercible
# to a period in from_sequence). For DatetimeArray, it's Timestamp...
# I don't know if mypy can do that, possibly with Generics.
# https://mypy.readthedocs.io/en/latest/generics.html
if is_list_like(value):
is_slice = isinstance(key, slice)
if lib.is_scalar(key):
raise ValueError(""setting an array element with a sequence."")
if not is_slice:
key = cast(Sequence, key)
if len(key) != len(value) and not com.is_bool_indexer(key):
msg = (
f""shape mismatch: value array of length '{len(key)}' ""
""does not match indexing result of length ""
f""'{len(value)}'.""
)
raise ValueError(msg)
elif not len(key):
return
value = self._validate_setitem_value(value)
key = check_array_indexer(self, key)
self._data[key] = value
self._maybe_clear_freq()
def _maybe_clear_freq(self):
# inplace operations like __setitem__ may invalidate the freq of
# DatetimeArray and TimedeltaArray
pass
def astype(self, dtype, copy=True):
# Some notes on cases we don't have to handle here in the base class:
#   1. PeriodArray.astype handles period -> period
#   2. DatetimeArray.astype handles conversion between tz.
#   3. DatetimeArray.astype handles datetime -> period
dtype = pandas_dtype(dtype)
if is_object_dtype(dtype):
return self._box_values(self.asi8.ravel()).reshape(self.shape)
elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):
return self._format_native_types()
elif is_integer_dtype(dtype):
# we deliberately ignore int32 vs. int64 here.
# See https://github.com/pandas-dev/pandas/issues/24381 for more.
values = self.asi8
if is_unsigned_integer_dtype(dtype):
# Again, we ignore int32 vs. int64
values = values.view(""uint64"")
if copy:
values = values.copy()
return values
elif (
is_datetime_or_timedelta_dtype(dtype)
and not is_dtype_equal(self.dtype, dtype)
) or is_float_dtype(dtype):
# disallow conversion between datetime/timedelta,
# and conversions for any datetimelike to float
msg = f""Cannot cast {type(self).__name__} to dtype {dtype}""
raise TypeError(msg)
elif is_categorical_dtype(dtype):
arr_cls = dtype.construct_array_type()
return arr_cls(self, dtype=dtype)
else:
return np.asarray(self, dtype=dtype)
def view(self, dtype=None):
if dtype is None or dtype is self.dtype:
return type(self)(self._data, dtype=self.dtype)
return self._data.view(dtype=dtype)
# ------------------------------------------------------------------
# ExtensionArray Interface
def unique(self):
result = unique1d(self.asi8)
return type(self)(result, dtype=self.dtype)
@classmethod
def _concat_same_type(cls, to_concat, axis: int = 0):
# do not pass tz to set because tzlocal cannot be hashed
dtypes = {str(x.dtype) for x in to_concat}
if len(dtypes) != 1:
raise ValueError(""to_concat must have the same dtype (tz)"", dtypes)
obj = to_concat[0]
dtype = obj.dtype
i8values = [x.asi8 for x in to_concat]
values = np.concatenate(i8values, axis=axis)
new_freq = None
if is_period_dtype(dtype):
new_freq = obj.freq
elif axis == 0:
# GH 3232: If the concat result is evenly spaced, we can retain the
# original frequency
to_concat = [x for x in to_concat if len(x)]
if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):
pairs = zip(to_concat[:-1], to_concat[1:])
if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):
new_freq = obj.freq
return cls._simple_new(values, dtype=dtype, freq=new_freq)
def copy(self):
values = self.asi8.copy()
return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)
def _values_for_factorize(self):
return self.asi8, iNaT
@classmethod
def _from_factorized(cls, values, original):
return cls(values, dtype=original.dtype)
def _values_for_argsort(self):
return self._data
@Appender(ExtensionArray.shift.__doc__)
def shift(self, periods=1, fill_value=None, axis=0):
if not self.size or periods == 0:
return self.copy()
fill_value = self._validate_shift_value(fill_value)
new_values = shift(self._data, periods, axis, fill_value)
return type(self)._simple_new(new_values, dtype=self.dtype)
# ------------------------------------------------------------------
# Validation Methods
# TODO: try to de-duplicate these, ensure identical behavior
def _validate_fill_value(self, fill_value):
""""""
If a fill_value is passed to `take` convert it to an i8 representation,
raising ValueError if this is not possible.
fill_value : object
fill_value : np.int64
Raises
------
ValueError
if is_valid_nat_for_dtype(fill_value, self.dtype):
fill_value = iNaT
elif isinstance(fill_value, self._recognized_scalars):
self._check_compatible_with(fill_value)
fill_value = self._scalar_type(fill_value)
fill_value = self._unbox_scalar(fill_value)
else:
raise ValueError(
f""'fill_value' should be a {self._scalar_type}. ""
f""Got '{str(fill_value)}'.""
)
return fill_value
def _validate_shift_value(self, fill_value):
# TODO(2.0): once this deprecation is enforced, used _validate_fill_value
if is_valid_nat_for_dtype(fill_value, self.dtype):
fill_value = NaT
elif not isinstance(fill_value, self._recognized_scalars):
# only warn if we're not going to raise
if self._scalar_type is Period and lib.is_integer(fill_value):
# kludge for #31971 since Period(integer) tries to cast to str
new_fill = Period._from_ordinal(fill_value, freq=self.freq)
else:
new_fill = self._scalar_type(fill_value)
# stacklevel here is chosen to be correct when called from
#  DataFrame.shift or Series.shift
warnings.warn(
f""Passing {type(fill_value)} to shift is deprecated and ""
""will raise in a future version, pass ""
f""{self._scalar_type.__name__} instead."",
FutureWarning,
stacklevel=10,
)
fill_value = new_fill
fill_value = self._unbox_scalar(fill_value)
return fill_value
def _validate_searchsorted_value(self, value):
if isinstance(value, str):
value = self._scalar_from_string(value)
except ValueError as err:
raise TypeError(
""searchsorted requires compatible dtype or scalar""
) from err
elif is_valid_nat_for_dtype(value, self.dtype):
value = NaT
elif isinstance(value, self._recognized_scalars):
value = self._scalar_type(value)
elif is_list_like(value) and not isinstance(value, type(self)):
value = array(value)
if not type(self)._is_recognized_dtype(value):
raise TypeError(
""searchsorted requires compatible dtype or scalar, ""
f""not {type(value).__name__}""
)
if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):
raise TypeError(f""Unexpected type for 'value': {type(value)}"")
if isinstance(value, type(self)):
self._check_compatible_with(value)
value = value.asi8
else:
value = self._unbox_scalar(value)
return value
def _validate_setitem_value(self, value):
if lib.is_scalar(value) and not isna(value):
value = com.maybe_box_datetimelike(value)
if is_list_like(value):
value = type(self)._from_sequence(value, dtype=self.dtype)
self._check_compatible_with(value, setitem=True)
value = value.asi8
elif isinstance(value, self._scalar_type):
self._check_compatible_with(value, setitem=True)
value = self._unbox_scalar(value)
elif is_valid_nat_for_dtype(value, self.dtype):
value = iNaT
else:
msg = (
f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
f""or array of those. Got '{type(value).__name__}' instead.""
)
raise TypeError(msg)
return value
def _validate_insert_value(self, value):
if isinstance(value, self._recognized_scalars):
value = self._scalar_type(value)
elif is_valid_nat_for_dtype(value, self.dtype):
# GH#18295
value = NaT
elif lib.is_scalar(value) and isna(value):
raise TypeError(
f""cannot insert {type(self).__name__} with incompatible label""
)
return value
def _validate_where_value(self, other):
if is_valid_nat_for_dtype(other, self.dtype):
other = NaT
elif isinstance(other, self._recognized_scalars):
other = self._scalar_type(other)
self._check_compatible_with(other, setitem=True)
elif not is_list_like(other):
raise TypeError(f""Where requires matching dtype, not {type(other)}"")
else:
# Do type inference if necessary up front
# e.g. we passed PeriodIndex.values and got an ndarray of Periods
other = array(other)
other = extract_array(other, extract_numpy=True)
if is_categorical_dtype(other.dtype):
# e.g. we have a Categorical holding self.dtype
if is_dtype_equal(other.categories.dtype, self.dtype):
other = other._internal_get_values()
if not type(self)._is_recognized_dtype(other.dtype):
raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
self._check_compatible_with(other, setitem=True)
if lib.is_scalar(other):
other = self._unbox_scalar(other)
else:
other = other.view(""i8"")
return other
# ------------------------------------------------------------------
# Additional array methods
#  These are not part of the EA API, but we implement them because
#  pandas assumes they're there.
def searchsorted(self, value, side=""left"", sorter=None):
Find indices where elements should be inserted to maintain order.
Find the indices into a sorted array `self` such that, if the
corresponding elements in `value` were inserted before the indices,
the order of `self` would be preserved.
value : array_like
Values to insert into `self`.
side : {'left', 'right'}, optional
If 'left', the index of the first suitable location found is given.
If 'right', return the last such index.  If there is no suitable
index, return either 0 or N (where N is the length of `self`).
sorter : 1-D array_like, optional
Optional array of integer indices that sort `self` into ascending
order. They are typically the result of ``np.argsort``.
indices : array of ints
Array of insertion points with the same shape as `value`.
value = self._validate_searchsorted_value(value)
# TODO: Use datetime64 semantics for sorting, xref GH#29844
return self.asi8.searchsorted(value, side=side, sorter=sorter)
def repeat(self, repeats, *args, **kwargs):
""""""
Repeat elements of an array.
See Also
--------
numpy.ndarray.repeat
nv.validate_repeat(args, kwargs)
values = self._data.repeat(repeats)
return type(self)(values.view(""i8""), dtype=self.dtype)
def value_counts(self, dropna=False):
""""""
Return a Series containing counts of unique values.
dropna : bool, default True
Don't include counts of NaT values.
Returns
-------
Series
""""""
from pandas import Series, Index
if dropna:
values = self[~self.isna()]._data
else:
values = self._data
cls = type(self)
result = value_counts(values, sort=False, dropna=dropna)
index = Index(
cls(result.index.view(""i8""), dtype=self.dtype), name=result.index.name
)
return Series(result._values, index=index, name=result.name)
def map(self, mapper):
# TODO(GH-23179): Add ExtensionArray.map
# Need to figure out if we want ExtensionArray.map first.
# If so, then we can refactor IndexOpsMixin._map_values to
# a standalone function and call from here..
# Else, just rewrite _map_infer_values to do the right thing.
from pandas import Index
return Index(self).map(mapper).array
# ------------------------------------------------------------------
# Null Handling
def isna(self):
return self._isnan
@property  # NB: override with cache_readonly in immutable subclasses
def _isnan(self):
""""""
return if each value is nan
""""""
return self.asi8 == iNaT
@property  # NB: override with cache_readonly in immutable subclasses
def _hasnans(self):
""""""
return if I have any nans; enables various perf speedups
""""""
return bool(self._isnan.any())
def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):
""""""
Parameters
----------
result : a ndarray
fill_value : object, default iNaT
convert : str, dtype or None
Returns
-------
result : ndarray with values replace by the fill_value
mask the result if needed, convert to the provided dtype if its not
None
This is an internal routine.
""""""
if self._hasnans:
if convert:
result = result.astype(convert)
if fill_value is None:
fill_value = np.nan
result[self._isnan] = fill_value
return result
def fillna(self, value=None, method=None, limit=None):
# TODO(GH-20300): remove this
# Just overriding to ensure that we avoid an astype(object).
# Either 20300 or a `_values_for_fillna` would avoid this duplication.
if isinstance(value, ABCSeries):
value = value.array
value, method = validate_fillna_kwargs(value, method)
mask = self.isna()
if is_array_like(value):
if len(value) != len(self):
raise ValueError(
f""Length of 'value' does not match. Got ({len(value)}) ""
f"" expected {len(self)}""
)
value = value[mask]
if mask.any():
if method is not None:
if method == ""pad"":
func = missing.pad_1d
else:
func = missing.backfill_1d
values = self._data
if not is_period_dtype(self):
# For PeriodArray self._data is i8, which gets copied
#  by `func`.  Otherwise we need to make a copy manually
# to avoid modifying `self` in-place.
values = values.copy()
new_values = func(values, limit=limit, mask=mask)
if is_datetime64tz_dtype(self):
# we need to pass int64 values to the constructor to avoid
#  re-localizing incorrectly
new_values = new_values.view(""i8"")
new_values = type(self)(new_values, dtype=self.dtype)
else:
# fill with value
new_values = self.copy()
new_values[mask] = value
else:
new_values = self.copy()
return new_values
# ------------------------------------------------------------------
# Frequency Properties/Methods
Return the frequency object if it is set, otherwise None.
return self._freq
@freq.setter
def freq(self, value):
if value is not None:
value = frequencies.to_offset(value)
self._validate_frequency(self, value)
self._freq = value
@property  # NB: override with cache_readonly in immutable subclasses
def inferred_freq(self):
""""""
Tryies to return a string representing a frequency guess,
generated by infer_freq.  Returns None if it can't autodetect the
frequency.
""""""
if self.ndim != 1:
return None
try:
return frequencies.infer_freq(self)
except ValueError:
return None
@property  # NB: override with cache_readonly in immutable subclasses
def _resolution(self):
return frequencies.Resolution.get_reso_from_freq(self.freqstr)
@property  # NB: override with cache_readonly in immutable subclasses
def resolution(self):
""""""
Returns day, hour, minute, second, millisecond or microsecond
""""""
return frequencies.Resolution.get_str(self._resolution)
@classmethod
def _validate_frequency(cls, index, freq, **kwargs):
""""""
Validate that a frequency is compatible with the values of a given
Datetime Array/Index or Timedelta Array/Index
Parameters
----------
index : DatetimeIndex or TimedeltaIndex
The index on which to determine if the given frequency is valid
freq : DateOffset
The frequency to validate
""""""
if is_period_dtype(cls):
# Frequency validation is not meaningful for Period Array/Index
return None
inferred = index.inferred_freq
if index.size == 0 or inferred == freq.freqstr:
return None
try:
on_freq = cls._generate_range(
start=index[0], end=None, periods=len(index), freq=freq, **kwargs
)
if not np.array_equal(index.asi8, on_freq.asi8):
raise ValueError
except ValueError as e:
if ""non-fixed"" in str(e):
# non-fixed frequencies are not meaningful for timedelta64;
#  we retain that error message
raise e
# GH#11587 the main way this is reached is if the `np.array_equal`
#  check above is False.  This can also be reached if index[0]
#  is `NaT`, in which case the call to `cls._generate_range` will
#  raise a ValueError, which we re-raise with a more targeted
#  message.
raise ValueError(
f""Inferred frequency {inferred} from passed values ""
f""does not conform to passed frequency {freq.freqstr}""
) from e
# monotonicity/uniqueness properties are called via frequencies.infer_freq,
#  see GH#23789
@property
def _is_monotonic_increasing(self):
return algos.is_monotonic(self.asi8, timelike=True)[0]
@property
def _is_monotonic_decreasing(self):
return algos.is_monotonic(self.asi8, timelike=True)[1]
@property
def _is_unique(self):
return len(unique1d(self.asi8)) == len(self)
# ------------------------------------------------------------------
# Arithmetic Methods
_create_comparison_method = classmethod(_datetimelike_array_cmp)
# pow is invalid for all three subclasses; TimedeltaArray will override
#  the multiplication and division ops
__pow__ = make_invalid_op(""__pow__"")
__rpow__ = make_invalid_op(""__rpow__"")
__mul__ = make_invalid_op(""__mul__"")
__rmul__ = make_invalid_op(""__rmul__"")
__truediv__ = make_invalid_op(""__truediv__"")
__rtruediv__ = make_invalid_op(""__rtruediv__"")
__floordiv__ = make_invalid_op(""__floordiv__"")
__rfloordiv__ = make_invalid_op(""__rfloordiv__"")
__mod__ = make_invalid_op(""__mod__"")
__rmod__ = make_invalid_op(""__rmod__"")
__divmod__ = make_invalid_op(""__divmod__"")
__rdivmod__ = make_invalid_op(""__rdivmod__"")
def _add_datetimelike_scalar(self, other):
# Overridden by TimedeltaArray
raise TypeError(f""cannot add {type(self).__name__} and {type(other).__name__}"")
_add_datetime_arraylike = _add_datetimelike_scalar
def _sub_datetimelike_scalar(self, other):
# Overridden by DatetimeArray
assert other is not NaT
raise TypeError(f""cannot subtract a datelike from a {type(self).__name__}"")
_sub_datetime_arraylike = _sub_datetimelike_scalar
def _sub_period(self, other):
# Overridden by PeriodArray
raise TypeError(f""cannot subtract Period from a {type(self).__name__}"")
def _add_offset(self, offset):
raise AbstractMethodError(self)
def _add_timedeltalike_scalar(self, other):
""""""
Add a delta of a timedeltalike
Returns
-------
Same type as self
""""""
if isna(other):
# i.e np.timedelta64(""NaT""), not recognized by delta_to_nanoseconds
new_values = np.empty(self.shape, dtype=""i8"")
new_values[:] = iNaT
return type(self)(new_values, dtype=self.dtype)
inc = delta_to_nanoseconds(other)
new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(
""i8""
)
new_values = self._maybe_mask_results(new_values)
new_freq = None
if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):
# adding a scalar preserves freq
new_freq = self.freq
return type(self)(new_values, dtype=self.dtype, freq=new_freq)
def _add_timedelta_arraylike(self, other):
""""""
Add a delta of a TimedeltaIndex
Returns
-------
Same type as self
""""""
# overridden by PeriodArray
if len(self) != len(other):
raise ValueError(""cannot add indices of unequal length"")
if isinstance(other, np.ndarray):
# ndarray[timedelta64]; wrap in TimedeltaIndex for op
from pandas.core.arrays import TimedeltaArray
other = TimedeltaArray._from_sequence(other)
self_i8 = self.asi8
other_i8 = other.asi8
new_values = checked_add_with_arr(
self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan
)
if self._hasnans or other._hasnans:
mask = (self._isnan) | (other._isnan)
new_values[mask] = iNaT
return type(self)(new_values, dtype=self.dtype)
def _add_nat(self):
Add pd.NaT to self
""""""
if is_period_dtype(self):
raise TypeError(
f""Cannot add {type(self).__name__} and {type(NaT).__name__}""
)
# GH#19124 pd.NaT is treated like a timedelta for both timedelta
# and datetime dtypes
result = np.zeros(self.shape, dtype=np.int64)
result.fill(iNaT)
return type(self)(result, dtype=self.dtype, freq=None)
def _sub_nat(self):
""""""
Subtract pd.NaT from self
""""""
# GH#19124 Timedelta - datetime is not in general well-defined.
# We make an exception for pd.NaT, which in this case quacks
# like a timedelta.
# For datetime64 dtypes by convention we treat NaT as a datetime, so
# this subtraction returns a timedelta64 dtype.
# For period dtype, timedelta64 is a close-enough return dtype.
result = np.zeros(self.shape, dtype=np.int64)
result.fill(iNaT)
return result.view(""timedelta64[ns]"")
def _sub_period_array(self, other):
""""""
Subtract a Period Array/Index from self.  This is only valid if self
is itself a Period Array/Index, raises otherwise.  Both objects must
have the same frequency.
other : PeriodIndex or PeriodArray
Returns
-------
result : np.ndarray[object]
Array of DateOffset objects; nulls represented by NaT.
""""""
if not is_period_dtype(self):
raise TypeError(
f""cannot subtract {other.dtype}-dtype from {type(self).__name__}""
)
if self.freq != other.freq:
msg = DIFFERENT_FREQ.format(
cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr
)
raise IncompatibleFrequency(msg)
new_values = checked_add_with_arr(
self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan
)
new_values = np.array([self.freq.base * x for x in new_values])
if self._hasnans or other._hasnans:
mask = (self._isnan) | (other._isnan)
new_values[mask] = NaT
return new_values
def _addsub_object_array(self, other: np.ndarray, op):
""""""
Add or subtract array-like of DateOffset objects
Parameters
----------
other : np.ndarray[object]
op : {operator.add, operator.sub}
result : same class as self
assert op in [operator.add, operator.sub]
if len(other) == 1:
return op(self, other[0])
warnings.warn(
""Adding/subtracting array of DateOffsets to ""
f""{type(self).__name__} not vectorized"",
PerformanceWarning,
)
# Caller is responsible for broadcasting if necessary
assert self.shape == other.shape, (self.shape, other.shape)
res_values = op(self.astype(""O""), np.array(other))
result = array(res_values.ravel())
result = extract_array(result, extract_numpy=True).reshape(self.shape)
return result
def _time_shift(self, periods, freq=None):
""""""
Shift each value by `periods`.
Note this is different from ExtensionArray.shift, which
shifts the *position* of each element, padding the end with
missing values.
Parameters
----------
periods : int
Number of periods to shift by.
freq : pandas.DateOffset, pandas.Timedelta, or str
Frequency increment to shift by.
""""""
if freq is not None and freq != self.freq:
if isinstance(freq, str):
freq = frequencies.to_offset(freq)
offset = periods * freq
result = self + offset
if periods == 0:
# immutable so OK
return self.copy()
if self.freq is None:
raise NullFrequencyError(""Cannot shift with no freq"")
start = self[0] + periods * self.freq
end = self[-1] + periods * self.freq
# Note: in the DatetimeTZ case, _generate_range will infer the
#  appropriate timezone from `start` and `end`, so tz does not need
#  to be passed explicitly.
return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
@unpack_zerodim_and_defer(""__add__"")
def __add__(self, other):
# scalar others
if other is NaT:
result = self._add_nat()
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
result = self._add_timedeltalike_scalar(other)
elif isinstance(other, DateOffset):
# specifically _not_ a Tick
result = self._add_offset(other)
elif isinstance(other, (datetime, np.datetime64)):
result = self._add_datetimelike_scalar(other)
elif lib.is_integer(other):
# This check must come after the check for np.timedelta64
# as is_integer returns True for these
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._time_shift(other)
# array-like others
elif is_timedelta64_dtype(other):
# TimedeltaIndex, ndarray[timedelta64]
result = self._add_timedelta_arraylike(other)
elif is_object_dtype(other):
# e.g. Array/Index of DateOffset objects
result = self._addsub_object_array(other, operator.add)
elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
# DatetimeIndex, ndarray[datetime64]
return self._add_datetime_arraylike(other)
elif is_integer_dtype(other):
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._addsub_int_array(other, operator.add)
else:
# Includes Categorical, other ExtensionArrays
# For PeriodDtype, if self is a TimedeltaArray and other is a
#  PeriodArray with  a timedelta-like (i.e. Tick) freq, this
#  operation is valid.  Defer to the PeriodArray implementation.
#  In remaining cases, this will end up raising TypeError.
return NotImplemented
if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
from pandas.core.arrays import TimedeltaArray
return TimedeltaArray(result)
return result
def __radd__(self, other):
# alias for __add__
return self.__add__(other)
@unpack_zerodim_and_defer(""__sub__"")
def __sub__(self, other):
# scalar others
if other is NaT:
result = self._sub_nat()
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
result = self._add_timedeltalike_scalar(-other)
elif isinstance(other, DateOffset):
# specifically _not_ a Tick
result = self._add_offset(-other)
elif isinstance(other, (datetime, np.datetime64)):
result = self._sub_datetimelike_scalar(other)
elif lib.is_integer(other):
# This check must come after the check for np.timedelta64
# as is_integer returns True for these
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._time_shift(-other)
elif isinstance(other, Period):
result = self._sub_period(other)
# array-like others
elif is_timedelta64_dtype(other):
# TimedeltaIndex, ndarray[timedelta64]
result = self._add_timedelta_arraylike(-other)
elif is_object_dtype(other):
# e.g. Array/Index of DateOffset objects
result = self._addsub_object_array(other, operator.sub)
elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
# DatetimeIndex, ndarray[datetime64]
result = self._sub_datetime_arraylike(other)
elif is_period_dtype(other):
# PeriodIndex
result = self._sub_period_array(other)
elif is_integer_dtype(other):
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._addsub_int_array(other, operator.sub)
# Includes ExtensionArrays, float_dtype
return NotImplemented
if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
from pandas.core.arrays import TimedeltaArray
return TimedeltaArray(result)
return result
def __rsub__(self, other):
if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
# ndarray[datetime64] cannot be subtracted from self, so
# we need to wrap in DatetimeArray/Index and flip the operation
if lib.is_scalar(other):
# i.e. np.datetime64 object
return Timestamp(other) - self
if not isinstance(other, DatetimeLikeArrayMixin):
# Avoid down-casting DatetimeIndex
from pandas.core.arrays import DatetimeArray
other = DatetimeArray(other)
return other - self
elif (
is_datetime64_any_dtype(self.dtype)
and hasattr(other, ""dtype"")
and not is_datetime64_any_dtype(other.dtype)
):
# GH#19959 datetime - datetime is well-defined as timedelta,
# but any other type - datetime is not well-defined.
raise TypeError(
f""cannot subtract {type(self).__name__} from {type(other).__name__}""
)
elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):
# TODO: Can we simplify/generalize these cases at all?
raise TypeError(f""cannot subtract {type(self).__name__} from {other.dtype}"")
elif is_timedelta64_dtype(self.dtype):
if lib.is_integer(other) or is_integer_dtype(other):
# need to subtract before negating, since that flips freq
# -self flips self.freq, messing up results
return -(self - other)
return (-self) + other
return -(self - other)
def __iadd__(self, other):
result = self + other
self[:] = result[:]
if not is_period_dtype(self):
# restore freq, which is invalidated by setitem
self._freq = result._freq
return self
def __isub__(self, other):
result = self - other
self[:] = result[:]
if not is_period_dtype(self):
# restore freq, which is invalidated by setitem
self._freq = result._freq
return self
# --------------------------------------------------------------
# Reductions
def _reduce(self, name, axis=0, skipna=True, **kwargs):
op = getattr(self, name, None)
if op:
return op(skipna=skipna, **kwargs)
return super()._reduce(name, skipna, **kwargs)
def min(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the minimum value of the Array or minimum along
an axis.
See Also
--------
numpy.ndarray.min
Index.min : Return the minimum value in an Index.
Series.min : Return the minimum value in a Series.
nv.validate_min(args, kwargs)
nv.validate_minmax_axis(axis)
result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())
if isna(result):
# Period._from_ordinal does not handle np.nan gracefully
return NaT
return self._box_func(result)
def max(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the maximum value of the Array or maximum along
an axis.
See Also
--------
numpy.ndarray.max
Index.max : Return the maximum value in an Index.
Series.max : Return the maximum value in a Series.
""""""
# TODO: skipna is broken with max.
# See https://github.com/pandas-dev/pandas/issues/24265
nv.validate_max(args, kwargs)
nv.validate_minmax_axis(axis)
mask = self.isna()
if skipna:
values = self[~mask].asi8
elif mask.any():
return NaT
else:
values = self.asi8
if not len(values):
# short-circuit for empty max / min
return NaT
result = nanops.nanmax(values, skipna=skipna)
# Don't have to worry about NA `result`, since no NA went in.
return self._box_func(result)
def mean(self, skipna=True):
Return the mean value of the Array.
.. versionadded:: 0.25.0
skipna : bool, default True
Whether to ignore any NaT elements.
scalar
Timestamp or Timedelta.
See Also
--------
numpy.ndarray.mean : Returns the average of array elements along a given axis.
Series.mean : Return the mean value in a Series.
Notes
-----
mean is only defined for Datetime and Timedelta dtypes, not for Period.
""""""
if is_period_dtype(self):
# See discussion in GH#24757
raise TypeError(
f""mean is not implemented for {type(self).__name__} since the ""
""meaning is ambiguous.  An alternative is ""
""obj.to_timestamp(how='start').mean()""
)
mask = self.isna()
if skipna:
values = self[~mask]
elif mask.any():
return NaT
else:
values = self
if not len(values):
# short-circuit for empty max / min
return NaT
result = nanops.nanmean(values.view(""i8""), skipna=skipna)
# Don't have to worry about NA `result`, since no NA went in.
return self._box_func(result)
""""""
If a `periods` argument is passed to the Datetime/Timedelta Array/Index
constructor, cast it to an integer.
Parameters
----------
periods : None, float, int
Returns
-------
periods : None or int
Raises
------
TypeError
if periods is None, float, or int
""""""
if periods is not None:
if lib.is_float(periods):
periods = int(periods)
elif not lib.is_integer(periods):
raise TypeError(f""periods must be a number, got {periods}"")
return periods
""""""
Check that the `closed` argument is among [None, ""left"", ""right""]
Parameters
----------
closed : {None, ""left"", ""right""}
Returns
-------
left_closed : bool
right_closed : bool
Raises
------
ValueError : if argument is not among valid values
""""""
left_closed = False
right_closed = False
if closed is None:
left_closed = True
right_closed = True
elif closed == ""left"":
left_closed = True
elif closed == ""right"":
right_closed = True
else:
raise ValueError(""Closed has to be either 'left', 'right' or None"")
return left_closed, right_closed
""""""
If the user passes a freq and another freq is inferred from passed data,
require that they match.
Parameters
----------
freq : DateOffset or None
inferred_freq : DateOffset or None
freq_infer : bool
Returns
-------
freq : DateOffset or None
freq_infer : bool
Notes
-----
We assume at this point that `maybe_infer_freq` has been called, so
`freq` is either a DateOffset object or None.
""""""
if inferred_freq is not None:
if freq is not None and freq != inferred_freq:
raise ValueError(
f""Inferred frequency {inferred_freq} from passed ""
""values does not conform to passed frequency ""
f""{freq.freqstr}""
)
elif freq is None:
freq = inferred_freq
freq_infer = False
return freq, freq_infer
""""""
Comparing a DateOffset to the string ""infer"" raises, so we need to
be careful about comparisons.  Make a dummy variable `freq_infer` to
signify the case where the given freq is ""infer"" and set freq to None
to avoid comparison trouble later on.
Parameters
----------
freq : {DateOffset, None, str}
Returns
-------
freq : {DateOffset, None}
freq_infer : bool
Whether we should inherit the freq of passed data.
""""""
freq_infer = False
if not isinstance(freq, DateOffset):
# if a passed freq is None, don't infer automatically
if freq != ""infer"":
freq = frequencies.to_offset(freq)
else:
freq_infer = True
freq = None
return freq, freq_infer
msg = ""cannot insert DatetimeIndex with incompatible label""
msg = ""cannot insert TimedeltaIndex with incompatible label""
msg = ""cannot insert TimedeltaIndex with incompatible label""
msg = ""cannot insert DatetimeIndex with incompatible label""","ensure_int64,
is_bool_dtype,
is_integer,
is_scalar,
ExtensionIndex,
inherit_names,
make_wrapped_arith_op,
""""""
Create the join wrapper methods.
""""""
@staticmethod  # type: ignore
def wrapper(left, right):
if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
left = left.view(""i8"")
if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
right = right.view(""i8"")
results = joinf(left, right)
if with_indexers:
# dtype should be timedelta64[ns] for TimedeltaIndex
#  and datetime64[ns] for DatetimeIndex
dtype = left.dtype.base
join_index, left_indexer, right_indexer = results
join_index = join_index.view(dtype)
return join_index, left_indexer, right_indexer
return results
return wrapper
""""""
Dispatch the operation to the underlying ExtensionArray, and infer
the appropriate frequency for the result.
""""""
meth = make_wrapped_arith_op(opname)
def wrapped(self, other):
result = meth(self, other)
if result is NotImplemented:
return NotImplemented
new_freq = self._get_addsub_freq(other)
result._freq = new_freq
wrapped.__name__ = opname
return wrapped
[""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
DatetimeLikeArrayMixin,
cache=True,
[""mean"", ""asi8"", ""_box_func""], DatetimeLikeArrayMixin,
""""""
Common ops mixin to support a unified interface datetimelike Index.
""""""
_data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
freq: Optional[DateOffset]
freqstr: Optional[str]
_resolution: int
_bool_ops: List[str] = []
_field_ops: List[str] = []
hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
_hasnans = hasnans  # for index / array -agnostic code
@property
def is_all_dates(self) -> bool:
return True
# ------------------------------------------------------------------------
# Abstract data attributes
@property
def values(self):
# Note: PeriodArray overrides this to return an ndarray of objects.
return self._data._data
def __array_wrap__(self, result, context=None):
Gets called after a ufunc.
result = lib.item_from_zerodim(result)
if is_bool_dtype(result) or lib.is_scalar(result):
return result
attrs = self._get_attributes_dict()
if not is_period_dtype(self) and attrs[""freq""]:
# no need to infer if freq is None
attrs[""freq""] = ""infer""
return Index(result, **attrs)
# ------------------------------------------------------------------------
def equals(self, other) -> bool:
Determines if two Index objects contain the same elements.
if self.is_(other):
return True
if not isinstance(other, ABCIndexClass):
return False
elif not isinstance(other, type(self)):
try:
other = type(self)(other)
except (ValueError, TypeError, OverflowError):
# e.g.
#  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
#  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
#  OverflowError -> Index([very_large_timedeltas])
return False
if not is_dtype_equal(self.dtype, other.dtype):
# have different timezone
return False
return np.array_equal(self.asi8, other.asi8)
@Appender(Index.__contains__.__doc__)
def __contains__(self, key: Any) -> bool:
hash(key)
try:
res = self.get_loc(key)
except (KeyError, TypeError, ValueError):
return False
return bool(
is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))
)
def sort_values(self, return_indexer=False, ascending=True):
Return sorted copy of Index.
if return_indexer:
_as = self.argsort()
if not ascending:
_as = _as[::-1]
sorted_index = self.take(_as)
return sorted_index, _as
else:
# NB: using asi8 instead of _data matters in numpy 1.18
#  because the treatment of NaT has been changed to put NaT last
#  instead of first.
sorted_values = np.sort(self.asi8)
freq = self.freq
if freq is not None and not is_period_dtype(self):
if freq.n > 0 and not ascending:
freq = freq * -1
elif freq.n < 0 and ascending:
freq = freq * -1
if not ascending:
sorted_values = sorted_values[::-1]
arr = type(self._data)._simple_new(
sorted_values, dtype=self.dtype, freq=freq
)
return type(self)._simple_new(arr, name=self.name)
@Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
nv.validate_take(tuple(), kwargs)
indices = ensure_int64(indices)
maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
if isinstance(maybe_slice, slice):
return self[maybe_slice]
return ExtensionIndex.take(
self, indices, axis, allow_fill, fill_value, **kwargs
)
@doc(IndexOpsMixin.searchsorted, klass=""Datetime-like Index"")
def searchsorted(self, value, side=""left"", sorter=None):
if isinstance(value, str):
raise TypeError(
""searchsorted requires compatible dtype or scalar, ""
f""not {type(value).__name__}""
)
if isinstance(value, Index):
value = value._data
return self._data.searchsorted(value, side=side, sorter=sorter)
_can_hold_na = True
_na_value = NaT
""""""The expected NA value to use with this index.""""""
def _convert_tolerance(self, tolerance, target):
tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
if target.size != tolerance.size and tolerance.size > 1:
raise ValueError(""list-like tolerance size must match target index size"")
return tolerance
def tolist(self) -> List:
""""""
Return a list of the underlying data.
""""""
return list(self.astype(object))
def min(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the minimum value of the Index or minimum along
an axis.
See Also
numpy.ndarray.min
Series.min : Return the minimum value in a Series.
nv.validate_min(args, kwargs)
nv.validate_minmax_axis(axis)
if not len(self):
return self._na_value
i8 = self.asi8
try:
# quick check
if len(i8) and self.is_monotonic:
if i8[0] != iNaT:
return self._box_func(i8[0])
if self.hasnans:
if skipna:
min_stamp = self[~self._isnan].asi8.min()
else:
return self._na_value
else:
min_stamp = i8.min()
return self._box_func(min_stamp)
except ValueError:
return self._na_value
def argmin(self, axis=None, skipna=True, *args, **kwargs):
Returns the indices of the minimum values along an axis.
See `numpy.ndarray.argmin` for more information on the
`axis` parameter.
See Also
--------
numpy.ndarray.argmin
nv.validate_argmin(args, kwargs)
nv.validate_minmax_axis(axis)
i8 = self.asi8
if self.hasnans:
mask = self._isnan
if mask.all() or not skipna:
return -1
i8 = i8.copy()
i8[mask] = np.iinfo(""int64"").max
return i8.argmin()
def max(self, axis=None, skipna=True, *args, **kwargs):
Return the maximum value of the Index or maximum along
an axis.
See Also
--------
numpy.ndarray.max
Series.max : Return the maximum value in a Series.
""""""
nv.validate_max(args, kwargs)
nv.validate_minmax_axis(axis)
if not len(self):
return self._na_value
i8 = self.asi8
try:
# quick check
if len(i8) and self.is_monotonic:
if i8[-1] != iNaT:
return self._box_func(i8[-1])
if self.hasnans:
if skipna:
max_stamp = self[~self._isnan].asi8.max()
else:
return self._na_value
else:
max_stamp = i8.max()
return self._box_func(max_stamp)
except ValueError:
return self._na_value
def argmax(self, axis=None, skipna=True, *args, **kwargs):
Returns the indices of the maximum values along an axis.
See `numpy.ndarray.argmax` for more information on the
`axis` parameter.
See Also
--------
numpy.ndarray.argmax
nv.validate_argmax(args, kwargs)
nv.validate_minmax_axis(axis)
i8 = self.asi8
if self.hasnans:
mask = self._isnan
if mask.all() or not skipna:
return -1
i8 = i8.copy()
i8[mask] = 0
return i8.argmax()
# --------------------------------------------------------------------
# Rendering Methods
def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
return header + list(self._format_native_types(na_rep, **kwargs))
def _formatter_func(self):
def _format_attrs(self):
Return a list of tuples of the (attr,formatted_value).
attrs = super()._format_attrs()
for attrib in self._attributes:
if attrib == ""freq"":
freq = self.freqstr
if freq is not None:
freq = repr(freq)
attrs.append((""freq"", freq))
return attrs
# --------------------------------------------------------------------
# Indexing Methods
def _validate_partial_date_slice(self, reso: str):
raise NotImplementedError
def _parsed_string_to_bounds(self, reso: str, parsed: datetime):
raise NotImplementedError
def _partial_date_slice(
self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True
):
Parameters
----------
reso : str
parsed : datetime
use_lhs : bool, default True
use_rhs : bool, default True
slice or ndarray[intp]
self._validate_partial_date_slice(reso)
t1, t2 = self._parsed_string_to_bounds(reso, parsed)
i8vals = self.asi8
unbox = self._data._unbox_scalar
if self.is_monotonic:
if len(self) and (
(use_lhs and t1 < self[0] and t2 < self[0])
or ((use_rhs and t1 > self[-1] and t2 > self[-1]))
):
# we are out of range
raise KeyError
# TODO: does this depend on being monotonic _increasing_?
# a monotonic (sorted) series can be sliced
# Use asi8.searchsorted to avoid re-validating Periods/Timestamps
left = i8vals.searchsorted(unbox(t1), side=""left"") if use_lhs else None
right = i8vals.searchsorted(unbox(t2), side=""right"") if use_rhs else None
return slice(left, right)
lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True
rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True
# try to find the dates
return (lhs_mask & rhs_mask).nonzero()[0]
# --------------------------------------------------------------------
# Arithmetic Methods
def _get_addsub_freq(self, other) -> Optional[DateOffset]:
Find the freq we expect the result of an addition/subtraction operation
to have.
if is_period_dtype(self.dtype):
# Only used for ops that stay PeriodDtype
return self.freq
elif self.freq is None:
return None
elif lib.is_scalar(other) and isna(other):
return None
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
new_freq = None
if isinstance(self.freq, Tick):
new_freq = self.freq
return new_freq
elif isinstance(other, DateOffset):
# otherwise just DatetimeArray
return None  # TODO: Should we infer if it matches self.freq * n?
elif isinstance(other, (datetime, np.datetime64)):
return self.freq
elif is_timedelta64_dtype(other):
return None  # TODO: shouldnt we be able to do self.freq + other.freq?
elif is_object_dtype(other):
return None  # TODO: is this quite right?  sometimes we unpack singletons
elif is_datetime64_any_dtype(other):
return None  # TODO: shouldnt we be able to do self.freq + other.freq?
else:
raise NotImplementedError
__add__ = _make_wrapped_arith_op_with_freq(""__add__"")
__sub__ = _make_wrapped_arith_op_with_freq(""__sub__"")
__radd__ = make_wrapped_arith_op(""__radd__"")
__rsub__ = make_wrapped_arith_op(""__rsub__"")
__pow__ = make_wrapped_arith_op(""__pow__"")
__rpow__ = make_wrapped_arith_op(""__rpow__"")
__mul__ = make_wrapped_arith_op(""__mul__"")
__rmul__ = make_wrapped_arith_op(""__rmul__"")
__floordiv__ = make_wrapped_arith_op(""__floordiv__"")
__rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
__mod__ = make_wrapped_arith_op(""__mod__"")
__rmod__ = make_wrapped_arith_op(""__rmod__"")
__divmod__ = make_wrapped_arith_op(""__divmod__"")
__rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
__truediv__ = make_wrapped_arith_op(""__truediv__"")
__rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
def isin(self, values, level=None):
""""""
Compute boolean array of whether each index value is found in the
passed set of values.
values : set or sequence of values
is_contained : ndarray (boolean dtype)
if level is not None:
self._validate_index_level(level)
if not isinstance(values, type(self)):
values = type(self)(values)
except ValueError:
return self.astype(object).isin(values)
return algorithms.isin(self.asi8, values.asi8)
@Appender(Index.where.__doc__)
def where(self, cond, other=None):
values = self.view(""i8"")
try:
other = self._data._validate_where_value(other)
except (TypeError, ValueError) as err:
# Includes tzawareness mismatch and IncompatibleFrequencyError
oth = getattr(other, ""dtype"", other)
raise TypeError(f""Where requires matching dtype, not {oth}"") from err
result = np.where(cond, values, other).astype(""i8"")
arr = type(self._data)._simple_new(result, dtype=self.dtype)
return type(self)._simple_new(arr, name=self.name)
def _summary(self, name=None) -> str:
Return a summarized representation.
name : str
Name to use in the summary representation.
str
Summarized representation of the index.
formatter = self._formatter_func
if len(self) > 0:
index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
else:
index_summary = """"
if name is None:
name = type(self).__name__
result = f""{name}: {len(self)} entries{index_summary}""
if self.freq:
result += f""\nFreq: {self.freqstr}""
# display as values, not quoted
result = result.replace(""'"", """")
return result
def shift(self, periods=1, freq=None):
Shift index by desired number of time frequency increments.
This method is for shifting the values of datetime-like indexes
by a specified time increment a given number of times.
periods : int, default 1
Number of periods (or increments) to shift by,
can be positive or negative.
.. versionchanged:: 0.24.0
freq : pandas.DateOffset, pandas.Timedelta or string, optional
Frequency increment to shift by.
If None, the index is shifted by its own `freq` attribute.
Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.
Returns
-------
pandas.DatetimeIndex
Shifted index.
See Also
--------
Index.shift : Shift values of Index.
PeriodIndex.shift : Shift values of PeriodIndex.
""""""
arr = self._data.view()
arr._freq = self.freq
result = arr._time_shift(periods, freq=freq)
return type(self)(result, name=self.name)
# --------------------------------------------------------------------
# List-like Methods
def delete(self, loc):
new_i8s = np.delete(self.asi8, loc)
freq = None
if is_period_dtype(self):
freq = self.freq
elif is_integer(loc):
if loc in (0, -len(self), -1, len(self) - 1):
freq = self.freq
else:
if is_list_like(loc):
loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
if isinstance(loc, slice) and loc.step in (1, None):
if loc.start in (0, None) or loc.stop in (len(self), None):
freq = self.freq
arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
return type(self)._simple_new(arr, name=self.name)
# --------------------------------------------------------------------
# Join/Set Methods
def _wrap_joined_index(self, joined: np.ndarray, other):
assert other.dtype == self.dtype, (other.dtype, self.dtype)
name = get_op_result_name(self, other)
if is_period_dtype(self.dtype):
freq = self.freq
else:
self = cast(DatetimeTimedeltaMixin, self)
freq = self.freq if self._can_fast_union(other) else None
new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)
return type(self)._simple_new(new_data, name=name)
""""""
Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
but not PeriodIndex
""""""
# Compat for frequency inference, see GH#23789
_is_monotonic_increasing = Index.is_monotonic_increasing
_is_monotonic_decreasing = Index.is_monotonic_decreasing
_is_unique = Index.is_unique
_freq = lib.no_default
In limited circumstances, our freq may differ from that of our _data.
if self._freq is not lib.no_default:
return self._freq
return self._data.freq
def _with_freq(self, freq):
index = self.copy(deep=False)
if freq is None:
# Even if we _can_ have a freq, we might want to set it to None
index._freq = None
elif len(self) == 0 and isinstance(freq, DateOffset):
# Always valid.  In the TimedeltaArray case, we assume this
#  is a Tick offset.
index._freq = freq
else:
assert freq == ""infer"", freq
freq = to_offset(self.inferred_freq)
index._freq = freq
return index
def _shallow_copy(self, values=None, name: Label = lib.no_default):
name = self.name if name is lib.no_default else name
cache = self._cache.copy() if values is None else {}
if values is None:
values = self._data
if isinstance(values, np.ndarray):
# TODO: We would rather not get here
values = type(self._data)(values, dtype=self.dtype)
result = type(self)._simple_new(values, name=name)
result._cache = cache
return result
# --------------------------------------------------------------------
# Set Operation Methods
@Appender(Index.difference.__doc__)
def difference(self, other, sort=None):
new_idx = super().difference(other, sort=sort)._with_freq(None)
return new_idx
def intersection(self, other, sort=False):
Specialized intersection for DatetimeIndex/TimedeltaIndex.
May be much faster than Index.intersection
other : Same type as self or array-like
sort : False or None, default False
Sort the resulting index if possible.
.. versionadded:: 0.24.0
.. versionchanged:: 0.24.1
Changed the default to ``False`` to match the behaviour
from before 0.24.0.
.. versionchanged:: 0.25.0
The `sort` keyword is added
y : Index or same type as self
self._validate_sort_keyword(sort)
self._assert_can_do_setop(other)
if self.equals(other):
return self._get_reconciled_name_object(other)
if len(self) == 0:
return self.copy()
if len(other) == 0:
return other.copy()
if not isinstance(other, type(self)):
result = Index.intersection(self, other, sort=sort)
if isinstance(result, type(self)):
if result.freq is None:
result = result._with_freq(""infer"")
return result
elif (
other.freq is None
or self.freq is None
or other.freq != self.freq
or not other.freq.is_anchored()
or (not self.is_monotonic or not other.is_monotonic)
):
result = Index.intersection(self, other, sort=sort)
result = result._with_freq(""infer"")
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
else:
left, right = other, self
# after sorting, the intersection always starts with the right index
# and ends with the index of which the last elements is smallest
end = min(left[-1], right[-1])
start = right[0]
if end < start:
return type(self)(data=[], dtype=self.dtype, freq=self.freq)
else:
lslice = slice(*left.slice_locs(start, end))
left_chunk = left._values[lslice]
return self._shallow_copy(left_chunk)
def _can_fast_union(self, other) -> bool:
if not isinstance(other, type(self)):
return False
freq = self.freq
if freq is None or freq != other.freq:
return False
if not self.is_monotonic or not other.is_monotonic:
return False
if len(self) == 0 or len(other) == 0:
return True
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
else:
left, right = other, self
right_start = right[0]
left_end = left[-1]
# Only need to ""adjoin"", not overlap
try:
return (right_start == left_end + freq) or right_start in left
except ValueError:
# if we are comparing a freq that does not propagate timezones
# this will raise
return False
def _fast_union(self, other, sort=None):
if len(other) == 0:
return self.view(type(self))
if len(self) == 0:
return other.view(type(self))
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
elif sort is False:
# TDIs are not in the ""correct"" order and we don't want
#  to sort but want to remove overlaps
left, right = self, other
left_start = left[0]
loc = right.searchsorted(left_start, side=""left"")
right_chunk = right._values[:loc]
dates = concat_compat((left._values, right_chunk))
# TODO: can we infer that it has self.freq?
result = self._shallow_copy(dates)._with_freq(""infer"")
return result
left, right = other, self
left_end = left[-1]
right_end = right[-1]
# concatenate
if left_end < right_end:
loc = right.searchsorted(left_end, side=""right"")
right_chunk = right._values[loc:]
dates = concat_compat([left._values, right_chunk])
# TODO: can we infer that it has self.freq?
result = self._shallow_copy(dates)._with_freq(""infer"")
return result
else:
return left
def _union(self, other, sort):
if not len(other) or self.equals(other) or not len(self):
return super()._union(other, sort=sort)
# We are called by `union`, which is responsible for this validation
assert isinstance(other, type(self))
this, other = self._maybe_utc_convert(other)
if this._can_fast_union(other):
result = this._fast_union(other, sort=sort)
if result.freq is None:
result = result._with_freq(""infer"")
return result
i8self = Int64Index._simple_new(self.asi8, name=self.name)
i8other = Int64Index._simple_new(other.asi8, name=other.name)
i8result = i8self._union(i8other, sort=sort)
result = type(self)(i8result, dtype=self.dtype, freq=""infer"")
return result
# --------------------------------------------------------------------
# Join Methods
_join_precedence = 10
_inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
_outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
_left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
_left_indexer_unique = _join_i8_wrapper(
libjoin.left_join_indexer_unique, with_indexers=False
)
def join(
self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
):
See Index.join
""""""
if self._is_convertible_to_index_for_join(other):
try:
other = type(self)(other)
except (TypeError, ValueError):
pass
this, other = self._maybe_utc_convert(other)
return Index.join(
this,
other,
how=how,
level=level,
return_indexers=return_indexers,
sort=sort,
)
def _maybe_utc_convert(self, other):
this = self
if not hasattr(self, ""tz""):
return this, other
if isinstance(other, type(self)):
if self.tz is not None:
if other.tz is None:
raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
elif other.tz is not None:
raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
if not timezones.tz_compare(self.tz, other.tz):
this = self.tz_convert(""UTC"")
other = other.tz_convert(""UTC"")
return this, other
@classmethod
def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
""""""
return a boolean whether I can attempt conversion to a
DatetimeIndex/TimedeltaIndex
""""""
if isinstance(other, cls):
return False
elif len(other) > 0 and other.inferred_type not in (
""floating"",
""mixed-integer"",
""integer"",
""integer-na"",
""mixed-integer-float"",
""mixed"",
):
return True
return False
# --------------------------------------------------------------------
# List-Like Methods
def insert(self, loc, item):
Make new Index inserting new item at location
loc : int
item : object
if not either a Python datetime or a numpy integer-like, returned
Index dtype will be object rather than datetime.
new_index : Index
""""""
if isinstance(item, str):
# TODO: Why are strings special?
# TODO: Should we attempt _scalar_from_string?
return self.astype(object).insert(loc, item)
item = self._data._validate_insert_value(item)
freq = None
# check freq can be preserved on edge cases
if self.freq is not None:
if self.size:
if item is NaT:
pass
elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:
freq = self.freq
elif (loc == len(self)) and item - self.freq == self[-1]:
freq = self.freq
else:
# Adding a single item to an empty index may preserve freq
if self.freq.is_on_offset(item):
freq = self.freq
item = self._data._unbox_scalar(item)
new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])
arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
return type(self)._simple_new(arr, name=self.name)
msg = ""cannot insert DatetimeArray with incompatible label""
msg = ""cannot insert TimedeltaArray with incompatible label""
msg = ""cannot insert TimedeltaArray with incompatible label""
msg = ""cannot insert DatetimeArray with incompatible label"""
pandas,18,"if not isinstance(window, BaseIndexer):
self.min_periods or 1,","""skew"",
if not isinstance(self.window, BaseIndexer):
window_indexer.window_size,"
pandas,19,"# convert various list-like indexers
# to a list of keys
# we will use the *values* of the object
# and NOT the index if its a PandasObject
if isinstance(labels, ABCMultiIndex):
if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:
# Series, or 0,1 ndim ndarray
# GH 14730
key = list(key)
elif isinstance(key, ABCDataFrame):
# GH 15438
raise NotImplementedError(
""Indexing a MultiIndex with a ""
""DataFrame key is not ""
""implemented""
)
elif hasattr(key, ""ndim"") and key.ndim > 1:
raise NotImplementedError(
""Indexing a MultiIndex with a ""
""multidimensional key is not ""
""implemented""
)
if (
not isinstance(key, tuple)
and len(key)
and not isinstance(key[0], tuple)
):
key = tuple([key])",
pandas,2,"key = list(self._convert_key(key, is_setter=True))","key = list(self._convert_key(key, is_setter=True))
# GH 26989
# For series, unpacking key needs to result in the label.
# This is already the case for len(key) == 1; e.g. (1,)
if self.ndim == 1 and len(key) > 1:
key = (key,)"
pandas,20,"# TODO: going through __new__ raises on call to _validate_frequency;
#  are we passing incorrect freq?
return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)
# TODO: going through __new__ raises on call to _validate_frequency;
#  are we passing incorrect freq?
return type(dtindex)._simple_new(
shifted, freq=dtindex.freq, dtype=dtindex.dtype
)
# TODO: going through __new__ raises on call to _validate_frequency;
#  are we passing incorrect freq?
return type(dtindex)._simple_new(
shifted, freq=dtindex.freq, dtype=dtindex.dtype
)","return type(i)._simple_new(shifted, dtype=i.dtype)
return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)
return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)"
pandas,21,"if isinstance(key, list):
# handle the dup indexing case GH#4246
return self.loc[key]
return self.reindex(key)","# handle the dup indexing case GH#4246
return self.loc[key]"
pandas,22,"if isinstance(self.window, BaseIndexer):
validate_baseindexer_support(""count"")
if self.is_freq_type:","""count"",
# GH 32865. Using count with custom BaseIndexer subclass
# implementations shouldn't end up here
assert not isinstance(self.window, BaseIndexer)
# GH 32865. Use a custom count function implementation
# when using a BaseIndexer subclass as a window
if self.is_freq_type or isinstance(self.window, BaseIndexer):"
pandas,23,"return type(self)(data=[])
left_chunk = left.values[lslice]
exp = DatetimeIndex(rng.view(np.ndarray)[:5])
exp = DatetimeIndex(rng.view(np.ndarray)[:5])","return type(self)(data=[], dtype=self.dtype, freq=self.freq)
left_chunk = left._values[lslice]
exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=""B"")
assert smaller.freq == exp.freq
exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=""C"")
assert smaller.freq == exp.freq"
pandas,24,"dtype='datetime64[ns, US/Eastern]', freq='D')
dtype='datetime64[ns]', freq='D')
return self._simple_new(new_dates, dtype=dtype, freq=self.freq)","dtype='datetime64[ns, US/Eastern]', freq=None)
dtype='datetime64[ns]', freq=None)
freq = None
if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates[0])):
# we can preserve freq
# TODO: Also for fixed-offsets
freq = self.freq
elif tz is None and self.tz is None:
# no-op
freq = self.freq
return self._simple_new(new_dates, dtype=dtype, freq=freq)
dti._set_freq(""infer"")  # freq not preserved by tz_localize"
pandas,25,sarray = fields.build_isocalendar_sarray(self.asi8),"if self.tz is not None and not timezones.is_utc(self.tz):
values = self._local_timestamps()
else:
values = self.asi8
sarray = fields.build_isocalendar_sarray(values)"
pandas,26,"if skipna:
if skipna:","if skipna and good.any():
if skipna and good.any():"
pandas,27,freq = get_period_alias(freq),"res = get_period_alias(freq)
#  https://github.com/pandas-dev/pandas/issues/33358
if res is None:
base, stride = libfrequencies._base_and_stride(freq)
res = f""{stride}{base}""
freq = res"
pandas,28,"return [Series(others._values, index=others)]","return [Series(others._values, index=idx)]"
pandas,29,"if needs_float_conversion:
left = left.astype(""float"")
left.values[key] = value_left
if needs_float_conversion:
right = right.astype(""float"")
right.values[key] = value_right","if needs_float_conversion:
raise ValueError(""Cannot set float NaN to integer-backed IntervalArray"")
left._values[key] = value_left
right._values[key] = value_right"
pandas,3,"assert isinstance(self.index, PeriodIndex)
assert isinstance(self.index, DatetimeIndex)","if not isinstance(self.index, PeriodIndex):
raise TypeError(f""unsupported Type {type(self.index).__name__}"")
if not isinstance(self.index, DatetimeIndex):
raise TypeError(f""unsupported Type {type(self.index).__name__}"")"
pandas,30,"except (ValueError, OverflowError):","except (ValueError, OverflowError, TypeError):"
pandas,31,"if is_integer_dtype(vals):
elif is_datetime64_dtype(vals):","is_bool_dtype,
is_extension_array_dtype,
if is_integer_dtype(vals.dtype):
if is_extension_array_dtype(vals.dtype):
vals = vals.to_numpy(dtype=float, na_value=np.nan)
elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):
vals = vals.to_numpy(dtype=float, na_value=np.nan)
elif is_datetime64_dtype(vals.dtype):"
pandas,32,"# Copy to BytesIO, and ensure no encoding
contents = filepath_or_buffer.read()
try:
contents = contents.encode(self._encoding)
except UnicodeEncodeError:
pass
self.filepath_or_buffer = BytesIO(contents)","# Since xport files include non-text byte sequences, xport files
# should already be opened in binary mode in Python 3.
self.filepath_or_buffer = filepath_or_buffer"
pandas,33,data[self._mask] = data.min() - 1,"if self._mask.any():
data[self._mask] = data.min() - 1"
pandas,34,"ambiguous=""infer"",","# GH 25758: If DST lands at midnight (e.g. 'America/Havana'), user feedback
# has noted that ambiguous=True provides the most sensible result
ambiguous=True,"
pandas,35,"# To avoid a reference cycle, pass a weakref of self to _engine_type.
period = weakref.ref(self)","# To avoid a reference cycle, pass a weakref of self._values to _engine_type.
period = weakref.ref(self._values)"
pandas,36,"is_datetime64_dtype,
is_datetime64tz_dtype,
is_period_dtype,
is_timedelta64_dtype,
ABCDatetimeArray,
ABCTimedeltaArray,
elif isinstance(
obj,
(
ABCSeries,
np.ndarray,
ABCIndexClass,
ABCExtensionArray,
ABCDatetimeArray,
ABCTimedeltaArray,
),
):
return obj is None
return obj is None
is_extension = is_extension_array_dtype(obj)
if not is_extension:
# Avoid accessing `.values` on things like
# PeriodIndex, which may be expensive.
values = getattr(obj, ""_values"", obj)
else:
values = obj
if isinstance(obj, (ABCIndexClass, ABCSeries)):
values = obj._values
else:
values = obj
elif isinstance(obj, ABCDatetimeArray):
return obj.isna()
# Working around NumPy ticket 1542
shape = values.shape
if is_string_like_dtype(dtype):
# object array of strings
result = np.zeros(values.shape, dtype=bool)
else:
# object array of non-strings
result = np.empty(shape, dtype=bool)
vec = libmissing.isnaobj(values.ravel())
result[...] = vec.reshape(shape)
# Working around NumPy ticket 1542
shape = values.shape
if is_string_like_dtype(dtype):
result = np.zeros(values.shape, dtype=bool)
else:
result = np.empty(shape, dtype=bool)
vec = libmissing.isnaobj_old(values.ravel())
result[:] = vec.reshape(shape)
elif is_datetime64_dtype(dtype):
if (
is_datetime64_dtype(dtype)
or is_datetime64tz_dtype(dtype)
or is_timedelta64_dtype(dtype)
or is_period_dtype(dtype)
):","elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
return False
return False
is_extension = is_extension_array_dtype(obj.dtype)
values = getattr(obj, ""_values"", obj)
result = _isna_string_dtype(values, dtype, old=False)
result = _isna_string_dtype(values, dtype, old=True)
elif needs_i8_conversion(dtype):
# Working around NumPy ticket 1542
shape = values.shape
if is_string_like_dtype(dtype):
result = np.zeros(values.shape, dtype=bool)
else:
result = np.empty(shape, dtype=bool)
if old:
vec = libmissing.isnaobj_old(values.ravel())
else:
vec = libmissing.isnaobj(values.ravel())
result[...] = vec.reshape(shape)
return result
if needs_i8_conversion(dtype):"
pandas,37,,
pandas,38,"if rlocs == []:
clocs = [v if i > v else v - 1 for v in clocs]","if not rlocs:
clocs = [v if v < val else v - 1 for v in clocs]"
pandas,39,,
pandas,4,"return multi_join_idx, lidx, ridx","if return_indexers:
return multi_join_idx, lidx, ridx
else:
return multi_join_idx"
pandas,40,"_factorize_keys(left_keys[n], right_keys[n], sort=sort)
lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)
is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)","is_categorical,
_factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)
lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)
lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = ""inner""
""""""
Encode left and right keys as enumerated types.
This is used to get the join indexers to be used when merging DataFrames.
Parameters
----------
lk : array-like
Left key.
rk : array-like
Right key.
sort : bool, defaults to True
If True, the encoding is done such that the unique elements in the
keys are sorted.
how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’
Type of merge.
Returns
-------
array
Left (resp. right if called with `key='right'`) labels, as enumerated type.
array
Right (resp. left if called with `key='right'`) labels, as enumerated type.
int
Number of unique elements in union of left and right labels.
See Also
--------
merge : Merge DataFrame or named Series objects
with a database-style join.
algorithms.factorize : Encode the object as an enumerated type
or categorical variable.
Examples
--------
>>> lk = np.array([""a"", ""c"", ""b""])
>>> rk = np.array([""a"", ""c""])
Here, the unique values are `'a', 'b', 'c'`. With the default
`sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:
>>> pd.core.reshape.merge._factorize_keys(lk, rk)
(array([0, 2, 1]), array([0, 2]), 3)
With the `sort=False`, the encoding will correspond to the order
in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:
>>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)
(array([0, 1, 2]), array([0, 1]), 3)
""""""
is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)
assert is_categorical(lk) and is_categorical(rk)
lk = cast(Categorical, lk)
rk = cast(Categorical, rk)
if how == ""right"":
return rlab, llab, count"
pandas,41,"Modify Block in-place with new item value
Returns
-------
None
Set the value inplace, returning a a maybe different typed block.
def should_store(self, value):
def set(self, locs, values, check=False):
self.values = values
Set the value inplace, returning a same-typed block.
def should_store(self, value) -> bool:
def should_store(self, value) -> bool:
def should_store(self, value) -> bool:
def should_store(self, value) -> bool:
return is_datetime64_dtype(value.dtype)
Modify Block in-place with new item value
Returns
-------
None
def should_store(self, value) -> bool:
return is_timedelta64_dtype(value.dtype)
def should_store(self, value) -> bool:
def should_store(self, value) -> bool:","Modify block values in-place with new item value.
Notes
-----
`set` never creates a new array or new Block, whereas `setitem` _may_
create a new array and always creates a new Block.
Attempt self.values[indexer] = value, possibly creating a new array.
def should_store(self, value: ArrayLike) -> bool:
""""""
Can we set the given array-like value inplace?
""""""
def set(self, locs, values):
self.values[:] = values
Attempt self.values[indexer] = value, possibly creating a new array.
def should_store(self, value: ArrayLike) -> bool:
def should_store(self, value: ArrayLike) -> bool:
def should_store(self, value: ArrayLike) -> bool:
def should_store(self, value):
return is_dtype_equal(self.dtype, value.dtype)
See Block.set.__doc__
should_store = DatetimeBlock.should_store
def should_store(self, value: ArrayLike) -> bool:
def should_store(self, value: ArrayLike) -> bool:
def should_store(self, arr: ArrayLike):
return isinstance(arr, self._holder) and is_dtype_equal(self.dtype, arr.dtype)"
pandas,42,"elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):
elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):","elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):
elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):"
pandas,43,"left: ""DataFrame"", right, axis, default_axis: int, fill_value, level
if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):","left: ""DataFrame"", right, op, axis, default_axis: int, fill_value, level
if op is operator.pow or op is rpow:
# GH#32685 pow has special semantics for operating with null values
return False
if _should_reindex_frame_op(
self, other, op, axis, default_axis, fill_value, level
):"
pandas,44,"is_categorical,
if is_categorical(target):
elif self.is_all_dates and target.is_all_dates:  # GH 30399
tgt_values = target.asi8
if isinstance(target, PeriodIndex):
if target.freq != self.freq:
no_matches = -1 * np.ones(self.shape, dtype=np.intp)
return no_matches, no_matches
target = target.asi8","# Remove tz so Index will try non-DatetimeIndex inference
attributes.pop(""tz"", None)
if is_categorical_dtype(target.dtype):
ensure_platform_int,
def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
""""""
Can we compare values of the given dtype to our own?
""""""
raise AbstractMethodError(self)
@Appender(Index.get_indexer_non_unique.__doc__)
def get_indexer_non_unique(self, target):
target = ensure_index(target)
pself, ptarget = self._maybe_promote(target)
if pself is not self or ptarget is not target:
return pself.get_indexer_non_unique(ptarget)
if not self._is_comparable_dtype(target.dtype):
no_matches = -1 * np.ones(self.shape, dtype=np.intp)
return no_matches, no_matches
tgt_values = target.asi8
indexer, missing = self._engine.get_indexer_non_unique(tgt_values)
return ensure_platform_int(indexer), missing
_NS_DTYPE,
is_datetime64_any_dtype,
is_datetime64_dtype,
is_datetime64tz_dtype,
is_float,
is_integer,
is_scalar,
def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
""""""
Can we compare values of the given dtype to our own?
""""""
if not is_datetime64_any_dtype(dtype):
return False
if self.tz is not None:
# If we have tz, we can compare to tzaware
return is_datetime64tz_dtype(dtype)
# if we dont have tz, we can only compare to tznaive
return is_datetime64_dtype(dtype)
def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
""""""
Can we compare values of the given dtype to our own?
""""""
if not isinstance(dtype, PeriodDtype):
return False
return dtype.freq == self.freq
if not self._is_comparable_dtype(target.dtype):
no_matches = -1 * np.ones(self.shape, dtype=np.intp)
return no_matches, no_matches
target = target.asi8
def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
""""""
Can we compare values of the given dtype to our own?
""""""
return is_timedelta64_dtype(dtype)"
pandas,45,,
pandas,46,"return [
np.tile(
np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])
)
for i, x in enumerate(X)
]","# codes are all ndarrays, so cartesian_product is lossless
return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]
""""""
Index compat for np.tile.
Notes
-----
Does not support multi-dimensional `num`.
""""""
if isinstance(arr, np.ndarray):
return np.tile(arr, num)
# Otherwise we have an Index
taker = np.tile(np.arange(len(arr)), num)
return arr.take(taker)
def test_tzaware_retained(self):
x = date_range(""2000-01-01"", periods=2, tz=""US/Pacific"")
y = np.array([3, 4])
result1, result2 = cartesian_product([x, y])
expected = x.repeat(2)
tm.assert_index_equal(result1, expected)
def test_tzaware_retained_categorical(self):
x = date_range(""2000-01-01"", periods=2, tz=""US/Pacific"").astype(""category"")
y = np.array([3, 4])
result1, result2 = cartesian_product([x, y])
expected = x.repeat(2)
tm.assert_index_equal(result1, expected)"
pandas,47,,
pandas,48,except ValueError:,"except (ValueError, TypeError):"
pandas,49,,
pandas,5,"return multi_join_idx, lidx, ridx
join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)","if return_indexers:
return multi_join_idx, lidx, ridx
else:
return multi_join_idx
join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)
# GH 34074
midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[""a"", ""b"", ""c""])
midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[""a"", ""b""])
result = midx1.join(midx2, return_indexers=False)
tm.assert_index_equal(result, midx1)"
pandas,50,ret[mask] = False,"if opname == ""__ne__"":
ret[(self._codes == -1) & (other_codes == -1)] = True
else:
ret[mask] = False"
pandas,51,,
pandas,52,"# GH 27951
# temporary fix while we wait for NumPy bug 12629 to be fixed
val[isna(val)] = np.datetime64(""NaT"")
try:
sorter = np.lexsort((val, ids))
except TypeError:  # catches object dtypes
msg = f""val.dtype must be object, got {val.dtype}""
assert val.dtype == object, msg
val, _ = algorithms.factorize(val, sort=False)
sorter = np.lexsort((val, ids))
_isna = lambda a: a == -1
else:
_isna = isna
ids, val = ids[sorter], val[sorter]
inc = np.r_[1, val[1:] != val[:-1]]
mask = _isna(val)","codes, _ = algorithms.factorize(val, sort=False)
sorter = np.lexsort((codes, ids))
codes = codes[sorter]
ids = ids[sorter]
inc = np.r_[1, codes[1:] != codes[:-1]]
mask = codes == -1"
pandas,53,"if not self.holds_integer():
return self.index.get_value(self, label)
msg = (
""cannot do label indexing on Index ""
r""with these indexers \[0\] of type int""
)
with pytest.raises(TypeError, match=msg):
with pytest.raises(TypeError, match=msg):
def test_frame_raises_type_error(self):
msg = (
""cannot do label indexing on Index ""
r""with these indexers \[0\] of type int""
)
with pytest.raises(TypeError, match=msg):
with pytest.raises(TypeError, match=msg):","if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):
# Similar to Index.get_value, but we do not fall back to positional
loc = self.index.get_loc(label)
return self.index._get_values_for_loc(self, loc, label)
with pytest.raises(KeyError, match=""^0$""):
with pytest.raises(KeyError, match=""^0$""):
def test_frame_raises_key_error(self):
with pytest.raises(KeyError, match=""^0$""):
with pytest.raises(KeyError, match=""^0$""):"
pandas,54,"if not isinstance(indices, RangeIndex):","elif not isinstance(dtype, CategoricalDtype):
raise ValueError(f""Cannot not construct CategoricalDtype from {dtype}"")
if not isinstance(indices, (RangeIndex, CategoricalIndex)):
# TODO: CategoricalIndex can be re-allowed following GH#32167"
pandas,55,"def _iget_item_cache(self, item):
lower = self._take_with_is_copy(item, axis=self._info_axis_number)
ax = self.obj.axes[i]
if not ax.is_unique:
return False","def _iget_item_cache(self, item: int):
return self._ixs(item, axis=1)"
pandas,56,"series = self._iget_item_cache(col)
return com.maybe_box_datetimelike(series._values[index])","series = self._ixs(col, axis=1)
return series._values[index]"
pandas,57,"assert_categorical_equal(left.values, right.values, obj=f""{obj} category"")
if to_replace in cat.categories:
if isna(value):
cat.remove_categories(to_replace, inplace=True)
else:
index = categories.index(to_replace)
if value in cat.categories:
value_index = categories.index(value)
cat.remove_categories(to_replace, inplace=True)
categories[index] = value","check_category_order=True,
check_category_order : bool, default True
Whether to compare category order of internal Categoricals
.. versionadded:: 1.0.2
assert_categorical_equal(
left.values,
right.values,
obj=f""{obj} category"",
check_category_order=check_category_order,
)
# build a dict of (to replace -> value) pairs
if is_list_like(to_replace):
# if to_replace is list-like and value is scalar
replace_dict = {replace_value: value for replace_value in to_replace}
else:
# if both to_replace and value are scalar
replace_dict = {to_replace: value}
# other cases, like if both to_replace and value are list-like or if
# to_replace is a dict, are handled separately in NDFrame
for replace_value, new_value in replace_dict.items():
if replace_value in cat.categories:
if isna(new_value):
cat.remove_categories(replace_value, inplace=True)
continue
index = categories.index(replace_value)
if new_value in cat.categories:
value_index = categories.index(new_value)
cat.remove_categories(replace_value, inplace=True)
categories[index] = new_value"
pandas,58,codes = np.asarray(codes)  # #21767,"if is_extension_array_dtype(codes) and is_integer_dtype(codes):
# Avoid the implicit conversion of Int to object
if isna(codes).any():
raise ValueError(""codes cannot contain NA values"")
codes = codes.to_numpy(dtype=np.int64)
else:
codes = np.asarray(codes)"
pandas,59,window = self._get_window(other),window = self._get_window(other) if not self.is_freq_type else self.win_freq
pandas,6,"except (KeyError, IndexError):","except (KeyError, IndexError, ValueError):
# TODO: ValueError: Given date string not likely a datetime.
# should be KeyError?"
pandas,60,# name=func for WindowGroupByMixin._apply,"# name=func & raw=raw for WindowGroupByMixin._apply
raw=raw,"
pandas,61,return self._get_values(key),return self.iloc[key]
pandas,62,,
pandas,63,"for ax, i in zip(self.obj.axes, key):
if ax.is_integer():
if not is_integer(i):
raise ValueError(
""At based indexing on an integer index ""
""can only have integer indexers""
)
else:
if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):
raise ValueError(
""At based indexing on an non-integer ""
""index can only have non-integer ""
""indexers""
)
return key","lkey = list(key)
for n, (ax, i) in enumerate(zip(self.obj.axes, key)):
lkey[n] = ax._convert_scalar_indexer(i, kind=""loc"")
return tuple(lkey)"
pandas,64,self.df = df,self.df = df.reindex(columns=cols)
pandas,65,"need_text_wrapping = (BufferedIOBase, S3File)
need_text_wrapping = BufferedIOBase  # type: ignore
if not isinstance(f, BufferedIOBase):
if isinstance(src, BufferedIOBase):","need_text_wrapping = (BufferedIOBase, RawIOBase, S3File)
need_text_wrapping = (BufferedIOBase, RawIOBase)  # type: ignore
if not isinstance(f, (BufferedIOBase, RawIOBase)):
if isinstance(src, (BufferedIOBase, RawIOBase)):"
pandas,66,"new_values = self._data.fast_xs(loc)
# may need to box a datelike-scalar
#
# if we encounter an array-like and we only have 1 dim
# that means that their are list/ndarrays inside the Series!
# so just return them (GH 6394)
if not is_list_like(new_values) or self.ndim == 1:
return com.maybe_box_datetimelike(new_values)
return self._block.values[loc]","# In this case loc should be an integer
if self.ndim == 1:
# if we encounter an array-like and we only have 1 dim
# that means that their are list/ndarrays inside the Series!
# so just return them (GH 6394)
return self._values[loc]
new_values = self._data.fast_xs(loc)
raise NotImplementedError(""Use series._values[loc] instead"")"
pandas,67,,
pandas,68,,
pandas,69,if is_integer(i) and not ax.holds_integer():,if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):
pandas,7,"target = np.asarray(target)
left_distances = abs(self.values[left_indexer] - target)
right_distances = abs(self.values[right_indexer] - target)","left_distances = np.abs(self[left_indexer] - target)
right_distances = np.abs(self[right_indexer] - target)"
pandas,70,"# return the same type (Series) as our caller
cls = dtype.construct_array_type()
result = try_cast_to_ea(cls, result, dtype=dtype)
[1, 4, 7], index=pd.date_range(""1/1/2000"", periods=3, freq=""3T""), dtype=""Int64""
expected[""Group""] = expected[""Group_obj""].astype(""category"")","if len(result) and isinstance(result[0], dtype.type):
cls = dtype.construct_array_type()
result = try_cast_to_ea(cls, result, dtype=dtype)
elif (
how == ""add""
and is_integer_dtype(orig_values.dtype)
and is_extension_array_dtype(orig_values.dtype)
):
# We need this to ensure that Series[Int64Dtype].resample().sum()
# remains int64 dtype.
# Two options for avoiding this special case
# 1. mask-aware ops and avoid casting to float with NaN above
# 2. specify the result dtype when calling this method
result = result.astype(""int64"")
# https://github.com/pandas-dev/pandas/pull/31359
# This is currently failing to cast back to Int64Dtype.
# The presence of the NA causes two problems
# 1. NA is not an instance of Int64Dtype.type (numpy.int64)
# 2. The presence of an NA forces object type, so the non-NA values is
#    a Python int rather than a NumPy int64. Python ints aren't
#    instances of numpy.int64.
def aggfunc(x):
if all(x > 2):
return 1
else:
return pd.NA
df = pd.DataFrame({""A"": pd.array([1, 2, 3])})
result = df.groupby([1, 1, 2]).agg(aggfunc)
expected = pd.DataFrame({""A"": pd.array([1, pd.NA], dtype=""Int64"")}, index=[1, 2])
tm.assert_frame_equal(result, expected)
[1, 4, 7],
index=pd.date_range(""1/1/2000"", periods=3, freq=""3T""),
dtype=""float64"",
expected[""Group""] = expected[""Group_obj""]"
pandas,71,,
pandas,72,"# if we are an exact match (ex-broadcasting),
# then use the resultant dtype
len(arr_value.shape)
and arr_value.shape[0] == values.shape[0]
and arr_value.size == values.size","exact_match = (
len(arr_value.shape)
and arr_value.shape[0] == values.shape[0]
and arr_value.size == values.size
)
exact_match
and is_categorical_dtype(arr_value.dtype)
and not is_categorical_dtype(values)
# GH25495 - If the current dtype is not categorical,
# we need to create a new categorical block
return self.make_block(Categorical(self.values, dtype=arr_value.dtype))
# if we are an exact match (ex-broadcasting),
# then use the resultant dtype
elif exact_match:
values[indexer] = value"
pandas,73,"def _combine_frame(self, other, func, fill_value=None, level=None):
def _combine_match_index(self, other, func):
new_data = func(self.values.T, other.values).T
shape = result.shape
nan_mask = (zmask & (x == 0)).ravel()
neginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()
posinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()
result = result.astype(""float64"", copy=False).ravel()
np.putmask(result, nan_mask, np.nan)
np.putmask(result, posinf_mask, np.inf)
np.putmask(result, neginf_mask, -np.inf)
result = result.reshape(shape)","def _combine_frame(self, other: ""DataFrame"", func, fill_value=None):
def _combine_match_index(self, other: Series, func):
other_vals = other.values.reshape(-1, 1)
new_data = func(self.values, other_vals)
new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)
nan_mask = zmask & (x == 0)
neginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))
posinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))
result = result.astype(""float64"", copy=False)
result[nan_mask] = np.nan
result[posinf_mask] = np.inf
result[neginf_mask] = -np.inf"
pandas,74,"if isinstance(data, TimedeltaArray):","if isinstance(data, TimedeltaArray) and freq is None:"
pandas,75,"except (TypeError, ValueError):
return self._get_string_slice(key)
except (TypeError, KeyError, ValueError, OverflowError):
key = asdt","except (TypeError, ValueError, OverflowError):
loc = self._get_string_slice(key)
return loc
except (TypeError, ValueError):
grp = resolution.Resolution.get_freq_group(reso)
freqn = resolution.get_freq_group(self.freq)
# _get_string_slice will handle cases where grp < freqn
assert grp >= freqn
if grp == freqn:
key = Period(asdt, freq=self.freq)
loc = self.get_loc(key, method=method, tolerance=tolerance)
return loc
elif method is None:
raise KeyError(key)
else:
key = asdt"
pandas,76,"except (TypeError, ValueError):","except (TypeError, ValueError, OverflowError):"
pandas,77,"result = libops.vec_binop(x, y, op)
return result","result = libops.vec_binop(x.ravel(), y.ravel(), op)
return result.reshape(x.shape)"
pandas,78,"result = Series(result, index=labels)","result = self._constructor_sliced(result, index=labels)"
pandas,79,"except (KeyError, TypeError):
except TypeError:","except (KeyError, TypeError, InvalidIndexError):
if not is_scalar(key):
raise InvalidIndexError(key)
if not is_scalar(key):
raise InvalidIndexError(key)
except (TypeError, InvalidIndexError):
except InvalidIndexError:
# e.g. slice
self._set_with(key, value)"
pandas,8,"if not mask.any():
if inplace:
return [self]
return [self.copy()]",
pandas,80,"arr = operator.inv(com.values_from_object(self))
return self.__array_wrap__(arr)
result = result.to_dense()
return pd.BooleanDtype()
values = np.array([True, False, True, False], dtype=""bool"")
mask = np.array([False, False, False, True], dtype=""bool"")
result = BooleanArray(values, mask)
expected = pd.array([True, False, True, None], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
with pytest.raises(TypeError, match=""values should be boolean numpy array""):
BooleanArray(values.tolist(), mask)
with pytest.raises(TypeError, match=""mask should be boolean numpy array""):
BooleanArray(values, mask.tolist())
with pytest.raises(TypeError, match=""values should be boolean numpy array""):
BooleanArray(values.astype(int), mask)
with pytest.raises(TypeError, match=""mask should be boolean numpy array""):
BooleanArray(values, None)
with pytest.raises(ValueError, match=""values must be a 1D array""):
BooleanArray(values.reshape(1, -1), mask)
with pytest.raises(ValueError, match=""mask must be a 1D array""):
BooleanArray(values, mask.reshape(1, -1))
values = np.array([True, False, True, False], dtype=""bool"")
mask = np.array([False, False, False, True], dtype=""bool"")
result = BooleanArray(values, mask)
assert result._data is values
assert result._mask is mask
result = BooleanArray(values, mask, copy=True)
assert result._data is not values
assert result._mask is not mask
expected = BooleanArray(
np.array([True, False, True]), np.array([False, False, False])
)
result = pd.array([True, False, True], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = pd.array(np.array([True, False, True]), dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = pd.array(np.array([True, False, True], dtype=object), dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
# with missing values
expected = BooleanArray(
np.array([True, False, True]), np.array([False, False, True])
)
result = pd.array([True, False, None], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = pd.array(np.array([True, False, None], dtype=object), dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
expected = BooleanArray(np.array([True, True, True]), np.array([True, True, True]))
result = pd.array([None, None, None], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = pd.array(np.array([None, None, None], dtype=object), dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
""a, b"",
[
([True, False, None, np.nan, pd.NA], [True, False, None, None, None]),
([True, np.nan], [True, None]),
([True, pd.NA], [True, None]),
([np.nan, np.nan], [None, None]),
(np.array([np.nan, np.nan], dtype=float), [None, None]),
],
result = pd.array(a, dtype=""boolean"")
expected = pd.array(b, dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
""values"",
[
[""foo"", ""bar""],
[""1"", ""2""],
# ""foo"",
[1, 2],
[1.0, 2.0],
pd.date_range(""20130101"", periods=2),
np.array([""foo""]),
np.array([1, 2]),
np.array([1.0, 2.0]),
[np.nan, {""a"": 1}],
],
# error in converting existing arrays to BooleanArray
with pytest.raises(TypeError):
pd.array(values, dtype=""boolean"")
result = pd.array(np.array([1, 0, 1, 0]), dtype=""boolean"")
expected = pd.array([True, False, True, False], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
# with missing values
result = pd.array(np.array([1, 0, 1, None]), dtype=""boolean"")
expected = pd.array([True, False, True, None], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = pd.array(np.array([1.0, 0.0, 1.0, 0.0]), dtype=""boolean"")
expected = pd.array([True, False, True, False], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
# with missing values
result = pd.array(np.array([1.0, 0.0, 1.0, np.nan]), dtype=""boolean"")
expected = pd.array([True, False, True, None], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
# integers of 0's and 1's
result = pd.array([1, 0, 1, 0], dtype=""boolean"")
expected = pd.array([True, False, True, False], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
# with missing values
result = pd.array([1, 0, 1, None], dtype=""boolean"")
expected = pd.array([True, False, True, None], dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
# TODO this is currently not public API
values = np.array([True, False, True, False], dtype=""bool"")
mask = np.array([False, False, False, True], dtype=""bool"")
result = BooleanArray(*coerce_to_array(values, mask=mask))
expected = BooleanArray(values, mask)
tm.assert_extension_array_equal(result, expected)
assert result._data is values
assert result._mask is mask
result = BooleanArray(*coerce_to_array(values, mask=mask, copy=True))
expected = BooleanArray(values, mask)
tm.assert_extension_array_equal(result, expected)
assert result._data is not values
assert result._mask is not mask
# mixed missing from values and mask
values = [True, False, None, False]
mask = np.array([False, False, False, True], dtype=""bool"")
result = BooleanArray(*coerce_to_array(values, mask=mask))
expected = BooleanArray(
np.array([True, False, True, True]), np.array([False, False, True, True])
)
tm.assert_extension_array_equal(result, expected)
result = BooleanArray(*coerce_to_array(np.array(values, dtype=object), mask=mask))
tm.assert_extension_array_equal(result, expected)
result = BooleanArray(*coerce_to_array(values, mask=mask.tolist()))
tm.assert_extension_array_equal(result, expected)
# raise errors for wrong dimension
values = np.array([True, False, True, False], dtype=""bool"")
mask = np.array([False, False, False, True], dtype=""bool"")
with pytest.raises(ValueError, match=""values must be a 1D list-like""):
coerce_to_array(values.reshape(1, -1))
with pytest.raises(ValueError, match=""mask must be a 1D list-like""):
coerce_to_array(values, mask=mask.reshape(1, -1))
# passing BooleanArray to coerce_to_array
values = np.array([True, False, True, False], dtype=""bool"")
mask = np.array([False, False, False, True], dtype=""bool"")
arr = BooleanArray(values, mask)
result = BooleanArray(*coerce_to_array(arr))
tm.assert_extension_array_equal(result, arr)
# no copy
assert result._data is arr._data
assert result._mask is arr._mask
result = BooleanArray(*coerce_to_array(arr), copy=True)
tm.assert_extension_array_equal(result, arr)
assert result._data is not arr._data
assert result._mask is not arr._mask
with pytest.raises(ValueError, match=""cannot pass mask for BooleanArray input""):
coerce_to_array(arr, mask=mask)
# with missing values -> object dtype
arr = pd.array([True, False, None], dtype=""boolean"")
result = np.array(arr)
expected = np.array([True, False, pd.NA], dtype=""object"")
tm.assert_numpy_array_equal(result, expected)
# also with no missing values -> object dtype
arr = pd.array([True, False, True], dtype=""boolean"")
result = np.array(arr)
expected = np.array([True, False, True], dtype=""object"")
tm.assert_numpy_array_equal(result, expected)
# force bool dtype
result = np.array(arr, dtype=""bool"")
expected = np.array([True, False, True], dtype=""bool"")
tm.assert_numpy_array_equal(result, expected)
# with missing values will raise error
arr = pd.array([True, False, None], dtype=""boolean"")
with pytest.raises(ValueError):
np.array(arr, dtype=""bool"")
result = BooleanArray._from_sequence_of_strings(
np.array([""True"", ""False"", np.nan], dtype=object)
)
expected = BooleanArray(
np.array([True, False, False]), np.array([False, False, True])
)
tm.assert_extension_array_equal(result, expected)
with pytest.raises(ValueError, match=""cannot be cast""):
BooleanArray._from_sequence_of_strings([""donkey""])
df = pd.DataFrame({""A"": pd.array([True, False, None], dtype=""boolean"")})
expected = ""       A\n0   True\n1  False\n2   <NA>""
assert repr(df) == expected
expected = ""0     True\n1    False\n2     <NA>\nName: A, dtype: boolean""
assert repr(df.A) == expected
expected = ""<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean""
assert repr(df.A.array) == expected
con = pd.Series if box else pd.array
# default (with or without missing values) -> object dtype
arr = con([True, False, True], dtype=""boolean"")
result = arr.to_numpy()
expected = np.array([True, False, True], dtype=""object"")
tm.assert_numpy_array_equal(result, expected)
arr = con([True, False, None], dtype=""boolean"")
result = arr.to_numpy()
expected = np.array([True, False, pd.NA], dtype=""object"")
tm.assert_numpy_array_equal(result, expected)
arr = con([True, False, None], dtype=""boolean"")
result = arr.to_numpy(dtype=""str"")
expected = np.array([True, False, pd.NA], dtype=""<U5"")
tm.assert_numpy_array_equal(result, expected)
# no missing values -> can convert to bool, otherwise raises
arr = con([True, False, True], dtype=""boolean"")
result = arr.to_numpy(dtype=""bool"")
expected = np.array([True, False, True], dtype=""bool"")
tm.assert_numpy_array_equal(result, expected)
arr = con([True, False, None], dtype=""boolean"")
with pytest.raises(ValueError, match=""cannot convert to 'bool'-dtype""):
result = arr.to_numpy(dtype=""bool"")
# specify dtype and na_value
arr = con([True, False, None], dtype=""boolean"")
result = arr.to_numpy(dtype=object, na_value=None)
expected = np.array([True, False, None], dtype=""object"")
tm.assert_numpy_array_equal(result, expected)
result = arr.to_numpy(dtype=bool, na_value=False)
expected = np.array([True, False, False], dtype=""bool"")
tm.assert_numpy_array_equal(result, expected)
result = arr.to_numpy(dtype=""int64"", na_value=-99)
expected = np.array([1, 0, -99], dtype=""int64"")
tm.assert_numpy_array_equal(result, expected)
result = arr.to_numpy(dtype=""float64"", na_value=np.nan)
expected = np.array([1, 0, np.nan], dtype=""float64"")
tm.assert_numpy_array_equal(result, expected)
# converting to int or float without specifying na_value raises
with pytest.raises(ValueError, match=""cannot convert to 'int64'-dtype""):
arr.to_numpy(dtype=""int64"")
with pytest.raises(ValueError, match=""cannot convert to 'float64'-dtype""):
arr.to_numpy(dtype=""float64"")
# to_numpy can be zero-copy if no missing values
arr = pd.array([True, False, True], dtype=""boolean"")
result = arr.to_numpy(dtype=bool)
result[0] = False
tm.assert_extension_array_equal(
arr, pd.array([False, False, True], dtype=""boolean"")
)
arr = pd.array([True, False, True], dtype=""boolean"")
result = arr.to_numpy(dtype=bool, copy=True)
result[0] = False
tm.assert_extension_array_equal(arr, pd.array([True, False, True], dtype=""boolean""))
# with missing values
arr = pd.array([True, False, None], dtype=""boolean"")
with pytest.raises(ValueError, match=""cannot convert NA to integer""):
arr.astype(""int64"")
with pytest.raises(ValueError, match=""cannot convert float NaN to""):
arr.astype(""bool"")
result = arr.astype(""float64"")
expected = np.array([1, 0, np.nan], dtype=""float64"")
tm.assert_numpy_array_equal(result, expected)
result = arr.astype(""str"")
expected = np.array([""True"", ""False"", ""<NA>""], dtype=""object"")
tm.assert_numpy_array_equal(result, expected)
# no missing values
arr = pd.array([True, False, True], dtype=""boolean"")
result = arr.astype(""int64"")
expected = np.array([1, 0, 1], dtype=""int64"")
tm.assert_numpy_array_equal(result, expected)
result = arr.astype(""bool"")
expected = np.array([True, False, True], dtype=""bool"")
tm.assert_numpy_array_equal(result, expected)
# astype to BooleanArray
arr = pd.array([True, False, None], dtype=""boolean"")
result = arr.astype(""boolean"")
tm.assert_extension_array_equal(result, arr)
result = arr.astype(pd.BooleanDtype())
tm.assert_extension_array_equal(result, arr)
# astype to IntegerArray
arr = pd.array([True, False, None], dtype=""boolean"")
result = arr.astype(""Int64"")
expected = pd.array([1, 0, None], dtype=""Int64"")
tm.assert_extension_array_equal(result, expected)
arr = pd.array([True, False, None], dtype=""boolean"")
expected = pd.array([True, None, None], dtype=""boolean"")
arr[1] = na
tm.assert_extension_array_equal(arr, expected)
""ufunc"", [np.add, np.logical_or, np.logical_and, np.logical_xor]
# two BooleanArrays
a = pd.array([True, False, None], dtype=""boolean"")
result = ufunc(a, a)
expected = pd.array(ufunc(a._data, a._data), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_extension_array_equal(result, expected)
s = pd.Series(a)
result = ufunc(s, a)
expected = pd.Series(ufunc(a._data, a._data), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_series_equal(result, expected)
# Boolean with numpy array
arr = np.array([True, True, False])
result = ufunc(a, arr)
expected = pd.array(ufunc(a._data, arr), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_extension_array_equal(result, expected)
result = ufunc(arr, a)
expected = pd.array(ufunc(arr, a._data), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_extension_array_equal(result, expected)
# BooleanArray with scalar
result = ufunc(a, True)
expected = pd.array(ufunc(a._data, True), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_extension_array_equal(result, expected)
result = ufunc(True, a)
expected = pd.array(ufunc(True, a._data), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_extension_array_equal(result, expected)
# not handled types
with pytest.raises(TypeError):
ufunc(a, ""test"")
a = pd.array([True, False, None], dtype=""boolean"")
result = ufunc(a)
expected = pd.array(ufunc(a._data), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_extension_array_equal(result, expected)
s = pd.Series(a)
result = ufunc(s)
expected = pd.Series(ufunc(a._data), dtype=""boolean"")
expected[a._mask] = np.nan
tm.assert_series_equal(result, expected)
a = pd.array(values, dtype=""boolean"")
with pytest.raises(NotImplementedError):
np.add.reduce(a)
def test_numpy_scalars_ok(self, all_logical_operators):
a = pd.array([True, False, None], dtype=""boolean"")
op = getattr(a, all_logical_operators)
tm.assert_extension_array_equal(op(True), op(np.bool(True)))
tm.assert_extension_array_equal(op(False), op(np.bool(False)))
def get_op_from_name(self, op_name):
short_opname = op_name.strip(""_"")
short_opname = short_opname if ""xor"" in short_opname else short_opname + ""_""
try:
op = getattr(operator, short_opname)
except AttributeError:
# Assume it is the reverse operator
rop = getattr(operator, short_opname[1:])
op = lambda x, y: rop(y, x)
return op
def test_empty_ok(self, all_logical_operators):
a = pd.array([], dtype=""boolean"")
op_name = all_logical_operators
result = getattr(a, op_name)(True)
tm.assert_extension_array_equal(a, result)
result = getattr(a, op_name)(False)
tm.assert_extension_array_equal(a, result)
# TODO: pd.NA
# result = getattr(a, op_name)(pd.NA)
# tm.assert_extension_array_equal(a, result)
def test_logical_length_mismatch_raises(self, all_logical_operators):
op_name = all_logical_operators
a = pd.array([True, False, None], dtype=""boolean"")
msg = ""Lengths must match to compare""
with pytest.raises(ValueError, match=msg):
getattr(a, op_name)([True, False])
with pytest.raises(ValueError, match=msg):
getattr(a, op_name)(np.array([True, False]))
with pytest.raises(ValueError, match=msg):
getattr(a, op_name)(pd.array([True, False], dtype=""boolean""))
def test_logical_nan_raises(self, all_logical_operators):
op_name = all_logical_operators
a = pd.array([True, False, None], dtype=""boolean"")
msg = ""Got float instead""
with pytest.raises(TypeError, match=msg):
getattr(a, op_name)(np.nan)
@pytest.mark.parametrize(""other"", [""a"", 1])
def test_non_bool_or_na_other_raises(self, other, all_logical_operators):
a = pd.array([True, False], dtype=""boolean"")
with pytest.raises(TypeError, match=str(type(other).__name__)):
getattr(a, all_logical_operators)(other)
def test_kleene_or(self):
# A clear test of behavior.
a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
b = pd.array([True, False, None] * 3, dtype=""boolean"")
result = a | b
expected = pd.array(
[True, True, True, True, False, None, True, None, None], dtype=""boolean""
)
tm.assert_extension_array_equal(result, expected)
result = b | a
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
tm.assert_extension_array_equal(
a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
)
tm.assert_extension_array_equal(
b, pd.array([True, False, None] * 3, dtype=""boolean"")
)
@pytest.mark.parametrize(
""other, expected"",
[
(pd.NA, [True, None, None]),
(True, [True, True, True]),
(np.bool_(True), [True, True, True]),
(False, [True, False, None]),
(np.bool_(False), [True, False, None]),
],
)
def test_kleene_or_scalar(self, other, expected):
# TODO: test True & False
a = pd.array([True, False, None], dtype=""boolean"")
result = a | other
expected = pd.array(expected, dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = other | a
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
tm.assert_extension_array_equal(
a, pd.array([True, False, None], dtype=""boolean"")
)
def test_kleene_and(self):
# A clear test of behavior.
a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
b = pd.array([True, False, None] * 3, dtype=""boolean"")
result = a & b
expected = pd.array(
[True, False, None, False, False, False, None, False, None], dtype=""boolean""
)
tm.assert_extension_array_equal(result, expected)
result = b & a
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
tm.assert_extension_array_equal(
a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
)
tm.assert_extension_array_equal(
b, pd.array([True, False, None] * 3, dtype=""boolean"")
)
@pytest.mark.parametrize(
""other, expected"",
[
(pd.NA, [None, False, None]),
(True, [True, False, None]),
(False, [False, False, False]),
(np.bool_(True), [True, False, None]),
(np.bool_(False), [False, False, False]),
],
)
def test_kleene_and_scalar(self, other, expected):
a = pd.array([True, False, None], dtype=""boolean"")
result = a & other
expected = pd.array(expected, dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = other & a
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
tm.assert_extension_array_equal(
a, pd.array([True, False, None], dtype=""boolean"")
)
def test_kleene_xor(self):
a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
b = pd.array([True, False, None] * 3, dtype=""boolean"")
result = a ^ b
expected = pd.array(
[False, True, None, True, False, None, None, None, None], dtype=""boolean""
)
tm.assert_extension_array_equal(result, expected)
result = b ^ a
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
tm.assert_extension_array_equal(
a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
)
tm.assert_extension_array_equal(
b, pd.array([True, False, None] * 3, dtype=""boolean"")
)
@pytest.mark.parametrize(
""other, expected"",
[
(pd.NA, [None, None, None]),
(True, [False, True, None]),
(np.bool_(True), [False, True, None]),
(np.bool_(False), [True, False, None]),
],
)
def test_kleene_xor_scalar(self, other, expected):
a = pd.array([True, False, None], dtype=""boolean"")
result = a ^ other
expected = pd.array(expected, dtype=""boolean"")
tm.assert_extension_array_equal(result, expected)
result = other ^ a
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
tm.assert_extension_array_equal(
a, pd.array([True, False, None], dtype=""boolean"")
)
@pytest.mark.parametrize(
""other"", [True, False, pd.NA, [True, False, None] * 3],
)
def test_no_masked_assumptions(self, other, all_logical_operators):
# The logical operations should not assume that masked values are False!
a = pd.arrays.BooleanArray(
np.array([True, True, True, False, False, False, True, False, True]),
np.array([False] * 6 + [True, True, True]),
)
b = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
if isinstance(other, list):
other = pd.array(other, dtype=""boolean"")
result = getattr(a, all_logical_operators)(other)
expected = getattr(b, all_logical_operators)(other)
tm.assert_extension_array_equal(result, expected)
if isinstance(other, BooleanArray):
other._data[other._mask] = True
a._data[a._mask] = False
result = getattr(a, all_logical_operators)(other)
expected = getattr(b, all_logical_operators)(other)
tm.assert_extension_array_equal(result, expected)
def _compare_other(self, data, op_name, other):
op = self.get_op_from_name(op_name)
# array
result = pd.Series(op(data, other))
expected = pd.Series(op(data._data, other), dtype=""boolean"")
# propagate NAs
expected[data._mask] = pd.NA
tm.assert_series_equal(result, expected)
# series
s = pd.Series(data)
result = op(s, other)
expected = pd.Series(data._data)
expected = op(expected, other)
expected = expected.astype(""boolean"")
# propagate NAs
expected[data._mask] = pd.NA
tm.assert_series_equal(result, expected)
op_name = all_compare_operators
self._compare_other(data, op_name, True)
op_name = all_compare_operators
other = pd.array([True] * len(data), dtype=""boolean"")
self._compare_other(data, op_name, other)
other = np.array([True] * len(data))
self._compare_other(data, op_name, other)
other = pd.Series([True] * len(data))
self._compare_other(data, op_name, other)
@pytest.mark.parametrize(""other"", [True, False, pd.NA])
def test_scalar(self, other, all_compare_operators):
op = self.get_op_from_name(all_compare_operators)
a = pd.array([True, False, None], dtype=""boolean"")
result = op(a, other)
if other is pd.NA:
expected = pd.array([None, None, None], dtype=""boolean"")
else:
values = op(a._data, other)
expected = BooleanArray(values, a._mask, copy=True)
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
result[0] = None
tm.assert_extension_array_equal(
a, pd.array([True, False, None], dtype=""boolean"")
def test_array(self, all_compare_operators):
op = self.get_op_from_name(all_compare_operators)
a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
b = pd.array([True, False, None] * 3, dtype=""boolean"")
result = op(a, b)
values = op(a._data, b._data)
mask = a._mask | b._mask
expected = BooleanArray(values, mask)
tm.assert_extension_array_equal(result, expected)
# ensure we haven't mutated anything inplace
result[0] = None
tm.assert_extension_array_equal(
a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
tm.assert_extension_array_equal(
b, pd.array([True, False, None] * 3, dtype=""boolean"")
def test_error(self, data, all_arithmetic_operators):
# invalid ops
op = all_arithmetic_operators
s = pd.Series(data)
ops = getattr(s, op)
opa = getattr(data, op)
# invalid scalars
with pytest.raises(TypeError):
ops(""foo"")
with pytest.raises(TypeError):
ops(pd.Timestamp(""20180101""))
# invalid array-likes
if op not in (""__mul__"", ""__rmul__""):
# TODO(extension) numpy's mul with object array sees booleans as numbers
with pytest.raises(TypeError):
ops(pd.Series(""foo"", index=s.index))
# 2d
result = opa(pd.DataFrame({""A"": s}))
assert result is NotImplemented
with pytest.raises(NotImplementedError):
opa(np.arange(len(s)).reshape(-1, len(s)))
op = all_numeric_reductions
s = pd.Series(data)
if dropna:
s = s.dropna()
if op in (""sum"", ""prod""):
assert isinstance(getattr(s, op)(), np.int64)
elif op in (""min"", ""max""):
assert isinstance(getattr(s, op)(), np.bool_)
else:
# ""mean"", ""std"", ""var"", ""median"", ""kurt"", ""skew""
assert isinstance(getattr(s, op)(), np.float64)
""values, exp_any, exp_all, exp_any_noskip, exp_all_noskip"",
[
([True, pd.NA], True, True, True, pd.NA),
([False, pd.NA], False, False, pd.NA, False),
([pd.NA], False, True, pd.NA, pd.NA),
([], False, True, False, True),
],
# the methods return numpy scalars
exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)
exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)
exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)
exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)
for con in [pd.array, pd.Series]:
a = con(values, dtype=""boolean"")
assert a.any() is exp_any
assert a.all() is exp_all
assert a.any(skipna=False) is exp_any_noskip
assert a.all(skipna=False) is exp_all_noskip
assert np.any(a.any()) is exp_any
assert np.all(a.all()) is exp_all
# protocol added in 0.15.0
import pyarrow as pa
arr = pa.array(data)
# TODO use to_numpy(na_value=None) here
data_object = np.array(data, dtype=object)
data_object[data.isna()] = None
expected = pa.array(data_object, type=pa.bool_(), from_pandas=True)
assert arr.equals(expected)
# roundtrip possible from arrow 1.0.0
import pyarrow as pa
data = pd.array([True, False, None], dtype=""boolean"")
df = pd.DataFrame({""a"": data})
table = pa.table(df)
assert table.field(""a"").type == ""bool""
result = table.to_pandas()
assert isinstance(result[""a""].dtype, pd.BooleanDtype)
tm.assert_frame_equal(result, df)
arr = pd.array([True, False, pd.NA], dtype=""boolean"")
result = arr.value_counts(dropna=False)
expected = pd.Series([1, 1, 1], index=[True, False, pd.NA], dtype=""Int64"")
tm.assert_series_equal(result, expected)
result = arr.value_counts(dropna=True)
expected = pd.Series([1, 1], index=[True, False], dtype=""Int64"")
tm.assert_series_equal(result, expected)
a = pd.array(
[True, True, False, False, True, None, True, None, False], dtype=""boolean""
)
result = pd.core.algorithms.diff(a, 1)
expected = pd.array(
[None, False, True, False, True, None, None, None, None], dtype=""boolean""
)
tm.assert_extension_array_equal(result, expected)
s = pd.Series(a)
result = s.diff()
expected = pd.Series(expected)
tm.assert_series_equal(result, expected)","def __invert__(self):
return type(self)(~self._data, self._mask)
new_data = self._data.apply(operator.invert)
result = self._constructor(new_data).__finalize__(self)
return result
result = np.asarray(result)
return BooleanDtype()
return pd.array(np.ones(100), dtype=dtype)
return pd.array([np.nan, True], dtype=dtype)
return pd.array([True, True, False], dtype=dtype)
return pd.array([True, np.nan, False], dtype=dtype)
# we are pd.NA
return lambda x, y: x is pd.NA and y is pd.NA
return pd.NA
b = True
a = False
na = np.nan
return pd.array([b, b, na, na, a, a, b], dtype=dtype)
pass
pass
pass
pass
pass
pass
def check_opname(self, s, op_name, other, exc=None):
# overwriting to indicate ops don't raise an error
super().check_opname(s, op_name, other, exc=None)
def _check_op(self, s, op, other, op_name, exc=NotImplementedError):
if exc is None:
if op_name in (""__sub__"", ""__rsub__""):
# subtraction for bools raises TypeError (but not yet in 1.13)
if _np_version_under1p14:
pytest.skip(""__sub__ does not yet raise in numpy 1.13"")
with pytest.raises(TypeError):
op(s, other)
return
result = op(s, other)
expected = s.combine(other, op)
if op_name in (
""__floordiv__"",
""__rfloordiv__"",
""__pow__"",
""__rpow__"",
""__mod__"",
""__rmod__"",
):
# combine keeps boolean type
expected = expected.astype(""Int8"")
elif op_name in (""__truediv__"", ""__rtruediv__""):
# combine with bools does not generate the correct result
#  (numpy behaviour for div is to regard the bools as numeric)
expected = s.astype(float).combine(other, op)
if op_name == ""__rpow__"":
# for rpow, combine does not propagate NaN
expected[result.isna()] = np.nan
self.assert_series_equal(result, expected)
else:
with pytest.raises(exc):
op(s, other)
def _check_divmod_op(self, s, op, other, exc=None):
# override to not raise an error
super()._check_divmod_op(s, op, other, None)
@pytest.mark.skip(reason=""BooleanArray does not error on ops"")
def test_error(self, data, all_arithmetic_operators):
# other specific errors tested in the boolean array specific tests
pass
def check_opname(self, s, op_name, other, exc=None):
# overwriting to indicate ops don't raise an error
super().check_opname(s, op_name, other, exc=None)
def _compare_other(self, s, data, op_name, other):
self.check_opname(s, op_name, other)
@pytest.mark.skip(reason=""Tested in tests/arrays/test_boolean.py"")
pass
@pytest.mark.skip(reason=""Tested in tests/arrays/test_boolean.py"")
pass
pass
@pytest.mark.parametrize(""na_sentinel"", [-1, -2])
def test_factorize(self, data_for_grouping, na_sentinel):
# override because we only have 2 unique values
labels, uniques = pd.factorize(data_for_grouping, na_sentinel=na_sentinel)
expected_labels = np.array(
[0, 0, na_sentinel, na_sentinel, 1, 1, 0], dtype=np.intp
)
expected_uniques = data_for_grouping.take([0, 4])
tm.assert_numpy_array_equal(labels, expected_labels)
self.assert_extension_array_equal(uniques, expected_uniques)
def test_combine_le(self, data_repeated):
# override because expected needs to be boolean instead of bool dtype
orig_data1, orig_data2 = data_repeated(2)
s1 = pd.Series(orig_data1)
s2 = pd.Series(orig_data2)
result = s1.combine(s2, lambda x1, x2: x1 <= x2)
expected = pd.Series(
[a <= b for (a, b) in zip(list(orig_data1), list(orig_data2))],
dtype=""boolean"",
self.assert_series_equal(result, expected)
val = s1.iloc[0]
result = s1.combine(val, lambda x1, x2: x1 <= x2)
expected = pd.Series([a <= val for a in list(orig_data1)], dtype=""boolean"")
self.assert_series_equal(result, expected)
def test_searchsorted(self, data_for_sorting, as_series):
# override because we only have 2 unique values
data_for_sorting = pd.array([True, False], dtype=""boolean"")
b, a = data_for_sorting
arr = type(data_for_sorting)._from_sequence([a, b])
if as_series:
arr = pd.Series(arr)
assert arr.searchsorted(a) == 0
assert arr.searchsorted(a, side=""right"") == 1
assert arr.searchsorted(b) == 1
assert arr.searchsorted(b, side=""right"") == 2
result = arr.searchsorted(arr.take([0, 1]))
expected = np.array([0, 1], dtype=np.intp)
tm.assert_numpy_array_equal(result, expected)
# sorter
sorter = np.array([1, 0])
assert data_for_sorting.searchsorted(a, sorter=sorter) == 0
@pytest.mark.skip(reason=""uses nullable integer"")
def test_value_counts(self, all_data, dropna):
return super().test_value_counts(all_data, dropna)
pass
""""""
Groupby-specific tests are overridden because boolean only has 2
unique values, base tests uses 3 groups.
""""""
def test_grouping_grouper(self, data_for_grouping):
df = pd.DataFrame(
{""A"": [""B"", ""B"", None, None, ""A"", ""A"", ""B""], ""B"": data_for_grouping}
)
gr1 = df.groupby(""A"").grouper.groupings[0]
gr2 = df.groupby(""B"").grouper.groupings[0]
tm.assert_numpy_array_equal(gr1.grouper, df.A.values)
tm.assert_extension_array_equal(gr2.grouper, data_for_grouping)
@pytest.mark.parametrize(""as_index"", [True, False])
def test_groupby_extension_agg(self, as_index, data_for_grouping):
df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
result = df.groupby(""B"", as_index=as_index).A.mean()
_, index = pd.factorize(data_for_grouping, sort=True)
index = pd.Index(index, name=""B"")
expected = pd.Series([3, 1], index=index, name=""A"")
if as_index:
self.assert_series_equal(result, expected)
else:
expected = expected.reset_index()
self.assert_frame_equal(result, expected)
def test_groupby_extension_no_sort(self, data_for_grouping):
df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
result = df.groupby(""B"", sort=False).A.mean()
_, index = pd.factorize(data_for_grouping, sort=False)
index = pd.Index(index, name=""B"")
expected = pd.Series([1, 3], index=index, name=""A"")
self.assert_series_equal(result, expected)
def test_groupby_extension_transform(self, data_for_grouping):
valid = data_for_grouping[~data_for_grouping.isna()]
df = pd.DataFrame({""A"": [1, 1, 3, 3, 1], ""B"": valid})
result = df.groupby(""B"").A.transform(len)
expected = pd.Series([3, 3, 2, 2, 3], name=""A"")
self.assert_series_equal(result, expected)
def test_groupby_extension_apply(self, data_for_grouping, groupby_apply_op):
df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
df.groupby(""B"").apply(groupby_apply_op)
df.groupby(""B"").A.apply(groupby_apply_op)
df.groupby(""A"").apply(groupby_apply_op)
df.groupby(""A"").B.apply(groupby_apply_op)
def test_groupby_apply_identity(self, data_for_grouping):
df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
result = df.groupby(""A"").B.apply(lambda x: x.array)
expected = pd.Series(
[
df.B.iloc[[0, 1, 6]].array,
df.B.iloc[[2, 3]].array,
df.B.iloc[[4, 5]].array,
],
index=pd.Index([1, 2, 3], name=""A""),
name=""B"",
self.assert_series_equal(result, expected)
def test_in_numeric_groupby(self, data_for_grouping):
df = pd.DataFrame(
{
""A"": [1, 1, 2, 2, 3, 3, 1],
""B"": data_for_grouping,
""C"": [1, 1, 1, 1, 1, 1, 1],
}
result = df.groupby(""A"").sum().columns
if data_for_grouping.dtype._is_numeric:
expected = pd.Index([""B"", ""C""])
else:
expected = pd.Index([""C""])
tm.assert_index_equal(result, expected)
def check_reduce(self, s, op_name, skipna):
result = getattr(s, op_name)(skipna=skipna)
expected = getattr(s.astype(""float64""), op_name)(skipna=skipna)
# override parent function to cast to bool for min/max
if np.isnan(expected):
expected = pd.NA
elif op_name in (""min"", ""max""):
expected = bool(expected)
tm.assert_almost_equal(result, expected)
pass
pass
pass
BaseArithmeticOpsTests,
BaseComparisonOpsTests,
BaseOpsUtil,
BaseUnaryOpsTests,
def test_invert(self, data):
s = pd.Series(data, name=""name"")
result = ~s
expected = pd.Series(~data, name=""name"")
self.assert_series_equal(result, expected)
def test_invert_mixed(self):
shape = (10, 5)
df = pd.concat(
[
pd.DataFrame(np.zeros(shape, dtype=""bool"")),
pd.DataFrame(np.zeros(shape, dtype=int)),
],
axis=1,
ignore_index=True,
)
result = ~df
expected = pd.concat(
[
pd.DataFrame(np.ones(shape, dtype=""bool"")),
pd.DataFrame(-np.ones(shape, dtype=int)),
],
axis=1,
ignore_index=True,
)
tm.assert_frame_equal(result, expected)"
pandas,81,,
pandas,82,"return np.dtype(""M8[ns]""), tslibs.iNaT","return np.dtype(""M8[ns]""), np.datetime64(""NaT"", ""ns"")"
pandas,83,"objs, intersect: bool = False, axis=0, sort: bool = True
return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)
indexes: List[Index], intersect: bool = False, sort: bool = False
self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort","objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False
copy : bool, default False
If True, return a copy of the combined index.
return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)
indexes: List[Index],
intersect: bool = False,
sort: bool = False,
copy: bool = False,
copy : bool, default False
If True, return a copy of the combined index.
# GH 29879
if copy:
index = index.copy()
self.objs,
axis=data_axis,
intersect=self.intersect,
sort=self.sort,
copy=self.copy,"
pandas,84,"def test_unstack(self):
index = MultiIndex(
levels=[[""bar"", ""foo""], [""one"", ""three"", ""two""]],
codes=[[1, 1, 0, 0], [0, 1, 0, 2]],
)
s = Series(np.arange(4.0), index=index)
unstacked = s.unstack()
expected = DataFrame(
[[2.0, np.nan, 3.0], [0.0, 1.0, np.nan]],
index=[""bar"", ""foo""],
columns=[""one"", ""three"", ""two""],
)
tm.assert_frame_equal(unstacked, expected)
unstacked = s.unstack(level=0)
tm.assert_frame_equal(unstacked, expected.T)
index = MultiIndex(
levels=[[""bar""], [""one"", ""two"", ""three""], [0, 1]],
codes=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],
)
s = Series(np.random.randn(6), index=index)
exp_index = MultiIndex(
levels=[[""one"", ""two"", ""three""], [0, 1]],
codes=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],
)
expected = DataFrame({""bar"": s.values}, index=exp_index).sort_index(level=0)
unstacked = s.unstack(0).sort_index()
tm.assert_frame_equal(unstacked, expected)
# GH5873
idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])
ts = pd.Series([1, 2], index=idx)
left = ts.unstack()
right = DataFrame(
[[np.nan, 1], [2, np.nan]], index=[101, 102], columns=[np.nan, 3.5]
)
tm.assert_frame_equal(left, right)
idx = pd.MultiIndex.from_arrays(
[
[""cat"", ""cat"", ""cat"", ""dog"", ""dog""],
[""a"", ""a"", ""b"", ""a"", ""b""],
[1, 2, 1, 1, np.nan],
]
)
ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)
right = DataFrame(
[[1.0, 1.3], [1.1, np.nan], [np.nan, 1.4], [1.2, np.nan]],
columns=[""cat"", ""dog""],
)
tpls = [(""a"", 1), (""a"", 2), (""b"", np.nan), (""b"", 1)]
right.index = pd.MultiIndex.from_tuples(tpls)
tm.assert_frame_equal(ts.unstack(level=0), right)","# GH 19966 Make sure if MultiIndexed index has tuple name, they will be
# recognised as a whole
if clocs in index.names:
clocs = [clocs]"
pandas,85,,
pandas,86,,
pandas,87,"""__dummy__"",","original_df_cols = df.columns
[""__dummy__""],
# GH18321, after pivoting, an extra top level of column index of `__dummy__` is
# created, and this extra level should not be included in the further steps
if not table.empty:
cols_diff = df.columns.difference(original_df_cols)[0]
table = table[cols_diff]"
pandas,88,if table.index.nlevels > 1:,"# GH17038, this check should only happen if index is defined (not None)
if table.index.nlevels > 1 and index:"
pandas,89,result = result.unstack(val),"result = result.unstack(val, fill_value=fill_value)"
pandas,9,"if is_scalar(key) and isna(key):
if is_scalar(key) and isna(key):
hash(key)","if is_valid_nat_for_dtype(key, self.categories.dtype):
if is_valid_nat_for_dtype(key, self.categories.dtype):"
pandas,90,"obj : pandas object
path : str, default None
if path is None:
path = f""__{rands(10)}__.pickle""
with ensure_clean(path) as path:
pd.to_pickle(obj, path)
return pd.read_pickle(path)
path : str
File path where the pickled object will be stored.
A string representing the compression to use in the output file. By
default, infers from the file extension in specified path.
path = stringify_path(path)
f, fh = get_handle(path, ""wb"", compression=compression, is_text=False)
path : str
File path where the pickled object will be loaded.
For on-the-fly decompression of on-disk data. If 'infer', then use
gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',
or '.zip' respectively, and no decompression otherwise.
Set to None for no decompression.
path = stringify_path(path)
f, fh = get_handle(path, ""rb"", compression=compression, is_text=False)","obj: Any, path: Optional[FilePathOrBuffer] = None
obj : any object
path : str, path object or file-like object, default None
_path = path
if _path is None:
_path = f""__{rands(10)}__.pickle""
with ensure_clean(_path) as path:
pd.to_pickle(obj, _path)
return pd.read_pickle(_path)
obj: Any,
filepath_or_buffer: FilePathOrBuffer,
compression: Optional[str] = ""infer"",
protocol: int = pickle.HIGHEST_PROTOCOL,
filepath_or_buffer : str, path object or file-like object
File path, URL, or buffer where the pickled object will be stored.
.. versionchanged:: 1.0.0
Accept URL. URL has to be of S3 or GCS.
If 'infer' and 'path_or_url' is path-like, then detect compression from
the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no
compression) If 'infer' and 'path_or_url' is not path-like, then use
None (= no decompression).
fp_or_buf, _, compression, should_close = get_filepath_or_buffer(
filepath_or_buffer, compression=compression, mode=""wb""
)
if not isinstance(fp_or_buf, str) and compression == ""infer"":
compression = None
f, fh = get_handle(fp_or_buf, ""wb"", compression=compression, is_text=False)
if should_close:
try:
fp_or_buf.close()
except ValueError:
pass
filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = ""infer""
filepath_or_buffer : str, path object or file-like object
File path, URL, or buffer where the pickled object will be loaded from.
.. versionchanged:: 1.0.0
Accept URL. URL is not limited to S3 and GCS.
If 'infer' and 'path_or_url' is path-like, then detect compression from
the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no
compression) If 'infer' and 'path_or_url' is not path-like, then use
None (= no decompression).
fp_or_buf, _, compression, should_close = get_filepath_or_buffer(
filepath_or_buffer, compression=compression
)
if not isinstance(fp_or_buf, str) and compression == ""infer"":
compression = None
f, fh = get_handle(fp_or_buf, ""rb"", compression=compression, is_text=False)
if should_close:
try:
fp_or_buf.close()
except ValueError:
pass"
pandas,91,"value = np.array(value, dtype=_TD_DTYPE, copy=False)
else:
value = Timedelta(value).asm8.view(_TD_DTYPE)
return self.values.searchsorted(value, side=side, sorter=sorter)","if not type(self._data)._is_recognized_dtype(value):
raise TypeError(
""searchsorted requires compatible dtype or scalar, ""
f""not {type(value).__name__}""
)
value = type(self._data)(value)
self._data._check_compatible_with(value)
elif isinstance(value, self._data._recognized_scalars):
self._data._check_compatible_with(value)
value = self._data._scalar_type(value)
elif not isinstance(value, TimedeltaArray):
raise TypeError(
""searchsorted requires compatible dtype or scalar, ""
f""not {type(value).__name__}""
)
return self._data.searchsorted(value, side=side, sorter=sorter)"
pandas,92,"where = Period(where, freq=self.index.freq).ordinal
start = start.ordinal
if isinstance(value, Period):
if value.freq != self.freq:
raise raise_on_incompatible(self, value)
value = value.ordinal
value = Period(value, freq=self.freq).ordinal
return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)
self.searchsorted(t1.ordinal, side=""left""),
self.searchsorted(t2.ordinal, side=""right""),
bins = memb.searchsorted(rng, side=""left"")
result = ts[2007]
result = ts[2007]
msg = ""Input has different freq=H from PeriodIndex""
msg = ""Input has different freq=5D from PeriodIndex""","where = Period(where, freq=self.index.freq)
if isinstance(value, Period) or value is NaT:
self._data._check_compatible_with(value)
value = Period(value, freq=self.freq)
elif not isinstance(value, PeriodArray):
raise TypeError(
""PeriodIndex.searchsorted requires either a Period or PeriodArray""
)
return self._data.searchsorted(value, side=side, sorter=sorter)
self.searchsorted(t1, side=""left""), self.searchsorted(t2, side=""right"")
# Wrap in PeriodArray for PeriodArray.searchsorted
prng = type(memb._data)(rng, dtype=memb.dtype)
bins = memb.searchsorted(prng, side=""left"")
result = ts[""2007""]
result = ts[""2007""]
assert pidx.searchsorted(pd.NaT) == 0
msg = ""Input has different freq=H from PeriodArray""
msg = ""Input has different freq=5D from PeriodArray""
def test_searchsorted_invalid(self):
pidx = pd.PeriodIndex(
[""2014-01-01"", ""2014-01-02"", ""2014-01-03"", ""2014-01-04"", ""2014-01-05""],
freq=""D"",
)
other = np.array([0, 1], dtype=np.int64)
msg = ""requires either a Period or PeriodArray""
with pytest.raises(TypeError, match=msg):
pidx.searchsorted(other)
with pytest.raises(TypeError, match=msg):
pidx.searchsorted(other.astype(""timedelta64[ns]""))
with pytest.raises(TypeError, match=msg):
pidx.searchsorted(np.timedelta64(4))
with pytest.raises(TypeError, match=msg):
pidx.searchsorted(np.timedelta64(""NaT"", ""ms""))
with pytest.raises(TypeError, match=msg):
pidx.searchsorted(np.datetime64(4, ""ns""))
with pytest.raises(TypeError, match=msg):
pidx.searchsorted(np.datetime64(""NaT"", ""ns""))"
pandas,93,"is_datetime64_any_dtype,
is_datetime64_dtype,
is_datetime64tz_dtype,
is_datetime_or_timedelta_dtype,
is_float_dtype,
is_integer_dtype,
is_object_dtype,
is_string_dtype,
is_timedelta64_dtype,
is_unsigned_integer_dtype,
pandas_dtype,
Wrap comparison operations to convert Timestamp/Timedelta/Period-like to
boxed scalars/arrays.
opname = f""__{op.__name__}__""
nat_result = opname == ""__ne__""
@unpack_zerodim_and_defer(opname)
def wrapper(self, other):
if isinstance(other, str):
try:
# GH#18435 strings get a pass from tzawareness compat
other = self._scalar_from_string(other)
except ValueError:
# failed to parse as Timestamp/Timedelta/Period
return invalid_comparison(self, other, op)
if isinstance(other, self._recognized_scalars) or other is NaT:
other = self._scalar_type(other)
self._check_compatible_with(other)
other_i8 = self._unbox_scalar(other)
result = op(self.view(""i8""), other_i8)
if isna(other):
result.fill(nat_result)
elif not is_list_like(other):
return invalid_comparison(self, other, op)
elif len(other) != len(self):
raise ValueError(""Lengths must match"")
else:
if isinstance(other, list):
# TODO: could use pd.Index to do inference?
other = np.array(other)
if not isinstance(other, (np.ndarray, type(self))):
return invalid_comparison(self, other, op)
if is_object_dtype(other):
# We have to use comp_method_OBJECT_ARRAY instead of numpy
#  comparison otherwise it would fail to raise when
#  comparing tz-aware and tz-naive
with np.errstate(all=""ignore""):
result = ops.comp_method_OBJECT_ARRAY(
op, self.astype(object), other
)
o_mask = isna(other)
elif not type(self)._is_recognized_dtype(other.dtype):
return invalid_comparison(self, other, op)
else:
# For PeriodDType this casting is unnecessary
other = type(self)._from_sequence(other)
self._check_compatible_with(other)
result = op(self.view(""i8""), other.view(""i8""))
o_mask = other._isnan
if o_mask.any():
result[o_mask] = nat_result
if self._hasnans:
result[self._isnan] = nat_result
return result
return set_function_name(wrapper, opname, cls)
_data: np.ndarray
@classmethod
def _simple_new(cls, values, **kwargs):
raise AbstractMethodError(cls)
def _scalar_type(self) -> Type[DatetimeLikeScalar]:
""""""The scalar associated with this datelike
* PeriodArray : Period
* DatetimeArray : Timestamp
* TimedeltaArray : Timedelta
raise AbstractMethodError(self)
def _scalar_from_string(
self, value: str
) -> Union[Period, Timestamp, Timedelta, NaTType]:
Construct a scalar type from a string.
Parameters
----------
value : str
Returns
-------
Period, Timestamp, or Timedelta, or NaT
Whatever the type of ``self._scalar_type`` is.
Notes
-----
This should call ``self._check_compatible_with`` before
unboxing the result.
raise AbstractMethodError(self)
def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:
Unbox the integer value of a scalar `value`.
Parameters
----------
value : Union[Period, Timestamp, Timedelta]
Returns
-------
int
Examples
--------
>>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP
10000000000
raise AbstractMethodError(self)
def _check_compatible_with(
self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False
) -> None:
Verify that `self` and `other` are compatible.
* DatetimeArray verifies that the timezones (if any) match
* PeriodArray verifies that the freq matches
* Timedelta has no verification
In each case, NaT is considered compatible.
Parameters
----------
other
setitem : bool, default False
For __setitem__ we may have stricter compatiblity resrictions than
for comparisons.
Raises
------
Exception
""""""
raise AbstractMethodError(self)
""""""
Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.
""""""
@Substitution(
URL=""https://docs.python.org/3/library/datetime.html""
""#strftime-and-strptime-behavior""
)
def strftime(self, date_format):
Convert to Index using specified date_format.
Return an Index of formatted strings specified by date_format, which
supports the same string format as the python standard library. Details
of the string format can be found in `python string format
doc <%(URL)s>`__.
Parameters
----------
date_format : str
Date format string (e.g. ""%%Y-%%m-%%d"").
Returns
-------
ndarray
NumPy ndarray of formatted strings.
See Also
--------
to_datetime : Convert the given argument to datetime.
DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.
DatetimeIndex.round : Round the DatetimeIndex to the specified freq.
DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.
Examples
--------
>>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),
...                     periods=3, freq='s')
>>> rng.strftime('%%B %%d, %%Y, %%r')
Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',
'March 10, 2018, 09:00:02 AM'],
dtype='object')
result = self._format_native_types(date_format=date_format, na_rep=np.nan)
return result.astype(object)
""""""
Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.
""""""
_round_doc = """"""
Perform {op} operation on the data to the specified `freq`.
Parameters
----------
freq : str or Offset
The frequency level to {op} the index to. Must be a fixed
frequency like 'S' (second) not 'ME' (month end). See
:ref:`frequency aliases <timeseries.offset_aliases>` for
a list of possible `freq` values.
ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
Only relevant for DatetimeIndex:
- 'infer' will attempt to infer fall dst-transition hours based on
order
- bool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)
- 'NaT' will return NaT where there are ambiguous times
- 'raise' will raise an AmbiguousTimeError if there are ambiguous
times.
.. versionadded:: 0.24.0
nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \
A nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.
- 'shift_forward' will shift the nonexistent time forward to the
closest existing time
- 'shift_backward' will shift the nonexistent time backward to the
closest existing time
- 'NaT' will return NaT where there are nonexistent times
- timedelta objects will shift nonexistent times by the timedelta
- 'raise' will raise an NonExistentTimeError if there are
nonexistent times.
.. versionadded:: 0.24.0
Returns
-------
DatetimeIndex, TimedeltaIndex, or Series
Index of the same type for a DatetimeIndex or TimedeltaIndex,
or a Series with the same index for a Series.
Raises
------
ValueError if the `freq` cannot be converted.
Examples
**DatetimeIndex**
>>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')
>>> rng
DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',
'2018-01-01 12:01:00'],
dtype='datetime64[ns]', freq='T')
_round_example = """""">>> rng.round('H')
DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
'2018-01-01 12:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.round(""H"")
0   2018-01-01 12:00:00
1   2018-01-01 12:00:00
2   2018-01-01 12:00:00
dtype: datetime64[ns]
_floor_example = """""">>> rng.floor('H')
DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',
'2018-01-01 12:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.floor(""H"")
0   2018-01-01 11:00:00
1   2018-01-01 12:00:00
2   2018-01-01 12:00:00
dtype: datetime64[ns]
_ceil_example = """""">>> rng.ceil('H')
DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
'2018-01-01 13:00:00'],
dtype='datetime64[ns]', freq=None)
**Series**
>>> pd.Series(rng).dt.ceil(""H"")
0   2018-01-01 12:00:00
1   2018-01-01 12:00:00
2   2018-01-01 13:00:00
dtype: datetime64[ns]
def _round(self, freq, mode, ambiguous, nonexistent):
# round the local times
values = _ensure_datetimelike_to_i8(self)
result = round_nsint64(values, mode, freq)
result = self._maybe_mask_results(result, fill_value=NaT)
dtype = self.dtype
if is_datetime64tz_dtype(self):
dtype = None
return self._ensure_localized(
self._simple_new(result, dtype=dtype), ambiguous, nonexistent
)
@Appender((_round_doc + _round_example).format(op=""round""))
def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
@Appender((_round_doc + _floor_example).format(op=""floor""))
def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
@Appender((_round_doc + _ceil_example).format(op=""ceil""))
def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
""""""
Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
Assumes that __new__/__init__ defines:
_data
_freq
and that the inheriting class has methods:
_generate_range
""""""
@property
def ndim(self) -> int:
return self._data.ndim
@property
def shape(self):
return self._data.shape
def reshape(self, *args, **kwargs):
# Note: we drop any freq
data = self._data.reshape(*args, **kwargs)
return type(self)(data, dtype=self.dtype)
def ravel(self, *args, **kwargs):
# Note: we drop any freq
data = self._data.ravel(*args, **kwargs)
return type(self)(data, dtype=self.dtype)
def _box_func(self):
""""""
box function to get object from internal representation
""""""
def _box_values(self, values):
apply box func to passed values
return lib.map_infer(values, self._box_func)
def __iter__(self):
return (self._box_func(v) for v in self.asi8)
@property
def asi8(self) -> np.ndarray:
Integer representation of the values.
Returns
-------
ndarray
An ndarray with int64 dtype.
""""""
# do not cache or you'll create a memory leak
return self._data.view(""i8"")
@property
def _ndarray_values(self):
return self._data
# ----------------------------------------------------------------
# Rendering Methods
def _format_native_types(self, na_rep=""NaT"", date_format=None):
""""""
Helper method for astype when converting to strings.
ndarray[str]
raise AbstractMethodError(self)
def _formatter(self, boxed=False):
# TODO: Remove Datetime & DatetimeTZ formatters.
return ""'{}'"".format
# ----------------------------------------------------------------
# Array-Like / EA-Interface Methods
@property
def nbytes(self):
return self._data.nbytes
def __array__(self, dtype=None):
# used for Timedelta/DatetimeArray, overwritten by PeriodArray
if is_object_dtype(dtype):
return np.array(list(self), dtype=object)
return self._data
@property
def size(self) -> int:
""""""The number of elements in this array.""""""
return np.prod(self.shape)
def __len__(self) -> int:
return len(self._data)
def __getitem__(self, key):
""""""
This getitem defers to the underlying array, which by-definition can
only handle list-likes, slices, and integer scalars
""""""
is_int = lib.is_integer(key)
if lib.is_scalar(key) and not is_int:
raise IndexError(
""only integers, slices (`:`), ellipsis (`...`), ""
""numpy.newaxis (`None`) and integer or boolean ""
""arrays are valid indices""
)
getitem = self._data.__getitem__
if is_int:
val = getitem(key)
if lib.is_scalar(val):
# i.e. self.ndim == 1
return self._box_func(val)
return type(self)(val, dtype=self.dtype)
if com.is_bool_indexer(key):
key = check_bool_array_indexer(self, key)
if key.all():
key = slice(0, None, None)
else:
key = lib.maybe_booleans_to_slice(key.view(np.uint8))
is_period = is_period_dtype(self)
if is_period:
freq = self.freq
else:
freq = None
if isinstance(key, slice):
if self.freq is not None and key.step is not None:
freq = key.step * self.freq
else:
freq = self.freq
elif key is Ellipsis:
# GH#21282 indexing with Ellipsis is similar to a full slice,
#  should preserve `freq` attribute
freq = self.freq
result = getitem(key)
if result.ndim > 1:
# To support MPL which performs slicing with 2 dim
# even though it only has 1 dim by definition
if is_period:
return self._simple_new(result, dtype=self.dtype, freq=freq)
return result
return self._simple_new(result, dtype=self.dtype, freq=freq)
def __setitem__(
self,
key: Union[int, Sequence[int], Sequence[bool], slice],
value: Union[NaTType, Any, Sequence[Any]],
) -> None:
# I'm fudging the types a bit here. ""Any"" above really depends
# on type(self). For PeriodArray, it's Period (or stuff coercible
# to a period in from_sequence). For DatetimeArray, it's Timestamp...
# I don't know if mypy can do that, possibly with Generics.
# https://mypy.readthedocs.io/en/latest/generics.html
if lib.is_scalar(value) and not isna(value):
value = com.maybe_box_datetimelike(value)
if is_list_like(value):
is_slice = isinstance(key, slice)
if lib.is_scalar(key):
raise ValueError(""setting an array element with a sequence."")
if not is_slice:
key = cast(Sequence, key)
if len(key) != len(value) and not com.is_bool_indexer(key):
msg = (
f""shape mismatch: value array of length '{len(key)}' ""
""does not match indexing result of length ""
f""'{len(value)}'.""
)
raise ValueError(msg)
elif not len(key):
return
value = type(self)._from_sequence(value, dtype=self.dtype)
self._check_compatible_with(value, setitem=True)
value = value.asi8
elif isinstance(value, self._scalar_type):
self._check_compatible_with(value, setitem=True)
value = self._unbox_scalar(value)
elif is_valid_nat_for_dtype(value, self.dtype):
value = iNaT
msg = (
f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
f""or array of those. Got '{type(value).__name__}' instead.""
)
raise TypeError(msg)
self._data[key] = value
self._maybe_clear_freq()
def _maybe_clear_freq(self):
# inplace operations like __setitem__ may invalidate the freq of
# DatetimeArray and TimedeltaArray
pass
def astype(self, dtype, copy=True):
# Some notes on cases we don't have to handle here in the base class:
#   1. PeriodArray.astype handles period -> period
#   2. DatetimeArray.astype handles conversion between tz.
#   3. DatetimeArray.astype handles datetime -> period
from pandas import Categorical
dtype = pandas_dtype(dtype)
if is_object_dtype(dtype):
return self._box_values(self.asi8)
elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):
return self._format_native_types()
elif is_integer_dtype(dtype):
# we deliberately ignore int32 vs. int64 here.
# See https://github.com/pandas-dev/pandas/issues/24381 for more.
values = self.asi8
if is_unsigned_integer_dtype(dtype):
# Again, we ignore int32 vs. int64
values = values.view(""uint64"")
if copy:
values = values.copy()
return values
elif (
is_datetime_or_timedelta_dtype(dtype)
and not is_dtype_equal(self.dtype, dtype)
) or is_float_dtype(dtype):
# disallow conversion between datetime/timedelta,
# and conversions for any datetimelike to float
msg = f""Cannot cast {type(self).__name__} to dtype {dtype}""
raise TypeError(msg)
elif is_categorical_dtype(dtype):
return Categorical(self, dtype=dtype)
else:
return np.asarray(self, dtype=dtype)
def view(self, dtype=None):
if dtype is None or dtype is self.dtype:
return type(self)(self._data, dtype=self.dtype)
return self._data.view(dtype=dtype)
# ------------------------------------------------------------------
# ExtensionArray Interface
def unique(self):
result = unique1d(self.asi8)
return type(self)(result, dtype=self.dtype)
def _validate_fill_value(self, fill_value):
If a fill_value is passed to `take` convert it to an i8 representation,
raising ValueError if this is not possible.
fill_value : object
fill_value : np.int64
Raises
------
ValueError
if isna(fill_value):
fill_value = iNaT
elif isinstance(fill_value, self._recognized_scalars):
self._check_compatible_with(fill_value)
fill_value = self._scalar_type(fill_value)
fill_value = self._unbox_scalar(fill_value)
raise ValueError(
f""'fill_value' should be a {self._scalar_type}. Got '{fill_value}'.""
)
return fill_value
def take(self, indices, allow_fill=False, fill_value=None):
if allow_fill:
fill_value = self._validate_fill_value(fill_value)
new_values = take(
self.asi8, indices, allow_fill=allow_fill, fill_value=fill_value
)
return type(self)(new_values, dtype=self.dtype)
@classmethod
def _concat_same_type(cls, to_concat):
dtypes = {x.dtype for x in to_concat}
assert len(dtypes) == 1
dtype = list(dtypes)[0]
values = np.concatenate([x.asi8 for x in to_concat])
return cls(values, dtype=dtype)
def copy(self):
values = self.asi8.copy()
return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)
def _values_for_factorize(self):
return self.asi8, iNaT
@classmethod
def _from_factorized(cls, values, original):
return cls(values, dtype=original.dtype)
def _values_for_argsort(self):
return self._data
# ------------------------------------------------------------------
# Additional array methods
#  These are not part of the EA API, but we implement them because
#  pandas assumes they're there.
def searchsorted(self, value, side=""left"", sorter=None):
Find indices where elements should be inserted to maintain order.
Find the indices into a sorted array `self` such that, if the
corresponding elements in `value` were inserted before the indices,
the order of `self` would be preserved.
value : array_like
Values to insert into `self`.
side : {'left', 'right'}, optional
If 'left', the index of the first suitable location found is given.
If 'right', return the last such index.  If there is no suitable
index, return either 0 or N (where N is the length of `self`).
sorter : 1-D array_like, optional
Optional array of integer indices that sort `self` into ascending
order. They are typically the result of ``np.argsort``.
Returns
-------
indices : array of ints
Array of insertion points with the same shape as `value`.
""""""
if isinstance(value, str):
value = self._scalar_from_string(value)
if not (isinstance(value, (self._scalar_type, type(self))) or isna(value)):
raise ValueError(f""Unexpected type for 'value': {type(value)}"")
self._check_compatible_with(value)
if isinstance(value, type(self)):
value = value.asi8
else:
value = self._unbox_scalar(value)
return self.asi8.searchsorted(value, side=side, sorter=sorter)
def repeat(self, repeats, *args, **kwargs):
""""""
Repeat elements of an array.
numpy.ndarray.repeat
""""""
nv.validate_repeat(args, kwargs)
values = self._data.repeat(repeats)
return type(self)(values.view(""i8""), dtype=self.dtype)
def value_counts(self, dropna=False):
Return a Series containing counts of unique values.
Parameters
----------
dropna : bool, default True
Don't include counts of NaT values.
Returns
-------
Series
""""""
from pandas import Series, Index
if dropna:
values = self[~self.isna()]._data
values = self._data
cls = type(self)
result = value_counts(values, sort=False, dropna=dropna)
index = Index(
cls(result.index.view(""i8""), dtype=self.dtype), name=result.index.name
)
return Series(result.values, index=index, name=result.name)
def map(self, mapper):
# TODO(GH-23179): Add ExtensionArray.map
# Need to figure out if we want ExtensionArray.map first.
# If so, then we can refactor IndexOpsMixin._map_values to
# a standalone function and call from here..
# Else, just rewrite _map_infer_values to do the right thing.
from pandas import Index
return Index(self).map(mapper).array
# ------------------------------------------------------------------
# Null Handling
def isna(self):
return self._isnan
@property  # NB: override with cache_readonly in immutable subclasses
def _isnan(self):
""""""
return if each value is nan
return self.asi8 == iNaT
@property  # NB: override with cache_readonly in immutable subclasses
def _hasnans(self):
""""""
return if I have any nans; enables various perf speedups
""""""
return bool(self._isnan.any())
def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):
""""""
result : a ndarray
fill_value : object, default iNaT
convert : str, dtype or None
Returns
-------
result : ndarray with values replace by the fill_value
mask the result if needed, convert to the provided dtype if its not
None
This is an internal routine.
""""""
if self._hasnans:
if convert:
result = result.astype(convert)
if fill_value is None:
fill_value = np.nan
result[self._isnan] = fill_value
return result
def fillna(self, value=None, method=None, limit=None):
# TODO(GH-20300): remove this
# Just overriding to ensure that we avoid an astype(object).
# Either 20300 or a `_values_for_fillna` would avoid this duplication.
if isinstance(value, ABCSeries):
value = value.array
value, method = validate_fillna_kwargs(value, method)
mask = self.isna()
if is_array_like(value):
if len(value) != len(self):
raise ValueError(
f""Length of 'value' does not match. Got ({len(value)}) ""
f"" expected {len(self)}""
)
value = value[mask]
if mask.any():
if method is not None:
if method == ""pad"":
func = missing.pad_1d
else:
func = missing.backfill_1d
values = self._data
if not is_period_dtype(self):
# For PeriodArray self._data is i8, which gets copied
#  by `func`.  Otherwise we need to make a copy manually
# to avoid modifying `self` in-place.
values = values.copy()
new_values = func(values, limit=limit, mask=mask)
if is_datetime64tz_dtype(self):
# we need to pass int64 values to the constructor to avoid
#  re-localizing incorrectly
new_values = new_values.view(""i8"")
new_values = type(self)(new_values, dtype=self.dtype)
else:
# fill with value
new_values = self.copy()
new_values[mask] = value
new_values = self.copy()
return new_values
# ------------------------------------------------------------------
# Frequency Properties/Methods
@property
def freq(self):
""""""
Return the frequency object if it is set, otherwise None.
""""""
return self._freq
@freq.setter
def freq(self, value):
if value is not None:
value = frequencies.to_offset(value)
self._validate_frequency(self, value)
self._freq = value
@property
def freqstr(self):
""""""
Return the frequency object as a string if its set, otherwise None
""""""
if self.freq is None:
return None
return self.freq.freqstr
@property  # NB: override with cache_readonly in immutable subclasses
def inferred_freq(self):
""""""
Tryies to return a string representing a frequency guess,
generated by infer_freq.  Returns None if it can't autodetect the
frequency.
""""""
if self.ndim != 1:
return None
try:
return frequencies.infer_freq(self)
except ValueError:
return None
@property  # NB: override with cache_readonly in immutable subclasses
def _resolution(self):
return frequencies.Resolution.get_reso_from_freq(self.freqstr)
@property  # NB: override with cache_readonly in immutable subclasses
def resolution(self):
""""""
Returns day, hour, minute, second, millisecond or microsecond
return frequencies.Resolution.get_str(self._resolution)
@classmethod
def _validate_frequency(cls, index, freq, **kwargs):
""""""
Validate that a frequency is compatible with the values of a given
Datetime Array/Index or Timedelta Array/Index
index : DatetimeIndex or TimedeltaIndex
The index on which to determine if the given frequency is valid
freq : DateOffset
The frequency to validate
""""""
if is_period_dtype(cls):
# Frequency validation is not meaningful for Period Array/Index
return None
inferred = index.inferred_freq
if index.size == 0 or inferred == freq.freqstr:
return None
try:
on_freq = cls._generate_range(
start=index[0], end=None, periods=len(index), freq=freq, **kwargs
)
if not np.array_equal(index.asi8, on_freq.asi8):
raise ValueError
except ValueError as e:
if ""non-fixed"" in str(e):
# non-fixed frequencies are not meaningful for timedelta64;
#  we retain that error message
raise e
# GH#11587 the main way this is reached is if the `np.array_equal`
#  check above is False.  This can also be reached if index[0]
#  is `NaT`, in which case the call to `cls._generate_range` will
#  raise a ValueError, which we re-raise with a more targeted
#  message.
raise ValueError(
f""Inferred frequency {inferred} from passed values ""
f""does not conform to passed frequency {freq.freqstr}""
)
# monotonicity/uniqueness properties are called via frequencies.infer_freq,
#  see GH#23789
@property
def _is_monotonic_increasing(self):
return algos.is_monotonic(self.asi8, timelike=True)[0]
@property
def _is_monotonic_decreasing(self):
return algos.is_monotonic(self.asi8, timelike=True)[1]
@property
def _is_unique(self):
return len(unique1d(self.asi8)) == len(self)
# ------------------------------------------------------------------
# Arithmetic Methods
_create_comparison_method = classmethod(_datetimelike_array_cmp)
# pow is invalid for all three subclasses; TimedeltaArray will override
#  the multiplication and division ops
__pow__ = make_invalid_op(""__pow__"")
__rpow__ = make_invalid_op(""__rpow__"")
__mul__ = make_invalid_op(""__mul__"")
__rmul__ = make_invalid_op(""__rmul__"")
__truediv__ = make_invalid_op(""__truediv__"")
__rtruediv__ = make_invalid_op(""__rtruediv__"")
__floordiv__ = make_invalid_op(""__floordiv__"")
__rfloordiv__ = make_invalid_op(""__rfloordiv__"")
__mod__ = make_invalid_op(""__mod__"")
__rmod__ = make_invalid_op(""__rmod__"")
__divmod__ = make_invalid_op(""__divmod__"")
__rdivmod__ = make_invalid_op(""__rdivmod__"")
def _add_datetimelike_scalar(self, other):
# Overridden by TimedeltaArray
raise TypeError(f""cannot add {type(self).__name__} and {type(other).__name__}"")
_add_datetime_arraylike = _add_datetimelike_scalar
def _sub_datetimelike_scalar(self, other):
# Overridden by DatetimeArray
assert other is not NaT
raise TypeError(f""cannot subtract a datelike from a {type(self).__name__}"")
_sub_datetime_arraylike = _sub_datetimelike_scalar
def _sub_period(self, other):
# Overridden by PeriodArray
raise TypeError(f""cannot subtract Period from a {type(self).__name__}"")
def _add_offset(self, offset):
raise AbstractMethodError(self)
def _add_delta(self, other):
""""""
Add a timedelta-like, Tick or TimedeltaIndex-like object
to self, yielding an int64 numpy array
Parameters
----------
delta : {timedelta, np.timedelta64, Tick,
TimedeltaIndex, ndarray[timedelta64]}
result : ndarray[int64]
Notes
-----
The result's name is set outside of _add_delta by the calling
method (__add__ or __sub__), if necessary (i.e. for Indexes).
if isinstance(other, (Tick, timedelta, np.timedelta64)):
new_values = self._add_timedeltalike_scalar(other)
elif is_timedelta64_dtype(other):
# ndarray[timedelta64] or TimedeltaArray/index
new_values = self._add_delta_tdi(other)
return new_values
def _add_timedeltalike_scalar(self, other):
""""""
Add a delta of a timedeltalike
return the i8 result view
""""""
if isna(other):
# i.e np.timedelta64(""NaT""), not recognized by delta_to_nanoseconds
new_values = np.empty(self.shape, dtype=""i8"")
new_values[:] = iNaT
return new_values
inc = delta_to_nanoseconds(other)
new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(
""i8""
)
new_values = self._maybe_mask_results(new_values)
return new_values.view(""i8"")
def _add_delta_tdi(self, other):
""""""
Add a delta of a TimedeltaIndex
return the i8 result view
""""""
if len(self) != len(other):
raise ValueError(""cannot add indices of unequal length"")
if isinstance(other, np.ndarray):
# ndarray[timedelta64]; wrap in TimedeltaIndex for op
from pandas.core.arrays import TimedeltaArray
other = TimedeltaArray._from_sequence(other)
self_i8 = self.asi8
other_i8 = other.asi8
new_values = checked_add_with_arr(
self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan
)
if self._hasnans or other._hasnans:
mask = (self._isnan) | (other._isnan)
new_values[mask] = iNaT
return new_values.view(""i8"")
def _add_nat(self):
""""""
Add pd.NaT to self
""""""
if is_period_dtype(self):
raise TypeError(
f""Cannot add {type(self).__name__} and {type(NaT).__name__}""
# GH#19124 pd.NaT is treated like a timedelta for both timedelta
# and datetime dtypes
result = np.zeros(self.shape, dtype=np.int64)
result.fill(iNaT)
return type(self)(result, dtype=self.dtype, freq=None)
def _sub_nat(self):
""""""
Subtract pd.NaT from self
""""""
# GH#19124 Timedelta - datetime is not in general well-defined.
# We make an exception for pd.NaT, which in this case quacks
# like a timedelta.
# For datetime64 dtypes by convention we treat NaT as a datetime, so
# this subtraction returns a timedelta64 dtype.
# For period dtype, timedelta64 is a close-enough return dtype.
result = np.zeros(self.shape, dtype=np.int64)
result.fill(iNaT)
return result.view(""timedelta64[ns]"")
def _sub_period_array(self, other):
""""""
Subtract a Period Array/Index from self.  This is only valid if self
is itself a Period Array/Index, raises otherwise.  Both objects must
have the same frequency.
Parameters
----------
other : PeriodIndex or PeriodArray
Returns
-------
result : np.ndarray[object]
Array of DateOffset objects; nulls represented by NaT.
""""""
if not is_period_dtype(self):
raise TypeError(
f""cannot subtract {other.dtype}-dtype from {type(self).__name__}""
)
if self.freq != other.freq:
msg = DIFFERENT_FREQ.format(
cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr
)
raise IncompatibleFrequency(msg)
new_values = checked_add_with_arr(
self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan
)
new_values = np.array([self.freq.base * x for x in new_values])
if self._hasnans or other._hasnans:
mask = (self._isnan) | (other._isnan)
new_values[mask] = NaT
return new_values
def _addsub_object_array(self, other: np.ndarray, op):
""""""
Add or subtract array-like of DateOffset objects
Parameters
----------
other : np.ndarray[object]
op : {operator.add, operator.sub}
Returns
-------
result : same class as self
""""""
assert op in [operator.add, operator.sub]
if len(other) == 1:
return op(self, other[0])
warnings.warn(
""Adding/subtracting array of DateOffsets to ""
f""{type(self).__name__} not vectorized"",
PerformanceWarning,
)
# For EA self.astype('O') returns a numpy array, not an Index
left = self.astype(""O"")
res_values = op(left, np.array(other))
kwargs = {}
if not is_period_dtype(self):
kwargs[""freq""] = ""infer""
res = type(self)._from_sequence(res_values, **kwargs)
# e.g. we've passed a Timestamp to TimedeltaArray
res = res_values
return res
def _time_shift(self, periods, freq=None):
""""""
Shift each value by `periods`.
Note this is different from ExtensionArray.shift, which
shifts the *position* of each element, padding the end with
missing values.
Parameters
----------
periods : int
Number of periods to shift by.
freq : pandas.DateOffset, pandas.Timedelta, or str
Frequency increment to shift by.
""""""
if freq is not None and freq != self.freq:
if isinstance(freq, str):
freq = frequencies.to_offset(freq)
offset = periods * freq
result = self + offset
return result
if periods == 0:
# immutable so OK
return self.copy()
if self.freq is None:
raise NullFrequencyError(""Cannot shift with no freq"")
start = self[0] + periods * self.freq
end = self[-1] + periods * self.freq
# Note: in the DatetimeTZ case, _generate_range will infer the
#  appropriate timezone from `start` and `end`, so tz does not need
#  to be passed explicitly.
return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
@unpack_zerodim_and_defer(""__add__"")
def __add__(self, other):
# scalar others
if other is NaT:
result = self._add_nat()
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
result = self._add_delta(other)
elif isinstance(other, DateOffset):
# specifically _not_ a Tick
result = self._add_offset(other)
elif isinstance(other, (datetime, np.datetime64)):
result = self._add_datetimelike_scalar(other)
elif lib.is_integer(other):
# This check must come after the check for np.timedelta64
# as is_integer returns True for these
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._time_shift(other)
# array-like others
elif is_timedelta64_dtype(other):
# TimedeltaIndex, ndarray[timedelta64]
result = self._add_delta(other)
elif is_object_dtype(other):
# e.g. Array/Index of DateOffset objects
result = self._addsub_object_array(other, operator.add)
elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
# DatetimeIndex, ndarray[datetime64]
return self._add_datetime_arraylike(other)
elif is_integer_dtype(other):
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._addsub_int_array(other, operator.add)
# Includes Categorical, other ExtensionArrays
# For PeriodDtype, if self is a TimedeltaArray and other is a
#  PeriodArray with  a timedelta-like (i.e. Tick) freq, this
#  operation is valid.  Defer to the PeriodArray implementation.
#  In remaining cases, this will end up raising TypeError.
return NotImplemented
if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
from pandas.core.arrays import TimedeltaArray
return TimedeltaArray(result)
return result
def __radd__(self, other):
# alias for __add__
return self.__add__(other)
@unpack_zerodim_and_defer(""__sub__"")
def __sub__(self, other):
# scalar others
if other is NaT:
result = self._sub_nat()
elif isinstance(other, (Tick, timedelta, np.timedelta64)):
result = self._add_delta(-other)
elif isinstance(other, DateOffset):
# specifically _not_ a Tick
result = self._add_offset(-other)
elif isinstance(other, (datetime, np.datetime64)):
result = self._sub_datetimelike_scalar(other)
elif lib.is_integer(other):
# This check must come after the check for np.timedelta64
# as is_integer returns True for these
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._time_shift(-other)
elif isinstance(other, Period):
result = self._sub_period(other)
# array-like others
elif is_timedelta64_dtype(other):
# TimedeltaIndex, ndarray[timedelta64]
result = self._add_delta(-other)
elif is_object_dtype(other):
# e.g. Array/Index of DateOffset objects
result = self._addsub_object_array(other, operator.sub)
elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
# DatetimeIndex, ndarray[datetime64]
result = self._sub_datetime_arraylike(other)
elif is_period_dtype(other):
# PeriodIndex
result = self._sub_period_array(other)
elif is_integer_dtype(other):
if not is_period_dtype(self):
raise integer_op_not_supported(self)
result = self._addsub_int_array(other, operator.sub)
# Includes ExtensionArrays, float_dtype
return NotImplemented
if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
from pandas.core.arrays import TimedeltaArray
return TimedeltaArray(result)
return result
def __rsub__(self, other):
if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
# ndarray[datetime64] cannot be subtracted from self, so
# we need to wrap in DatetimeArray/Index and flip the operation
if lib.is_scalar(other):
# i.e. np.datetime64 object
return Timestamp(other) - self
if not isinstance(other, DatetimeLikeArrayMixin):
# Avoid down-casting DatetimeIndex
from pandas.core.arrays import DatetimeArray
other = DatetimeArray(other)
return other - self
elif (
is_datetime64_any_dtype(self.dtype)
and hasattr(other, ""dtype"")
and not is_datetime64_any_dtype(other.dtype)
):
# GH#19959 datetime - datetime is well-defined as timedelta,
# but any other type - datetime is not well-defined.
raise TypeError(
f""cannot subtract {type(self).__name__} from {type(other).__name__}""
)
elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):
# TODO: Can we simplify/generalize these cases at all?
raise TypeError(f""cannot subtract {type(self).__name__} from {other.dtype}"")
elif is_timedelta64_dtype(self.dtype):
if lib.is_integer(other) or is_integer_dtype(other):
# need to subtract before negating, since that flips freq
# -self flips self.freq, messing up results
return -(self - other)
return (-self) + other
return -(self - other)
def __iadd__(self, other):  # type: ignore
result = self + other
self[:] = result[:]
if not is_period_dtype(self):
# restore freq, which is invalidated by setitem
self._freq = result._freq
return self
def __isub__(self, other):  # type: ignore
result = self - other
self[:] = result[:]
if not is_period_dtype(self):
# restore freq, which is invalidated by setitem
self._freq = result._freq
return self
# --------------------------------------------------------------
# Comparison Methods
def _ensure_localized(
self, arg, ambiguous=""raise"", nonexistent=""raise"", from_utc=False
):
""""""
Ensure that we are re-localized.
This is for compat as we can then call this on all datetimelike
arrays generally (ignored for Period/Timedelta)
Parameters
----------
arg : Union[DatetimeLikeArray, DatetimeIndexOpsMixin, ndarray]
ambiguous : str, bool, or bool-ndarray, default 'raise'
nonexistent : str, default 'raise'
from_utc : bool, default False
If True, localize the i8 ndarray to UTC first before converting to
the appropriate tz. If False, localize directly to the tz.
Returns
-------
localized array
""""""
# reconvert to local tz
tz = getattr(self, ""tz"", None)
if tz is not None:
if not isinstance(arg, type(self)):
arg = self._simple_new(arg)
if from_utc:
arg = arg.tz_localize(""UTC"").tz_convert(self.tz)
else:
arg = arg.tz_localize(
self.tz, ambiguous=ambiguous, nonexistent=nonexistent
)
return arg
# --------------------------------------------------------------
# Reductions
def _reduce(self, name, axis=0, skipna=True, **kwargs):
op = getattr(self, name, None)
if op:
return op(skipna=skipna, **kwargs)
return super()._reduce(name, skipna, **kwargs)
def min(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the minimum value of the Array or minimum along
an axis.
See Also
--------
numpy.ndarray.min
Index.min : Return the minimum value in an Index.
Series.min : Return the minimum value in a Series.
""""""
nv.validate_min(args, kwargs)
nv.validate_minmax_axis(axis)
result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())
if isna(result):
# Period._from_ordinal does not handle np.nan gracefully
return NaT
return self._box_func(result)
def max(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the maximum value of the Array or maximum along
an axis.
See Also
--------
numpy.ndarray.max
Index.max : Return the maximum value in an Index.
Series.max : Return the maximum value in a Series.
# TODO: skipna is broken with max.
# See https://github.com/pandas-dev/pandas/issues/24265
nv.validate_max(args, kwargs)
nv.validate_minmax_axis(axis)
mask = self.isna()
if skipna:
values = self[~mask].asi8
elif mask.any():
return NaT
else:
values = self.asi8
if not len(values):
# short-circuit for empty max / min
return NaT
result = nanops.nanmax(values, skipna=skipna)
# Don't have to worry about NA `result`, since no NA went in.
return self._box_func(result)
def mean(self, skipna=True):
Return the mean value of the Array.
.. versionadded:: 0.25.0
Parameters
----------
skipna : bool, default True
Whether to ignore any NaT elements.
Returns
-------
scalar
Timestamp or Timedelta.
See Also
--------
numpy.ndarray.mean : Returns the average of array elements along a given axis.
Series.mean : Return the mean value in a Series.
Notes
-----
mean is only defined for Datetime and Timedelta dtypes, not for Period.
""""""
if is_period_dtype(self):
# See discussion in GH#24757
raise TypeError(
f""mean is not implemented for {type(self).__name__} since the ""
""meaning is ambiguous.  An alternative is ""
""obj.to_timestamp(how='start').mean()""
)
mask = self.isna()
if skipna:
values = self[~mask]
elif mask.any():
return NaT
values = self
if not len(values):
# short-circuit for empty max / min
return NaT
result = nanops.nanmean(values.view(""i8""), skipna=skipna)
# Don't have to worry about NA `result`, since no NA went in.
return self._box_func(result)
If a `periods` argument is passed to the Datetime/Timedelta Array/Index
constructor, cast it to an integer.
Parameters
----------
periods : None, float, int
Returns
-------
periods : None or int
Raises
------
TypeError
if periods is None, float, or int
if periods is not None:
if lib.is_float(periods):
periods = int(periods)
elif not lib.is_integer(periods):
raise TypeError(f""periods must be a number, got {periods}"")
return periods
""""""
Check that the `closed` argument is among [None, ""left"", ""right""]
Parameters
----------
closed : {None, ""left"", ""right""}
Returns
-------
left_closed : bool
right_closed : bool
Raises
------
ValueError : if argument is not among valid values
""""""
left_closed = False
right_closed = False
if closed is None:
left_closed = True
right_closed = True
elif closed == ""left"":
left_closed = True
elif closed == ""right"":
right_closed = True
else:
raise ValueError(""Closed has to be either 'left', 'right' or None"")
return left_closed, right_closed
""""""
If the user passes a freq and another freq is inferred from passed data,
require that they match.
Parameters
----------
freq : DateOffset or None
inferred_freq : DateOffset or None
freq_infer : bool
Returns
-------
freq : DateOffset or None
freq_infer : bool
Notes
-----
We assume at this point that `maybe_infer_freq` has been called, so
`freq` is either a DateOffset object or None.
""""""
if inferred_freq is not None:
if freq is not None and freq != inferred_freq:
raise ValueError(
f""Inferred frequency {inferred_freq} from passed ""
""values does not conform to passed frequency ""
f""{freq.freqstr}""
)
elif freq is None:
freq = inferred_freq
freq_infer = False
return freq, freq_infer
""""""
Comparing a DateOffset to the string ""infer"" raises, so we need to
be careful about comparisons.  Make a dummy variable `freq_infer` to
signify the case where the given freq is ""infer"" and set freq to None
to avoid comparison trouble later on.
Parameters
----------
freq : {DateOffset, None, str}
Returns
-------
freq : {DateOffset, None}
freq_infer : bool
""""""
freq_infer = False
if not isinstance(freq, DateOffset):
# if a passed freq is None, don't infer automatically
if freq != ""infer"":
freq = frequencies.to_offset(freq)
else:
freq_infer = True
freq = None
return freq, freq_infer
""""""
Helper for coercing an input scalar or array to i8.
Parameters
----------
other : 1d array
to_utc : bool, default False
If True, convert the values to UTC before extracting the i8 values
If False, extract the i8 values directly.
Returns
-------
i8 1d array
""""""
from pandas import Index
if lib.is_scalar(other) and isna(other):
return iNaT
elif isinstance(other, (ABCPeriodArray, ABCIndexClass, DatetimeLikeArrayMixin)):
# convert tz if needed
if getattr(other, ""tz"", None) is not None:
if to_utc:
other = other.tz_convert(""UTC"")
else:
other = other.tz_localize(None)
else:
try:
return np.array(other, copy=False).view(""i8"")
except TypeError:
# period array cannot be coerced to int
other = Index(other)
return other.asi8","ensure_int64,
is_bool_dtype,
is_float,
is_integer,
is_scalar,
needs_i8_conversion,
DatetimeArray,
ExtensionArray,
ExtensionOpsMixin,
TimedeltaArray,
ExtensionIndex,
inherit_names,
make_wrapped_arith_op,
make_wrapped_comparison_op,
Create the join wrapper methods.
@staticmethod  # type: ignore
def wrapper(left, right):
if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
left = left.view(""i8"")
if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
right = right.view(""i8"")
results = joinf(left, right)
if with_indexers:
# dtype should be timedelta64[ns] for TimedeltaIndex
#  and datetime64[ns] for DatetimeIndex
dtype = left.dtype.base
join_index, left_indexer, right_indexer = results
join_index = join_index.view(dtype)
return join_index, left_indexer, right_indexer
return results
return wrapper
[""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
DatetimeLikeArrayMixin,
cache=True,
[""__iter__"", ""mean"", ""freq"", ""freqstr"", ""_ndarray_values"", ""asi8"", ""_box_values""],
DatetimeLikeArrayMixin,
""""""
Common ops mixin to support a unified interface datetimelike Index.
""""""
_data: ExtensionArray
freq: Optional[DateOffset]
freqstr: Optional[str]
_resolution: int
_bool_ops: List[str] = []
_field_ops: List[str] = []
hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
_hasnans = hasnans  # for index / array -agnostic code
def is_all_dates(self) -> bool:
return True
@classmethod
def _create_comparison_method(cls, op):
Create a comparison method that dispatches to ``cls.values``.
return make_wrapped_comparison_op(f""__{op.__name__}__"")
# ------------------------------------------------------------------------
# Abstract data attributes
@property
def values(self):
# Note: PeriodArray overrides this to return an ndarray of objects.
return self._data._data
def __array_wrap__(self, result, context=None):
Gets called after a ufunc.
result = lib.item_from_zerodim(result)
if is_bool_dtype(result) or lib.is_scalar(result):
return result
attrs = self._get_attributes_dict()
if not is_period_dtype(self) and attrs[""freq""]:
# no need to infer if freq is None
attrs[""freq""] = ""infer""
return Index(result, **attrs)
# ------------------------------------------------------------------------
def equals(self, other) -> bool:
Determines if two Index objects contain the same elements.
if self.is_(other):
return True
if not isinstance(other, ABCIndexClass):
return False
elif not isinstance(other, type(self)):
try:
other = type(self)(other)
except (ValueError, TypeError, OverflowError):
# e.g.
#  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
#  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
#  OverflowError -> Index([very_large_timedeltas])
return False
if not is_dtype_equal(self.dtype, other.dtype):
# have different timezone
return False
return np.array_equal(self.asi8, other.asi8)
@Appender(_index_shared_docs[""contains""] % _index_doc_kwargs)
def __contains__(self, key):
try:
res = self.get_loc(key)
return (
is_scalar(res)
or isinstance(res, slice)
or (is_list_like(res) and len(res))
)
except (KeyError, TypeError, ValueError):
return False
# Try to run function on index first, and then on elements of index
# Especially important for group-by functionality
def map(self, mapper, na_action=None):
try:
result = mapper(self)
# Try to use this result if we can
if isinstance(result, np.ndarray):
result = Index(result)
if not isinstance(result, Index):
raise TypeError(""The map function must return an Index object"")
return result
except Exception:
return self.astype(object).map(mapper)
def sort_values(self, return_indexer=False, ascending=True):
Return sorted copy of Index.
if return_indexer:
_as = self.argsort()
if not ascending:
_as = _as[::-1]
sorted_index = self.take(_as)
return sorted_index, _as
else:
# NB: using asi8 instead of _ndarray_values matters in numpy 1.18
#  because the treatment of NaT has been changed to put NaT last
#  instead of first.
sorted_values = np.sort(self.asi8)
attribs = self._get_attributes_dict()
freq = attribs[""freq""]
if freq is not None and not is_period_dtype(self):
if freq.n > 0 and not ascending:
freq = freq * -1
elif freq.n < 0 and ascending:
freq = freq * -1
attribs[""freq""] = freq
if not ascending:
sorted_values = sorted_values[::-1]
return self._simple_new(sorted_values, **attribs)
@Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
nv.validate_take(tuple(), kwargs)
indices = ensure_int64(indices)
maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
if isinstance(maybe_slice, slice):
return self[maybe_slice]
return ExtensionIndex.take(
self, indices, axis, allow_fill, fill_value, **kwargs
)
_can_hold_na = True
_na_value = NaT
""""""The expected NA value to use with this index.""""""
def _convert_tolerance(self, tolerance, target):
tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
if target.size != tolerance.size and tolerance.size > 1:
raise ValueError(""list-like tolerance size must match target index size"")
return tolerance
def tolist(self) -> List:
""""""
Return a list of the underlying data.
""""""
return list(self.astype(object))
def min(self, axis=None, skipna=True, *args, **kwargs):
""""""
Return the minimum value of the Index or minimum along
an axis.
See Also
numpy.ndarray.min
Series.min : Return the minimum value in a Series.
nv.validate_min(args, kwargs)
nv.validate_minmax_axis(axis)
if not len(self):
return self._na_value
i8 = self.asi8
try:
# quick check
if len(i8) and self.is_monotonic:
if i8[0] != iNaT:
return self._box_func(i8[0])
if self.hasnans:
if skipna:
min_stamp = self[~self._isnan].asi8.min()
else:
return self._na_value
else:
min_stamp = i8.min()
return self._box_func(min_stamp)
except ValueError:
return self._na_value
def argmin(self, axis=None, skipna=True, *args, **kwargs):
Returns the indices of the minimum values along an axis.
See `numpy.ndarray.argmin` for more information on the
`axis` parameter.
See Also
--------
numpy.ndarray.argmin
nv.validate_argmin(args, kwargs)
nv.validate_minmax_axis(axis)
i8 = self.asi8
if self.hasnans:
mask = self._isnan
if mask.all() or not skipna:
return -1
i8 = i8.copy()
i8[mask] = np.iinfo(""int64"").max
return i8.argmin()
def max(self, axis=None, skipna=True, *args, **kwargs):
Return the maximum value of the Index or maximum along
an axis.
See Also
--------
numpy.ndarray.max
Series.max : Return the maximum value in a Series.
""""""
nv.validate_max(args, kwargs)
nv.validate_minmax_axis(axis)
if not len(self):
return self._na_value
i8 = self.asi8
try:
# quick check
if len(i8) and self.is_monotonic:
if i8[-1] != iNaT:
return self._box_func(i8[-1])
if self.hasnans:
if skipna:
max_stamp = self[~self._isnan].asi8.max()
else:
return self._na_value
else:
max_stamp = i8.max()
return self._box_func(max_stamp)
except ValueError:
return self._na_value
def argmax(self, axis=None, skipna=True, *args, **kwargs):
""""""
Returns the indices of the maximum values along an axis.
See `numpy.ndarray.argmax` for more information on the
`axis` parameter.
See Also
--------
numpy.ndarray.argmax
""""""
nv.validate_argmax(args, kwargs)
nv.validate_minmax_axis(axis)
i8 = self.asi8
if self.hasnans:
mask = self._isnan
if mask.all() or not skipna:
return -1
i8 = i8.copy()
i8[mask] = 0
return i8.argmax()
# --------------------------------------------------------------------
# Rendering Methods
def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
return header + list(self._format_native_types(na_rep, **kwargs))
def _formatter_func(self):
def _format_attrs(self):
Return a list of tuples of the (attr,formatted_value).
attrs = super()._format_attrs()
for attrib in self._attributes:
if attrib == ""freq"":
freq = self.freqstr
if freq is not None:
freq = repr(freq)
attrs.append((""freq"", freq))
return attrs
# --------------------------------------------------------------------
def _convert_scalar_indexer(self, key, kind=None):
We don't allow integer or float indexing on datetime-like when using
loc.
Parameters
----------
key : label of the slice bound
kind : {'ix', 'loc', 'getitem', 'iloc'} or None
""""""
assert kind in [""ix"", ""loc"", ""getitem"", ""iloc"", None]
# we don't allow integer/float indexing for loc
# we don't allow float indexing for ix/getitem
if is_scalar(key):
is_int = is_integer(key)
is_flt = is_float(key)
if kind in [""loc""] and (is_int or is_flt):
self._invalid_indexer(""index"", key)
elif kind in [""ix"", ""getitem""] and is_flt:
self._invalid_indexer(""index"", key)
return super()._convert_scalar_indexer(key, kind=kind)
__add__ = make_wrapped_arith_op(""__add__"")
__radd__ = make_wrapped_arith_op(""__radd__"")
__sub__ = make_wrapped_arith_op(""__sub__"")
__rsub__ = make_wrapped_arith_op(""__rsub__"")
__pow__ = make_wrapped_arith_op(""__pow__"")
__rpow__ = make_wrapped_arith_op(""__rpow__"")
__mul__ = make_wrapped_arith_op(""__mul__"")
__rmul__ = make_wrapped_arith_op(""__rmul__"")
__floordiv__ = make_wrapped_arith_op(""__floordiv__"")
__rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
__mod__ = make_wrapped_arith_op(""__mod__"")
__rmod__ = make_wrapped_arith_op(""__rmod__"")
__divmod__ = make_wrapped_arith_op(""__divmod__"")
__rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
__truediv__ = make_wrapped_arith_op(""__truediv__"")
__rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
def isin(self, values, level=None):
""""""
Compute boolean array of whether each index value is found in the
passed set of values.
Parameters
----------
values : set or sequence of values
is_contained : ndarray (boolean dtype)
if level is not None:
self._validate_index_level(level)
if not isinstance(values, type(self)):
try:
values = type(self)(values)
except ValueError:
return self.astype(object).isin(values)
return algorithms.isin(self.asi8, values.asi8)
@Appender(_index_shared_docs[""repeat""] % _index_doc_kwargs)
def repeat(self, repeats, axis=None):
nv.validate_repeat(tuple(), dict(axis=axis))
result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)
return self._shallow_copy(result)
@Appender(_index_shared_docs[""where""] % _index_doc_kwargs)
def where(self, cond, other=None):
values = self.view(""i8"")
if is_scalar(other) and isna(other):
other = NaT.value
# Do type inference if necessary up front
# e.g. we passed PeriodIndex.values and got an ndarray of Periods
other = Index(other)
if is_categorical_dtype(other):
# e.g. we have a Categorical holding self.dtype
if needs_i8_conversion(other.categories):
other = other._internal_get_values()
if not is_dtype_equal(self.dtype, other.dtype):
raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
other = other.view(""i8"")
result = np.where(cond, values, other).astype(""i8"")
return self._shallow_copy(result)
def _summary(self, name=None):
Return a summarized representation.
name : str
Name to use in the summary representation.
str
Summarized representation of the index.
formatter = self._formatter_func
if len(self) > 0:
index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
index_summary = """"
if name is None:
name = type(self).__name__
result = f""{name}: {len(self)} entries{index_summary}""
if self.freq:
result += f""\nFreq: {self.freqstr}""
# display as values, not quoted
result = result.replace(""'"", """")
return result
def _concat_same_dtype(self, to_concat, name):
""""""
Concatenate to_concat which has the same class.
""""""
attribs = self._get_attributes_dict()
attribs[""name""] = name
# do not pass tz to set because tzlocal cannot be hashed
if len({str(x.dtype) for x in to_concat}) != 1:
raise ValueError(""to_concat must have the same tz"")
new_data = type(self._values)._concat_same_type(to_concat).asi8
# GH 3232: If the concat result is evenly spaced, we can retain the
# original frequency
is_diff_evenly_spaced = len(unique_deltas(new_data)) == 1
if not is_period_dtype(self) and not is_diff_evenly_spaced:
# reset freq
attribs[""freq""] = None
return self._simple_new(new_data, **attribs)
@Appender(_index_shared_docs[""astype""])
def astype(self, dtype, copy=True):
if is_dtype_equal(self.dtype, dtype) and copy is False:
# Ensure that self.astype(self.dtype) is self
return self
new_values = self._data.astype(dtype, copy=copy)
# pass copy=False because any copying will be done in the
#  _data.astype call above
return Index(new_values, dtype=new_values.dtype, name=self.name, copy=False)
def shift(self, periods=1, freq=None):
Shift index by desired number of time frequency increments.
This method is for shifting the values of datetime-like indexes
by a specified time increment a given number of times.
periods : int, default 1
Number of periods (or increments) to shift by,
can be positive or negative.
.. versionchanged:: 0.24.0
freq : pandas.DateOffset, pandas.Timedelta or string, optional
Frequency increment to shift by.
If None, the index is shifted by its own `freq` attribute.
Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.
Returns
-------
pandas.DatetimeIndex
Shifted index.
Index.shift : Shift values of Index.
PeriodIndex.shift : Shift values of PeriodIndex.
result = self._data._time_shift(periods, freq=freq)
return type(self)(result, name=self.name)
# --------------------------------------------------------------------
# List-like Methods
def delete(self, loc):
new_i8s = np.delete(self.asi8, loc)
freq = None
if is_period_dtype(self):
freq = self.freq
elif is_integer(loc):
if loc in (0, -len(self), -1, len(self) - 1):
freq = self.freq
if is_list_like(loc):
loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
if isinstance(loc, slice) and loc.step in (1, None):
if loc.start in (0, None) or loc.stop in (len(self), None):
freq = self.freq
return self._shallow_copy(new_i8s, freq=freq)
""""""
Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
but not PeriodIndex
""""""
# Compat for frequency inference, see GH#23789
_is_monotonic_increasing = Index.is_monotonic_increasing
_is_monotonic_decreasing = Index.is_monotonic_decreasing
_is_unique = Index.is_unique
def _set_freq(self, freq):
Set the _freq attribute on our underlying DatetimeArray.
freq : DateOffset, None, or ""infer""
""""""
# GH#29843
if freq is None:
# Always valid
pass
elif len(self) == 0 and isinstance(freq, DateOffset):
# Always valid.  In the TimedeltaIndex case, we assume this
#  is a Tick offset.
pass
# As an internal method, we can ensure this assertion always holds
assert freq == ""infer""
freq = to_offset(self.inferred_freq)
self._data._freq = freq
def _shallow_copy(self, values=None, **kwargs):
if values is None:
values = self._data
if isinstance(values, type(self)):
values = values._data
attributes = self._get_attributes_dict()
if ""freq"" not in kwargs and self.freq is not None:
if isinstance(values, (DatetimeArray, TimedeltaArray)):
if values.freq is None:
del attributes[""freq""]
attributes.update(kwargs)
return self._simple_new(values, **attributes)
# --------------------------------------------------------------------
# Set Operation Methods
@Appender(Index.difference.__doc__)
def difference(self, other, sort=None):
new_idx = super().difference(other, sort=sort)
new_idx._set_freq(None)
return new_idx
def intersection(self, other, sort=False):
Specialized intersection for DatetimeIndex/TimedeltaIndex.
May be much faster than Index.intersection
other : Same type as self or array-like
sort : False or None, default False
Sort the resulting index if possible.
.. versionadded:: 0.24.0
.. versionchanged:: 0.24.1
Changed the default to ``False`` to match the behaviour
from before 0.24.0.
.. versionchanged:: 0.25.0
The `sort` keyword is added
y : Index or same type as self
self._validate_sort_keyword(sort)
self._assert_can_do_setop(other)
if self.equals(other):
return self._get_reconciled_name_object(other)
if len(self) == 0:
return self.copy()
if len(other) == 0:
return other.copy()
if not isinstance(other, type(self)):
result = Index.intersection(self, other, sort=sort)
if isinstance(result, type(self)):
if result.freq is None:
result._set_freq(""infer"")
return result
elif (
other.freq is None
or self.freq is None
or other.freq != self.freq
or not other.freq.is_anchored()
or (not self.is_monotonic or not other.is_monotonic)
):
result = Index.intersection(self, other, sort=sort)
# Invalidate the freq of `result`, which may not be correct at
# this point, depending on the values.
result._set_freq(None)
result = self._shallow_copy(
result._data, name=result.name, dtype=result.dtype, freq=None
if result.freq is None:
result._set_freq(""infer"")
return result
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
else:
left, right = other, self
# after sorting, the intersection always starts with the right index
# and ends with the index of which the last elements is smallest
end = min(left[-1], right[-1])
start = right[0]
if end < start:
return type(self)(data=[])
else:
lslice = slice(*left.slice_locs(start, end))
left_chunk = left.values[lslice]
return self._shallow_copy(left_chunk)
def _can_fast_union(self, other) -> bool:
if not isinstance(other, type(self)):
return False
freq = self.freq
if freq is None or freq != other.freq:
return False
if not self.is_monotonic or not other.is_monotonic:
return False
if len(self) == 0 or len(other) == 0:
return True
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
else:
left, right = other, self
right_start = right[0]
left_end = left[-1]
# Only need to ""adjoin"", not overlap
return (right_start == left_end + freq) or right_start in left
# if we are comparing a freq that does not propagate timezones
# this will raise
return False
def _fast_union(self, other, sort=None):
if len(other) == 0:
return self.view(type(self))
if len(self) == 0:
return other.view(type(self))
# to make our life easier, ""sort"" the two ranges
if self[0] <= other[0]:
left, right = self, other
elif sort is False:
# TDIs are not in the ""correct"" order and we don't want
#  to sort but want to remove overlaps
left, right = self, other
left_start = left[0]
loc = right.searchsorted(left_start, side=""left"")
right_chunk = right.values[:loc]
dates = concat_compat((left.values, right_chunk))
return self._shallow_copy(dates)
left, right = other, self
left_end = left[-1]
right_end = right[-1]
# concatenate
if left_end < right_end:
loc = right.searchsorted(left_end, side=""right"")
right_chunk = right.values[loc:]
dates = concat_compat((left.values, right_chunk))
return self._shallow_copy(dates)
return left
def _union(self, other, sort):
if not len(other) or self.equals(other) or not len(self):
return super()._union(other, sort=sort)
# We are called by `union`, which is responsible for this validation
assert isinstance(other, type(self))
this, other = self._maybe_utc_convert(other)
if this._can_fast_union(other):
return this._fast_union(other, sort=sort)
result = Index._union(this, other, sort=sort)
if isinstance(result, type(self)):
assert result._data.dtype == this.dtype
if result.freq is None:
result._set_freq(""infer"")
return result
# --------------------------------------------------------------------
# Join Methods
_join_precedence = 10
_inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
_outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
_left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
_left_indexer_unique = _join_i8_wrapper(
libjoin.left_join_indexer_unique, with_indexers=False
)
def join(
self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
):
See Index.join
if self._is_convertible_to_index_for_join(other):
try:
other = type(self)(other)
except (TypeError, ValueError):
pass
this, other = self._maybe_utc_convert(other)
return Index.join(
this,
other,
how=how,
level=level,
return_indexers=return_indexers,
sort=sort,
)
def _maybe_utc_convert(self, other):
this = self
if not hasattr(self, ""tz""):
return this, other
if isinstance(other, type(self)):
if self.tz is not None:
if other.tz is None:
raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
elif other.tz is not None:
raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
if not timezones.tz_compare(self.tz, other.tz):
this = self.tz_convert(""UTC"")
other = other.tz_convert(""UTC"")
return this, other
@classmethod
def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
""""""
return a boolean whether I can attempt conversion to a
DatetimeIndex/TimedeltaIndex
""""""
if isinstance(other, cls):
return False
elif len(other) > 0 and other.inferred_type not in (
""floating"",
""mixed-integer"",
""integer"",
""integer-na"",
""mixed-integer-float"",
""mixed"",
):
return True
return False
def _wrap_joined_index(self, joined, other):
name = get_op_result_name(self, other)
if (
isinstance(other, type(self))
and self.freq == other.freq
and self._can_fast_union(other)
):
joined = self._shallow_copy(joined)
joined.name = name
return joined
kwargs = {}
if hasattr(self, ""tz""):
kwargs[""tz""] = getattr(other, ""tz"", None)
return self._simple_new(joined, name, **kwargs)
Delegation mechanism, specific for Datetime, Timedelta, and Period types.
Functionality is delegated from the Index class to an Array class. A
few things can be customized
* _delegated_methods, delegated_properties : List
The list of property / method names being delagated.
* raw_methods : Set
The set of methods whose results should should *not* be
boxed in an index, after being returned from the array
* raw_properties : Set
The set of properties whose results should should *not* be
boxed in an index, after being returned from the array
# raw_methods : dispatch methods that shouldn't be boxed in an Index
_raw_methods: Set[str] = set()
# raw_properties : dispatch properties that shouldn't be boxed in an Index
_raw_properties: Set[str] = set()
_data: ExtensionArray
def _delegate_property_get(self, name, *args, **kwargs):
result = getattr(self._data, name)
if name not in self._raw_properties:
result = Index(result, name=self.name)
return result
def _delegate_property_set(self, name, value, *args, **kwargs):
setattr(self._data, name, value)
def _delegate_method(self, name, *args, **kwargs):
result = operator.methodcaller(name, *args, **kwargs)(self._data)
if name not in self._raw_methods:
result = Index(result, name=self.name)
return result"
pandas,94,"taken = ExtensionIndex.take(
# keep freq in PeriodArray/Index, reset otherwise
freq = self.freq if is_period_dtype(self) else None
assert taken.freq == freq, (taken.freq, freq, taken)
return self._shallow_copy(taken, freq=freq)
freq = self.freq if is_period_dtype(self) else None
return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)","DatetimeArray,
ExtensionArray,
ExtensionOpsMixin,
TimedeltaArray,
return ExtensionIndex.take(
result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)
return self._shallow_copy(result)
def _shallow_copy(self, values=None, **kwargs):
if values is None:
values = self._data
if isinstance(values, type(self)):
values = values._data
attributes = self._get_attributes_dict()
if ""freq"" not in kwargs and self.freq is not None:
if isinstance(values, (DatetimeArray, TimedeltaArray)):
if values.freq is None:
del attributes[""freq""]
attributes.update(kwargs)
return self._simple_new(values, **attributes)"
pandas,95,"ordinal_op = getattr(self.asi8, opname)
elif isinstance(other, int):
# TODO: sure we want to allow this?  we dont for DTA/TDA
#  2 tests rely on this
other = Period(other, freq=self.freq)
result = ordinal_op(other.ordinal)",
pandas,96,skip_bd = BusinessDay(n=bd),"if isinstance(self, _CustomMixin):  # GH 30593
skip_bd = CustomBusinessDay(
n=bd,
weekmask=self.weekmask,
holidays=self.holidays,
calendar=self.calendar,
)
else:
skip_bd = BusinessDay(n=bd)"
pandas,97,"return this._fast_union(other)
def _fast_union(self, other):","return this._fast_union(other, sort=sort)
def _fast_union(self, other, sort=None):
elif sort is False:
# TDIs are not in the ""correct"" order and we don't want
#  to sort but want to remove overlaps
left, right = self, other
left_start = left[0]
loc = right.searchsorted(left_start, side=""left"")
right_chunk = right.values[:loc]
dates = concat_compat((left.values, right_chunk))
return self._shallow_copy(dates)"
pandas,98,"elif (
is_interval_dtype(data) or is_interval_dtype(dtype)
) and not is_object_dtype(dtype):
closed = kwargs.get(""closed"", None)
return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)
elif is_period_dtype(data) and not is_object_dtype(dtype):
return PeriodIndex(data, copy=copy, name=name, **kwargs)","elif is_interval_dtype(data) or is_interval_dtype(dtype):
closed = kwargs.pop(""closed"", None)
if is_dtype_equal(_o_dtype, dtype):
return IntervalIndex(
data, name=name, copy=copy, closed=closed, **kwargs
).astype(object)
return IntervalIndex(
data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs
)
elif is_period_dtype(data) or is_period_dtype(dtype):
if is_dtype_equal(_o_dtype, dtype):
return PeriodIndex(data, copy=False, name=name, **kwargs).astype(object)
return PeriodIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)"
pandas,99,"arg = getattr(arg, ""values"", arg)
result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)","arg = getattr(arg, ""_values"", arg)
# GH 30050 pass an ndarray to tslib.array_with_unit_to_datetime
# because it expects an ndarray argument
if isinstance(arg, IntegerArray):
# Explicitly pass NaT mask to array_with_unit_to_datetime
mask = arg.isna()
arg = arg._ndarray_values
else:
mask = None
result, tz_parsed = tslib.array_with_unit_to_datetime(
arg, mask, unit, errors=errors
)"
PySnooper,1,"encoding = 'ascii'
with open(self.path, 'w' if self.overwrite else 'a') as output_file:
template = '\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))
","from io import open
encoding = 'utf-8'
with open(self.path, 'w' if self.overwrite else 'a',
encoding='utf-8') as output_file:
template = u'\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))"
PySnooper,2,"result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]
encoding = 'ascii'
with open(self.path, 'w' if self.overwrite else 'a') as output_file:
stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])
get_local_reprs(frame, watch=self.watch)
return_value_repr = utils.get_shortish_repr(arg)","from io import open
result_items = [(key, utils.get_shortish_repr(value, custom_repr=custom_repr)) for key, value in frame.f_locals.items()]
encoding = 'utf-8'
with open(self.path, 'w' if self.overwrite else 'a',
encoding='utf-8') as output_file:
Customize how values are represented as strings::
@pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))
custom_repr=(),
if len(custom_repr) == 2 and not all(isinstance(x,
pycompat.collections_abc.Iterable) for x in custom_repr):
custom_repr = (custom_repr,)
self.custom_repr = custom_repr
if DISABLED:
return function
if DISABLED:
return
stack = self.thread_local.__dict__.setdefault(
'original_trace_functions', []
)
if DISABLED:
return
get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)
return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)"
PySnooper,3,"with open(output_path, 'a') as output_file:","with open(output, 'a') as output_file:"
sanic,1,self.named_response_middleware[_rn].append(middleware),self.named_response_middleware[_rn].appendleft(middleware)
sanic,2,,
sanic,3,"netloc = self.config.get(""SERVER_NAME"", """")","# If the route has host defined, split that off
# TODO: Retain netloc and path separately in Route objects
host = uri.find(""/"")
if host > 0:
host, uri = uri[:host], uri[host:]
else:
host = None
netloc = host or self.config.get(""SERVER_NAME"", """")"
sanic,4,"if ""//"" in self.app.config.SERVER_NAME:
return self.app.url_for(view_name, _external=True, **kwargs)","try:
if ""//"" in self.app.config.SERVER_NAME:
return self.app.url_for(view_name, _external=True, **kwargs)
except AttributeError:
pass"
sanic,5,"""root"": {""level"": ""INFO"", ""handlers"": [""console""]},","""sanic.root"": {""level"": ""INFO"", ""handlers"": [""console""]},"
scrapy,1,"if url_pattern.match(domain):
domains = [re.escape(d) for d in allowed_domains if d is not None]","domains = []
if domain is None:
continue
elif url_pattern.match(domain):
else:
domains.append(re.escape(domain))"
scrapy,10,"# HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8
location = to_native_str(response.headers['location'].decode('latin1'))",location = safe_url_string(response.headers['location'])
scrapy,11,output += f.extrabuf,output += f.extrabuf[-f.extrasize:]
scrapy,12,,
scrapy,13,EXPIRES = 0,EXPIRES = 90
scrapy,14,"return ctype in (b'application/x-gzip', b'application/gzip')",return _is_gzipped_re.search(ctype) is not None
scrapy,15,"to_native_str(parts.netloc.encode('idna')),","# IDNA encoding can fail for too long labels (>63 characters)
# or missing labels (e.g. http://.example.com)
try:
netloc = parts.netloc.encode('idna')
except UnicodeError:
netloc = parts.netloc
to_native_str(netloc),"
scrapy,16,"unquote)
- percent encode paths and query arguments. non-ASCII characters are
percent-encoded using UTF-8 (RFC-3986)
- remove query arguments with blank values (unless keep_blank_values is True)
- remove fragments (unless keep_fragments is True)
The url passed can be a str or unicode, while the url returned is always a
str.
scheme, netloc, path, params, query, fragment = parse_url(url)
keyvals = parse_qsl(query, keep_blank_values)
# XXX: copied from w3lib.url.safe_url_string to add encoding argument
# path = to_native_str(path, encoding)
# path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'
path = safe_url_string(_unquotepath(path)) or '/'
return unquote(path)
return urlparse(to_native_str(url, encoding))","quote, unquote)
from urllib.parse import unquote_to_bytes
return (
to_native_str(parts.scheme),
to_native_str(parts.netloc.encode('idna')),
# default encoding for path component SHOULD be UTF-8
quote(to_bytes(parts.path, path_encoding), _safe_chars),
quote(to_bytes(parts.params, path_encoding), _safe_chars),
# encoding of query and fragment follows page encoding
# or form-charset (if known and passed)
quote(to_bytes(parts.query, encoding), _safe_chars),
quote(to_bytes(parts.fragment, encoding), _safe_chars)
)
- percent encode paths ; non-ASCII characters are percent-encoded
using UTF-8 (RFC-3986)
- percent encode query arguments ; non-ASCII characters are percent-encoded
using passed `encoding` (UTF-8 by default)
- remove query arguments with blank values (unless `keep_blank_values` is True)
- remove fragments (unless `keep_fragments` is True)
The url passed can be bytes or unicode, while the url returned is
always a native str (bytes in Python 2, unicode in Python 3).
# If supplied `encoding` is not compatible with all characters in `url`,
# fallback to UTF-8 as safety net.
# UTF-8 can handle all Unicode characters,
# so we should be covered regarding URL normalization,
# if not for proper URL expected by remote website.
try:
scheme, netloc, path, params, query, fragment = _safe_ParseResult(
parse_url(url), encoding=encoding)
except UnicodeError as e:
if encoding != 'utf8':
scheme, netloc, path, params, query, fragment = _safe_ParseResult(
parse_url(url), encoding='utf8')
else:
raise
# 1. decode query-string as UTF-8 (or keep raw bytes),
#    sort values,
#    and percent-encode them back
if not six.PY2:
# Python3's urllib.parse.parse_qsl does not work as wanted
# for percent-encoded characters that do not match passed encoding,
# they get lost.
#
# e.g., 'q=b%a3' becomes [('q', 'b\ufffd')]
# (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),
#      instead of \xa3 that you get with Python2's parse_qsl)
#
# what we want here is to keep raw bytes, and percent encode them
# so as to preserve whatever encoding what originally used.
#
# See https://tools.ietf.org/html/rfc3987#section-6.4:
#
# For example, it is possible to have a URI reference of
# ""http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9"", where the
# document name is encoded in iso-8859-1 based on server settings, but
# where the fragment identifier is encoded in UTF-8 according to
# [XPointer]. The IRI corresponding to the above URI would be (in XML
# notation)
# ""http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;"".
# Similar considerations apply to query parts.  The functionality of
# IRIs (namely, to be able to include non-ASCII characters) can only be
# used if the query part is encoded in UTF-8.
keyvals = parse_qsl_to_bytes(query, keep_blank_values)
else:
keyvals = parse_qsl(query, keep_blank_values)
# 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)
#    and percent-encode path again (this normalizes to upper-case %XX)
uqp = _unquotepath(path)
path = quote(uqp, _safe_chars) or '/'
# every part should be safe already
if six.PY3:
# standard lib's unquote() does not work in Python 3
# for non-UTF-8 percent-escaped characters, they get lost.
# e.g., '%a3' becomes 'REPLACEMENT CHARACTER' (U+FFFD)
#
# unquote_to_bytes() returns raw bytes instead
return unquote_to_bytes(path)
else:
# in Python 2, '%a3' becomes '\xa3', which is what we want
return unquote(path)
return urlparse(to_unicode(url, encoding))
from urllib.parse import _coerce_args, unquote_to_bytes
def parse_qsl_to_bytes(qs, keep_blank_values=False, strict_parsing=False):
""""""Parse a query given as a string argument.
Data are returned as a list of name, value pairs as bytes.
Arguments:
qs: percent-encoded query string to be parsed
keep_blank_values: flag indicating whether blank values in
percent-encoded queries should be treated as blank strings.  A
true value indicates that blanks should be retained as blank
strings.  The default false value indicates that blank values
are to be ignored and treated as if they were  not included.
strict_parsing: flag indicating what to do with parsing errors. If
false (the default), errors are silently ignored. If true,
errors raise a ValueError exception.
""""""
# This code is the same as Python3's parse_qsl()
# (at https://hg.python.org/cpython/rev/c38ac7ab8d9a)
# except for the unquote(s, encoding, errors) calls replaced
# with unquote_to_bytes(s)
qs, _coerce_result = _coerce_args(qs)
pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]
r = []
for name_value in pairs:
if not name_value and not strict_parsing:
continue
nv = name_value.split('=', 1)
if len(nv) != 2:
if strict_parsing:
raise ValueError(""bad query field: %r"" % (name_value,))
# Handle case of a control-name with no equal sign
if keep_blank_values:
nv.append('')
else:
continue
if len(nv[1]) or keep_blank_values:
name = nv[0].replace('+', ' ')
name = unquote_to_bytes(name)
name = _coerce_result(name)
value = nv[1].replace('+', ' ')
value = unquote_to_bytes(value)
value = _coerce_result(value)
r.append((name, value))
return r"
scrapy,17,">>> response_status_message(200)
'200 OK'
>>> response_status_message(404)
'404 Not Found'
return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))","return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), ""Unknown Status"")))"
scrapy,18,filename = to_native_str(content_disposition).split(';')[1].split('=')[1],"filename = to_native_str(content_disposition,
encoding='latin-1', errors='replace').split(';')[1].split('=')[1]"
scrapy,19,"# python3 uses request.unverifiable
def get_origin_req_host(self):
return urlparse_cached(self.request).hostname","def get_origin_req_host(self):
return urlparse_cached(self.request).hostname
# python3 uses attributes instead of methods
@property
def full_url(self):
return self.get_full_url()
@property
def host(self):
return self.get_host()
@property
def type(self):
return self.get_type()
@property
def origin_req_host(self):
return self.get_origin_req_host()"
scrapy,2,"while len(self) >= self.limit:
self.popitem(last=False)","if self.limit:
while len(self) >= self.limit:
self.popitem(last=False)"
scrapy,20,for url in sitemap_urls_from_robots(response.body):,for url in sitemap_urls_from_robots(response.text):
scrapy,21,self._parsers.pop(netloc).callback(None),"rp_dfd = self._parsers[netloc]
self._parsers[netloc] = None
rp_dfd.callback(None)"
scrapy,22,else:,"elif isinstance(serialized_value, six.text_type):
else:
self._xg_characters(str(serialized_value))"
scrapy,23,"user_pass = '%s:%s' % (unquote(user), unquote(password))
request.headers['Proxy-Authorization'] = 'Basic ' + creds
rsp = Response('http://www.scrapytest.org/503', body='', status=503)
rsp = Response('http://www.scrapytest.org/404', body='', status=404)
rsp = Response('http://www.scrapytest.org/503', body='', status=503)
rsp = Response('http://www.scrapytest.org/503', body='', status=503)","user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))
request.headers['Proxy-Authorization'] = b'Basic ' + creds
rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)
rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)"
scrapy,24,"_responseMatcher = re.compile('HTTP/1\.. 200')
tunnelReq = 'CONNECT %s:%s HTTP/1.1\r\n' % (self._tunneledHost,
self._tunneledPort)
tunnelReq += 'Proxy-Authorization: %s\r\n' % self._proxyAuthHeader
tunnelReq += '\r\n'","_responseMatcher = re.compile(b'HTTP/1\.. 200')
tunnelReq = (
b'CONNECT ' +
to_bytes(self._tunneledHost, encoding='ascii') + b':' +
to_bytes(str(self._tunneledPort)) +
b' HTTP/1.1\r\n')
tunnelReq += \
b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\r\n'
tunnelReq += b'\r\n'"
scrapy,25,"return form.action or form.base_url
root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)","return urljoin(form.base_url, form.action)
root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))"
scrapy,26,"
``False`` and ``None`` return ``False``.
compsett = BaseSettings(self[name + ""_BASE""], priority='default')
compsett.update(self[name])
else:
return self[name]","``False`` and ``None`` return ``False``.
# When users defined a _BASE setting, they explicitly don't want to
# use any of Scrapy's defaults. Therefore, we only use these entries
# from self[name] (where the defaults now live) that have a priority
# higher than 'default'
compsett = BaseSettings(self[basename], priority='default')
for k in self[name]:
prio = self[name].getpriority(k)
if prio > get_settings_priority('default'):
compsett.set(k, self[name][k], prio)
return self[name]"
scrapy,27,"response.status in getattr(spider, 'handle_httpstatus_list', [])):","response.status in getattr(spider, 'handle_httpstatus_list', []) or
response.status in request.meta.get('handle_httpstatus_list', []) or
request.meta.get('handle_httpstatus_all', False)):"
scrapy,28,,
scrapy,29,"s += b""Host: "" + to_bytes(parsed.hostname) + b""\r\n""","s += b""Host: "" + to_bytes(parsed.hostname or b'') + b""\r\n"""
scrapy,3,location = safe_url_string(response.headers['location']),"location = safe_url_string(response.headers['Location'])
if response.headers['Location'].startswith(b'//'):
request_scheme = urlparse(request.url).scheme
location = request_scheme + '://' + location.lstrip('/')"
scrapy,30,"for obj in vars(module).itervalues():
issubclass(obj, ScrapyCommand) and \
obj.__module__ == module.__name__:
self.out = ''
self.err = ''
comm = proc.communicate()
return comm[0].strip()","for obj in vars(module).values():
issubclass(obj, ScrapyCommand) and \
obj.__module__ == module.__name__:
self.out = b''
self.err = b''
encoding = getattr(sys.stdout, 'encoding') or 'utf-8'
comm = proc.communicate()[0].strip()
return comm.decode(encoding)"
scrapy,31,"return to_native_str(self.request.headers.get(name, default))
(to_native_str(k), [to_native_str(x) for x in v])
return [to_native_str(v) for v in self.response.headers.getlist(name)]","return to_native_str(self.request.headers.get(name, default),
errors='replace')
(to_native_str(k, errors='replace'),
[to_native_str(x, errors='replace') for x in v])
return [to_native_str(v, errors='replace')
for v in self.response.headers.getlist(name)]"
scrapy,32,"configure_logging(settings)
log_scrapy_info(settings)","configure_logging(self.settings)
log_scrapy_info(self.settings)"
scrapy,33,"extra={'spider': spider, 'failure': f}))
extra={'spider': spider, 'failure': f}))
extra={'spider': spider, 'failure': f}))
extra={'spider': spider, 'failure': f}))
logger.error(msg, extra={'spider': spider, 'failure': failure})
extra={'spider': spider, 'failure': f}))
extra={'spider': spider, 'failure': _failure}
extra={'spider': spider, 'failure': download_failure})
extra={'spider': spider, 'failure': output})
extra={'spider': spider, 'failure': failure})
extra={'spider': spider, 'failure': f}))
""the builtin Python library for logging. Read the updated ""
""logging entry in the documentation to learn more."",
ScrapyDeprecationWarning, stacklevel=2)
warnings.warn('log.msg has been deprecated, create a python logger and '
'log through it instead',
ScrapyDeprecationWarning, stacklevel=2)
level = kw.pop('level', _level)
message = kw.pop('format', message)
# NOTE: logger.log doesn't handle well passing empty dictionaries with format
# arguments because of some weird use-case:
# https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269
logger.log(level, message, *[kw] if kw else [])
warnings.warn('log.err has been deprecated, create a python logger and '
'use its error method instead',
ScrapyDeprecationWarning, stacklevel=2)
level = kw.pop('level', logging.ERROR)
failure = kw.pop('failure', _stuff) or Failure()
message = kw.pop('why', _why) or failure.value
logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})
extra={'spider': info.spider, 'failure': f})
f.value, extra={'spider': info.spider, 'failure': f})
extra={'spider': info.spider, 'failure': value}
extra={'spider': spider, 'failure': failure})","exc_info=failure_to_exc_info(f),
extra={'spider': spider}))
exc_info=failure_to_exc_info(f),
extra={'spider': spider}))
exc_info=failure_to_exc_info(f),
extra={'spider': spider}))
exc_info=failure_to_exc_info(f),
extra={'spider': spider}))
logger.error(
msg,
exc_info=failure_to_exc_info(failure),
extra={'spider': spider}
)
exc_info=failure_to_exc_info(f),
extra={'spider': spider}))
exc_info=failure_to_exc_info(_failure),
extra={'spider': spider}
exc_info=failure_to_exc_info(download_failure),
extra={'spider': spider})
exc_info=failure_to_exc_info(output),
extra={'spider': spider})
exc_info=failure_to_exc_info(failure),
extra={'spider': spider})
exc_info=failure_to_exc_info(f),
extra={'spider': spider}))
""""""Extract exc_info from Failure instances""""""
if isinstance(failure, Failure):
return (failure.type, failure.value, failure.tb)
""""""Keep only top level loggers's name (direct children from root) from
records.
This filter will replace Scrapy loggers' names with 'scrapy'. This mimics
the old Scrapy log behaviour and helps shortening long names.
Since it can't be set for just one logger (it won't propagate for its
children), it's going to be set in the root handler, with a parametrized
`loggers` list where it should act.
""""""
def __init__(self, loggers=None):
self.loggers = loggers or []
def filter(self, record):
if any(record.name.startswith(l + '.') for l in self.loggers):
record.name = record.name.split('.', 1)[0]
return True
'version': 1,
'disable_existing_loggers': False,
'loggers': {
'scrapy': {
'level': 'DEBUG',
},
'twisted': {
'level': 'ERROR',
},
}
""""""Initialize and configure default loggers
This function does:
- Route warnings and twisted logging through Python standard logging
- Set FailureFormatter filter on Scrapy logger
- Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
- Create a handler for the root logger according to given settings
""""""
if not sys.warnoptions:
# Route warnings through python logging
logging.captureWarnings(True)
observer = twisted_log.PythonLoggingObserver('twisted')
observer.start()
dictConfig(DEFAULT_LOGGING)
if isinstance(settings, dict):
settings = Settings(settings)
if settings:
logging.root.setLevel(logging.NOTSET)
if settings.getbool('LOG_STDOUT'):
sys.stdout = StreamLogger(logging.getLogger('stdout'))
# Set up the default log handler
filename = settings.get('LOG_FILE')
if filename:
encoding = settings.get('LOG_ENCODING')
handler = logging.FileHandler(filename, encoding=encoding)
elif settings.getbool('LOG_ENABLED'):
handler = logging.StreamHandler()
else:
handler = logging.NullHandler()
formatter = logging.Formatter(
fmt=settings.get('LOG_FORMAT'),
datefmt=settings.get('LOG_DATEFORMAT')
)
handler.setFormatter(formatter)
handler.setLevel(settings.get('LOG_LEVEL'))
handler.addFilter(TopLevelFormatter(['scrapy']))
logging.root.addHandler(handler)
logger.info(""Scrapy %(version)s started (bot: %(bot)s)"",
{'version': scrapy.__version__, 'bot': settings['BOT_NAME']})
logger.info(""Optional features available: %(features)s"",
{'features': "", "".join(scrapy.optional_features)})
d = dict(overridden_settings(settings))
logger.info(""Overridden settings: %(settings)r"", {'settings': d})
""""""Fake file-like stream object that redirects writes to a logger instance
Taken from:
http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
""""""
def __init__(self, logger, log_level=logging.INFO):
self.logger = logger
self.log_level = log_level
self.linebuf = ''
def write(self, buf):
for line in buf.rstrip().splitlines():
self.logger.log(self.log_level, line.rstrip())
""""""Record log levels count into a crawler stats""""""
def __init__(self, crawler, *args, **kwargs):
super(LogCounterHandler, self).__init__(*args, **kwargs)
self.crawler = crawler
def emit(self, record):
sname = 'log_count/{}'.format(record.levelname)
self.crawler.stats.inc_value(sname)
""""""
Helper that takes the dictionary output from the methods in LogFormatter
and adapts it into a tuple of positional arguments for logger.log calls,
handling backward compatibility as well.
""""""
if not {'level', 'msg', 'args'} <= set(logkws):
warnings.warn('Missing keys in LogFormatter method',
ScrapyDeprecationWarning)
if 'format' in logkws:
warnings.warn('`format` key in LogFormatter methods has been '
'deprecated, use `msg` instead',
ScrapyDeprecationWarning)
level = logkws.get('level', logging.INFO)
message = logkws.get('format', logkws.get('msg'))
# NOTE: This also handles 'args' being an empty dict, that case doesn't
# play well in logger.log calls
args = logkws if not logkws.get('args') else logkws['args']
return (level, message, args)
exc_info=failure_to_exc_info(f),
extra={'spider': info.spider})
f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})
exc_info=failure_to_exc_info(value),
extra={'spider': info.spider}
exc_info=failure_to_exc_info(failure),
extra={'spider': spider})"
scrapy,34,fields = {},"fields = getattr(_class, 'fields', {})"
scrapy,35,"cls_path = settings.get('SPIDER_LOADER_CLASS',
settings.get('SPIDER_MANAGER_CLASS'))","cls_path = settings.get('SPIDER_MANAGER_CLASS',
settings.get('SPIDER_LOADER_CLASS'))"
scrapy,36,"return objcls.from_crawler(crawler, *args, **kwargs)
return objcls.from_settings(settings, *args, **kwargs)
return objcls(*args, **kwargs)","Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
extension has not been implemented correctly).
instance = objcls.from_crawler(crawler, *args, **kwargs)
method_name = 'from_crawler'
instance = objcls.from_settings(settings, *args, **kwargs)
method_name = 'from_settings'
instance = objcls(*args, **kwargs)
method_name = '__new__'
if instance is None:
raise TypeError(""%s.%s returned None"" % (objcls.__qualname__, method_name))
return instance"
scrapy,37,if ':' not in self._url:,if ('://' not in self._url) and (not self._url.startswith('data:')):
scrapy,38,"'descendant::*[(self::input or self::button)'
' and re:test(@type, ""^submit$"", ""i"")]'
'|descendant::button[not(@type)]',","'descendant::input[re:test(@type, ""^(submit|image)$"", ""i"")]'
'|descendant::button[not(@type) or re:test(@type, ""^submit$"", ""i"")]',"
scrapy,39,"if self.make_requests_from_url is not Spider.make_requests_from_url:
""Spider.make_requests_from_url method is deprecated; ""
""it won't be called in future Scrapy releases. ""
""Please override start_requests method instead.""","cls = self.__class__
if cls.make_requests_from_url is not Spider.make_requests_from_url:
""Spider.make_requests_from_url method is deprecated; it ""
""won't be called in future Scrapy releases. Please ""
""override Spider.start_requests method instead (see %s.%s)."" % (
cls.__module__, cls.__name__
),"
scrapy,4,"exc_info = failure.value, failure.type, failure.getTracebackObject()","exc_info = failure.type, failure.value, failure.getTracebackObject()"
scrapy,40,"if self.binary:
return to_bytes(value, encoding=self.encoding)
else:
return to_unicode(value, encoding=self.encoding)","encode_func = to_bytes if self.binary else to_unicode
if isinstance(value, (six.text_type, bytes)):
return encode_func(value, encoding=self.encoding)
return value"
scrapy,5,,
scrapy,6,,
scrapy,7,"return urljoin(form.base_url, form.action)","action = form.get('action')
if action is None:
return form.base_url
return urljoin(form.base_url, strip_html5_whitespace(action))"
scrapy,8,,
scrapy,9,,
spacy,1,"msg = getattr(err_cls, code)
return ""[{code}] {msg}"".format(code=code, msg=msg)","if not code.startswith('__'):
msg = getattr(err_cls, code)
return ""[{code}] {msg}"".format(code=code, msg=msg)
else:
return super().__getattribute__(code)"
spacy,10,,
spacy,2,,
spacy,3,text_search = text_regex.search(article_text),"text_search = text_tag_regex.sub("""", article_text)
text_search = text_regex.search(text_search)"
spacy,4,"head = (int(head) - 1) if head != ""0"" else id_","head = (int(head) - 1) if head not in [""0"", ""_""] else id_"
spacy,5,"docs = _pipe(pipe, docs, kwargs)","docs = _pipe(docs, pipe, kwargs)"
spacy,6,return self.pipeline.pop(self.pipe_names.index(name)),"removed = self.pipeline.pop(self.pipe_names.index(name))
return removed"
spacy,7,"get_sort_key = lambda span: (span.end - span.start, span.start)
seen_tokens.update(range(span.start, span.end))
get_sort_key = lambda span: (span.end - span.start, span.start)","# For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()
get_sort_key = lambda span: (span.end - span.start, -span.start)
# Check for end - 1 here because boundaries are inclusive
seen_tokens.update(range(span.start, span.end))
result = sorted(result, key=lambda span: span.start)
get_sort_key = lambda span: (span.end - span.start, -span.start)"
spacy,8,,
spacy,9,,
thefuck,1,"broken_cmd = re.findall(r'ERROR: unknown command \""([a-z]+)\""',
new_cmd = re.findall(r'maybe you meant \""([a-z]+)\""', command.output)[0]","broken_cmd = re.findall(r'ERROR: unknown command ""([^""]+)""',
new_cmd = re.findall(r'maybe you meant ""([^""]+)""', command.output)[0]"
thefuck,10,"last_arg = command.script_parts[-1]
last_arg + ' --help',","last_arg = command.script_parts[-1]
help_command = last_arg + ' --help'
# If there are no man pages for last_arg, suggest `last_arg --help` instead.
# Otherwise, suggest `--help` after suggesting other man page sections.
if command.stderr.strip() == 'No manual entry for ' + last_arg:
return [help_command]
help_command,"
thefuck,11,"return replace_argument(command.script, 'push', push_upstream)","# If --set-upstream or -u are passed, remove it and its argument. This is
# because the remaining arguments are concatenated onto the command suggested
# by git, which includes --set-upstream and its argument
upstream_option_index = -1
try:
upstream_option_index = command.script_parts.index('--set-upstream')
except ValueError:
pass
try:
upstream_option_index = command.script_parts.index('-u')
except ValueError:
pass
if upstream_option_index is not -1:
command.script_parts.pop(upstream_option_index)
command.script_parts.pop(upstream_option_index)
return replace_argument("" "".join(command.script_parts), 'push', push_upstream)"
thefuck,12,"get_valid_history_without_current, get_closest
return (command.script_parts","get_valid_history_without_current, get_closest, which
return (not which(command.script_parts[0])"
thefuck,13,"return ('branch' in command.script
and ""fatal: A branch named '"" in command.stderr","return (""fatal: A branch named '"" in command.stderr
['git branch -d {0}', 'git checkout -b {0}'],
['git branch -D {0}', 'git checkout -b {0}'],"
thefuck,14,"overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()
if overridden_aliases:
return [alias.strip() for alias in overridden_aliases.split(',')]
else:
return ['cd', 'grep', 'ls', 'man', 'open']","default = {'cd', 'grep', 'ls', 'man', 'open'}
for alias in os.environ.get('TF_OVERRIDDEN_ALIASES', '').split(','):
default.add(alias.strip())
return default"
thefuck,15,"return ('did not match any file(s) known to git.' in command.stderr
and ""Did you forget to 'git add'?"" in command.stderr)
r""error: pathspec '([^']*)' ""
r""did not match any file\(s\) known to git."", command.stderr)[0]","return 'did not match any file(s) known to git.' in command.stderr
r""error: pathspec '([^']*)' ""
r'did not match any file\(s\) known to git.', command.stderr)[0]"
thefuck,16,"alias = ""TF_ALIAS={0}"" \
"" alias {0}='PYTHONIOENCODING=utf-8"" \
"" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && "" \
alias = ""alias {0}='TF_ALIAS={0}"" \
' TF_SHELL_ALIASES=$(alias)' \
"" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&"" \
os.environ.get('PYTHONIOENCODING', '>-not-set-<')))","# It is VERY important to have the variables declared WITHIN the alias
alias = ""alias {0}='TF_CMD=$(TF_ALIAS={0}"" \
"" PYTHONIOENCODING=utf-8"" \
"" TF_SHELL_ALIASES=$(alias)"" \
"" thefuck $(fc -ln -1)) &&"" \
# It is VERY important to have the variables declared WITHIN the alias
# It is VERY important to have the variables declared WITHIN the alias
alias = ""alias {0}='TF_CMD=$(TF_ALIAS={0}"" \
"" TF_SHELL_ALIASES=$(alias)"" \
"" thefuck $(fc -ln -1 | tail -n 1)) &&"" \
os.environ.get('PYTHONIOENCODING', '!!not-set!!')))"
thefuck,17,""" TF_CMD=$(thefuck $(fc -ln -1)) && "" \
@cache('.bashrc', '.bash_profile')
proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)
return dict(
self._parse_alias(alias)
for alias in proc.stdout.read().decode('utf-8').split('\n')
if alias and '=' in alias)
raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\n')",""" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && "" \
raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')
return dict(self._parse_alias(alias)
for alias in raw_aliases if alias and '=' in alias)
raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')"
thefuck,18,,
thefuck,19,"return replace_argument(command.script, 'push', 'push --force')","return replace_argument(command.script, 'push', 'push --force-with-lease')"
thefuck,2,"for path in os.environ.get('PATH', '').split(':')","for path in os.environ.get('PATH', '').split(os.pathsep)"
thefuck,20,"for c in command.script.split()[1:]:
return '{} -d {}'.format(command.script, _zip_file(command)[:-4])","for c in command.split_script[1:]:
return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))"
thefuck,21,"return (command.script.split()[1] == 'stash'
and 'usage:' in command.stderr)","splited_script = command.script.split()
if len(splited_script) > 1:
return (splited_script[1] == 'stash'
and 'usage:' in command.stderr)
else:
return False"
thefuck,22,"commands = self._remove_duplicates(self._commands)
self._cached = [self._cached[0]] + sorted(
commands, key=lambda corrected_command: corrected_command.priority)","if self._cached:
commands = self._remove_duplicates(self._commands)
self._cached = [self._cached[0]] + sorted(
commands, key=lambda corrected_command: corrected_command.priority)"
thefuck,23,with shelve.open(cache_path) as db:,"# A bit obscure, but simplest way to generate unique key for
# functions and methods in python 2 and 3:
with closing(shelve.open(cache_path)) as db:"
thefuck,24,"if command.script != first.script or \
command.side_effect != first.side_effect:
commands = {(command.script, command.side_effect): command
if command.script != self._cached[0].script
or command.side_effect != self._cached[0].side_effect}
return commands.values()","def __init__(self, script, side_effect, priority):
self.script = script
self.side_effect = side_effect
self.priority = priority
def __eq__(self, other):
""""""Ignores `priority` field.""""""
if isinstance(other, CorrectedCommand):
return (other.script, other.side_effect) ==\
(self.script, self.side_effect)
else:
return False
def __hash__(self):
return (self.script, self.side_effect).__hash__()
def __repr__(self):
return 'CorrectedCommand(script={}, side_effect={}, priority={})'.format(
self.script, self.side_effect, self.priority)
if command != first:
commands = {command
if command.script != self._cached[0]}
return commands"
thefuck,25,"return re.sub('^mkdir (.*)', 'mkdir -p \\1', command.script)","return re.sub('\\bmkdir (.*)', 'mkdir -p \\1', command.script)"
thefuck,26,"machine = """"
return shells.and_(""vagrant up "" +  machine, command.script)","machine = None
startAllInstances = shells.and_(""vagrant up"", command.script)
if machine is None:
return startAllInstances
else:
return [ shells.and_(""vagrant up "" +  machine, command.script), startAllInstances]"
thefuck,27,return 'open http://' + command.script[5:],"return command.script.replace('open ', 'open http://')"
thefuck,28,"'^{file} \(line {line}\):',
# ghc, make, ruby, zsh:
'^{file}:{line}:',
# ignored for now
editor_call = '{} {} +{}'.format(os.environ['EDITOR'],
m.group('file'),
m.group('line'))","'^{file} \\(line {line}\\):',
# ghc, make, ruby, zsh:
'^{file}:{line}:',
'fixcolcmd': None})
# ignored by default
if settings.fixcolcmd and 'col' in m.groupdict():
editor_call = settings.fixcolcmd.format(editor=os.environ['EDITOR'],
file=m.group('file'),
line=m.group('line'),
col=m.group('col'))
else:
editor_call = settings.fixlinecmd.format(editor=os.environ['EDITOR'],
file=m.group('file'),
line=m.group('line'))"
thefuck,29,"""""""Returns new settings with new values from `kwargs`.""""""
conf = dict(self)
conf.update(kwargs)","""""""
Returns new settings with values from `kwargs` for unset settings.
""""""
conf = dict(kwargs)
conf.update(self)"
thefuck,3,"proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],
version = proc.stdout.read().decode('utf-8').strip()","proc = Popen(['fish', '--version'],
version = proc.stdout.read().decode('utf-8').split()[-1]"
thefuck,30,return 'EDITOR' in os.environ and _search(command.stderr),"if 'EDITOR' not in os.environ:
return False
m = _search(command.stderr)
return m and os.path.isfile(m.group('file'))"
thefuck,31,return '{} --staged'.format(command.script),"return command.script.replace(' diff', ' diff --staged')"
thefuck,32,return 'ls' in command.script and not ('ls -' in command.script),"return (command.script == 'ls'
or command.script.startswith('ls ')
and not ('ls -' in command.script))"
thefuck,4,"alias_out = proc.stdout.read().decode('utf-8').strip().split('\n')
for alias in alias_out:
name, value = alias.replace('alias ', '', 1).split(' ', 1)","alias_out = proc.stdout.read().decode('utf-8').strip()
if not alias_out:
return aliases
for alias in alias_out.split('\n'):
for separator in (' ', '='):
split_alias = alias.replace('alias ', '', 1).split(separator, 1)
if len(split_alias) == 2:
name, value = split_alias
break
else:
continue"
thefuck,5,and 'set-upstream' in command.output),and 'git push --set-upstream' in command.output)
thefuck,6,"and "" already exists."" in command.output)
r""fatal: A branch named '([^']*)' already exists."", command.output)[0]","and ""' already exists."" in command.output)
r""fatal: A branch named '(.+)' already exists."", command.output)[0]
branch_name = branch_name.replace(""'"", r""\'"")"
thefuck,7,"return ""php -s"" in command.script","return "" -s "" in command.script"
thefuck,8,"# The regex has to be a bytes-style regex since reading from a file
# like stdin returns a bytes-style object and a string-style regex
# wouldn't work.
operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)
lines = proc.stdout.read()","operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)
lines = proc.stdout.read().decode(""utf-8"")"
thefuck,9,command.script_parts.pop(upstream_option_index),"try:
command.script_parts.pop(upstream_option_index)
except IndexError:
# This happens for `git push -u`
pass"
tornado,1,"assert self.stream is not None
self.stream.set_nodelay(value)","assert self.ws_connection is not None
self.ws_connection.set_nodelay(value)
@abc.abstractmethod
def set_nodelay(self, x: bool) -> None:
raise NotImplementedError()
def set_nodelay(self, x: bool) -> None:
self.stream.set_nodelay(x)"
tornado,10,,
tornado,11,"if headers.get(""Transfer-Encoding"") == ""chunked"":","if headers.get(""Transfer-Encoding"", """").lower() == ""chunked"":"
tornado,12,"args = escape.parse_qs_bytes(escape.native_str(response.body))
return self.oauth2_request(url, callback, access_token,
post_args, **args)","args = urlparse.parse_qs(escape.native_str(response.body))
# Thanks to the _auth_return_future decorator, our ""callback""
# argument is a Future, which we cannot pass as a callback to
# oauth2_request. Instead, have oauth2_request return a
# future and chain them together.
oauth_future = self.oauth2_request(url, access_token=access_token,
post_args=post_args, **args)
chain_future(oauth_future, callback)"
tornado,13,"or start_line.method in (""HEAD"", ""GET"")):","or getattr(start_line, 'method', None) in (""HEAD"", ""GET"")):
# start_line may be a request or reponse start line; only
# the former has a method attribute.
'tornado.test.http1connection_test',"
tornado,14,if IOLoop.current(instance=False) is None:,if IOLoop.current(instance=False) is not None:
tornado,15,"root = os.path.abspath(root)
# os.path.abspath strips a trailing /
# it needs to be temporarily added back for requests to root/","""static_foo.txt"",
# os.path.abspath strips a trailing /.
# We must add it back to `root` so that we only match files
# in a directory named `root` instead of files starting with
# that prefix.
root = os.path.abspath(root) + os.path.sep
# The trailing slash also needs to be temporarily added back
# the requested path so a request to root/ will match."
tornado,16,,
tornado,2,"and ""Transfer-Encoding"" not in headers","and (
""Transfer-Encoding"" not in headers
or headers[""Transfer-Encoding""] == ""chunked""
)"
tornado,3,"if self._instance_cache.get(self.io_loop) is not self:
del self._instance_cache[self.io_loop]","cached_val = self._instance_cache.pop(self.io_loop, None)
# If there's an object other than self in the instance
# cache for our IOLoop, something has gotten mixed up. A
# value of None appears to be possible when this is called
# from a destructor (HTTPClient.__del__) as the weakref
# gets cleared before the destructor runs.
if cached_val is not None and cached_val is not self:"
tornado,4,"if (start is not None and start >= size) or end == 0:
# content, or when a suffix with length 0 is specified
if start is not None and start < 0:
start += size","if start is not None and start < 0:
start += size
if start < 0:
start = 0
if (
start is not None
and (start >= size or (end is not None and start >= end))
) or end == 0:
# content, or when a suffix with length 0 is specified.
# https://tools.ietf.org/html/rfc7233#section-2.1
# A byte-range-spec is invalid if the last-byte-pos value is present
# and less than the first-byte-pos."
tornado,5,callback_time_sec = self.callback_time / 1000.0,"callback_time_sec = self.callback_time / 1000.0
# The period should be measured from the start of one call
# to the start of the next. If one call takes too long,
# skip cycles to get back to a multiple of the original
# schedule.
else:
# If the clock moved backwards, ensure we advance the next
# timeout instead of recomputing the same value again.
# This may result in long gaps between callbacks if the
# clock jumps backwards by a lot, but the far more common
# scenario is a small NTP adjustment that should just be
# ignored.
#
# Note that on some systems if time.time() runs slower
# than time.monotonic() (most common on windows), we
# effectively experience a small backwards time jump on
# every iteration because PeriodicCallback uses
# time.time() while asyncio schedules callbacks using
# time.monotonic().
# https://github.com/tornadoweb/tornado/issues/2333
self._next_timeout += callback_time_sec"
tornado,6,_ioloop_for_asyncio = weakref.WeakKeyDictionary(),"_ioloop_for_asyncio = dict()
# If an asyncio loop was closed through an asyncio interface
# instead of IOLoop.close(), we'd never hear about it and may
# have left a dangling reference in our map. In case an
# application (or, more likely, a test suite) creates and
# destroys a lot of event loops in this way, check here to
# ensure that we don't have a lot of dead loops building up in
# the map.
#
# TODO(bdarnell): consider making self.asyncio_loop a weakref
# for AsyncIOMainLoop and make _ioloop_for_asyncio a
# WeakKeyDictionary.
for loop in list(IOLoop._ioloop_for_asyncio):
if loop.is_closed():
del IOLoop._ioloop_for_asyncio[loop]
del IOLoop._ioloop_for_asyncio[self.asyncio_loop]"
tornado,7,"return executor.submit(func, *args)","c_future = executor.submit(func, *args)
# Concurrent Futures are not usable with await. Wrap this in a
# Tornado Future instead, using self.add_future for thread-safety.
t_future = TracebackFuture()
self.add_future(c_future, lambda f: chain_future(f, t_future))
return t_future"
tornado,8,,
tornado,9,,
tqdm,1,"return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))","return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)"
tqdm,2,"if ncols:
return disp_trim(res, ncols)
if ncols:
return disp_trim(res, ncols)
if RE_ANSI.search(data):  # assume ANSI reset is required
return data + ""\033[0m""","return disp_trim(res, ncols) if ncols else res
return disp_trim(res, ncols) if ncols else res
ansi_present = bool(RE_ANSI.search(data))
if ansi_present and bool(RE_ANSI.search(data)):
# assume ANSI reset is required
return data if data.endswith(""\033[0m"") else data + ""\033[0m"""
tqdm,3,,
tqdm,4,total *= unit_scale,"if total:
total *= unit_scale"
tqdm,5,"if total is None and iterable is not None:
try:
total = len(iterable)
except (TypeError, AttributeError):
total = None","if total is None and iterable is not None:
try:
total = len(iterable)
except (TypeError, AttributeError):
total = None
self.total = total"
tqdm,6,else self.total),"else getattr(self, ""total"", None))"
tqdm,7,,
tqdm,8,"l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)","l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)"
tqdm,9,"if abs(num) < 1000.0:
if abs(num) < 100.0:
if abs(num) < 10.0:
if gui: # pragma: no cover
if gui: # pragma: no cover
return len(self.iterable)
if gui: # pragma: no cover
if gui: # pragma: no cover","if abs(num) < 999.95:
if abs(num) < 99.95:
if abs(num) < 9.995:
if gui:  # pragma: no cover
if gui:  # pragma: no cover
return len(self.iterable) if self.iterable else self.total
if gui:  # pragma: no cover
if gui:  # pragma: no cover"
youtube-dl,1,"'': lambda v: v is not None,
'!': lambda v: v is None,","'': lambda v: (v is True) if isinstance(v, bool) else (v is not None),
'!': lambda v: (v is False) if isinstance(v, bool) else (v is None),"
youtube-dl,10,"""(?:[^""\\]*(?:\\\\|\\"")?)*""|
'(?:[^'\\]*(?:\\\\|\\')?)*'|","""(?:[^""\\]*(?:\\\\|\\['""nu]))*[^""\\]*""|
'(?:[^'\\]*(?:\\\\|\\['""nu]))*[^'\\]*'|"
youtube-dl,11,"if int_str is None:
return None","if not isinstance(int_str, compat_str):
return int_str"
youtube-dl,12,"op = lambda attr, value: not str_op","op = lambda attr, value: not str_op(attr, value)"
youtube-dl,13,"if re.match(r'^(?:https?:)?//', path):","if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):"
youtube-dl,14,"def _extract_chapters(description, duration):
chapters = self._extract_chapters(description_original, video_duration)","def _extract_chapters_from_json(self, webpage, video_id, duration):
if not webpage:
return
player = self._parse_json(
self._search_regex(
r'RELATED_PLAYER_ARGS[""\']\s*:\s*({.+})\s*,?\s*\n', webpage,
'player args', default='{}'),
video_id, fatal=False)
if not player or not isinstance(player, dict):
return
watch_next_response = player.get('watch_next_response')
if not isinstance(watch_next_response, compat_str):
return
response = self._parse_json(watch_next_response, video_id, fatal=False)
if not response or not isinstance(response, dict):
return
chapters_list = try_get(
response,
lambda x: x['playerOverlays']
['playerOverlayRenderer']
['decoratedPlayerBarRenderer']
['decoratedPlayerBarRenderer']
['playerBar']
['chapteredPlayerBarRenderer']
['chapters'],
list)
if not chapters_list:
return
def chapter_time(chapter):
return float_or_none(
try_get(
chapter,
lambda x: x['chapterRenderer']['timeRangeStartMillis'],
int),
scale=1000)
chapters = []
for next_num, chapter in enumerate(chapters_list, start=1):
start_time = chapter_time(chapter)
if start_time is None:
continue
end_time = (chapter_time(chapters_list[next_num])
if next_num < len(chapters_list) else duration)
if end_time is None:
continue
title = try_get(
chapter, lambda x: x['chapterRenderer']['title']['simpleText'],
compat_str)
chapters.append({
'start_time': start_time,
'end_time': end_time,
'title': title,
})
return chapters
def _extract_chapters_from_description(description, duration):
def _extract_chapters(self, webpage, description, video_id, duration):
return (self._extract_chapters_from_json(webpage, video_id, duration)
or self._extract_chapters_from_description(description, duration))
chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)"
youtube-dl,15,[a-zA-Z_][.a-zA-Z_0-9]*|,(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|
youtube-dl,16,"with io.open(dfxp_file, 'rt', encoding='utf-8') as f:
('http://www.w3.org/ns/ttml', [
'http://www.w3.org/2004/11/ttaf1',
'http://www.w3.org/2006/04/ttaf1',
'http://www.w3.org/2006/10/ttaf1',
('http://www.w3.org/ns/ttml#styling', [
'http://www.w3.org/ns/ttml#style',
dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))","with open(dfxp_file, 'rb') as f:
'''
@param dfxp_data A bytes-like object containing DFXP data
@returns A unicode object containing converted SRT data
'''
(b'http://www.w3.org/ns/ttml', [
b'http://www.w3.org/2004/11/ttaf1',
b'http://www.w3.org/2006/04/ttaf1',
b'http://www.w3.org/2006/10/ttaf1',
(b'http://www.w3.org/ns/ttml#styling', [
b'http://www.w3.org/ns/ttml#style',
dfxp = compat_etree_fromstring(dfxp_data)"
youtube-dl,17,,
youtube-dl,18,"for f in ('_type', 'url', 'ie_key'):","for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):"
youtube-dl,19,filename = expand_path(outtmpl % template_dict),"# expand_path translates '%%' into '%' and '$$' into '$'
# correspondingly that is not what we want since we need to keep
# '%%' intact for template dict substitution step. Working around
# with boundary-alike separator hack.
sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])
outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))
# outtmpl should be expand_path'ed before template dict substitution
# because meta fields may contain env variables we don't want to
# be expanded. For example, for outtmpl ""%(title)s.%(ext)s"" and
# title ""Hello $PATH"", we don't want `$PATH` to be expanded.
filename = expand_path(outtmpl).replace(sep, '') % template_dict"
youtube-dl,2,"try:
existing_format = next(
fo for fo in formats
if fo['format_id'] == representation_id)
except StopIteration:
full_info = formats_dict.get(representation_id, {}).copy()
full_info.update(f)
formats.append(full_info)
else:
existing_format.update(f)","# According to [1, 5.3.5.2, Table 7, page 35] @id of Representation
# is not necessarily unique within a Period thus formats with
# the same `format_id` are quite possible. There are numerous examples
# of such manifests (see https://github.com/rg3/youtube-dl/issues/15111,
# https://github.com/rg3/youtube-dl/issues/13919)
full_info = formats_dict.get(representation_id, {}).copy()
full_info.update(f)
formats.append(full_info)"
youtube-dl,20,"(?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'))*?
(?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'))*?","(?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'|))*?
(?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'|))*?"
youtube-dl,21,"if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):","if isinstance(path, bytes):
path = path.decode('utf-8')
if isinstance(base, bytes):
base = base.decode('utf-8')
if not isinstance(base, compat_str) or not re.match(
r'^(?:https?:)?//', base):"
youtube-dl,22,"if (m.group('strval') is not None or
comparison_value = m.group('strval') or m.group('intval')","(?P<quote>[""\'])(?P<quotedstrval>(?:\\.|(?!(?P=quote)|\\).)+?)(?P=quote)|
if (m.group('quotedstrval') is not None or
m.group('strval') is not None or
comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')
quote = m.group('quote')
if quote is not None:
comparison_value = comparison_value.replace(r'\%s' % quote, quote)"
youtube-dl,23,"elif v.startswith('/*') or v == ',':
/\*.*?\*/|,(?=\s*[\]}])|","elif v.startswith('/*') or v.startswith('//') or v == ',':
/\*.*?\*/|//[^\n]*|,(?=\s*[\]}])|"
youtube-dl,24,"if m.group('strval') is not None:
comparison_value = m.group('strval')
actual_value = dct.get(m.group('key'))","actual_value = dct.get(m.group('key'))
if (m.group('strval') is not None or
# If the original field is a string and matching comparisonvalue is
# a number we should respect the origin of the original field
# and process comparison value as a string (see
# https://github.com/rg3/youtube-dl/issues/11082).
actual_value is not None and m.group('intval') is not None and
isinstance(actual_value, compat_str)):
comparison_value = m.group('strval') or m.group('intval')"
youtube-dl,25,"(r'^0[xX][0-9a-fA-F]+', 16),
(r'^0+[0-7]+', 8),
i = int(im.group(0), base)","(r'^(0[xX][0-9a-fA-F]+)\s*:?$', 16),
(r'^(0+[0-7]+)\s*:?$', 8),
i = int(im.group(1), base)"
youtube-dl,26,(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|,\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
youtube-dl,27,"mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:\.\d+)?)$', time_expr)
return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))","mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:(?:\.|:)\d+)?)$', time_expr)
return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))"
youtube-dl,28,"return compat_chr(int(numstr, base))","# See https://github.com/rg3/youtube-dl/issues/7518
try:
return compat_chr(int(numstr, base))
except ValueError:
pass"
youtube-dl,29,return compat_str(upload_date),"if upload_date is not None:
return compat_str(upload_date)"
youtube-dl,3,"r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)","r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)"
youtube-dl,30,,
youtube-dl,31,"duration = int_or_none(self._html_search_regex(
r'(?s)<p class=""fileLeng[ht][th]"">.*?([0-9]+)\s*s',
(?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*(?:s|secs?|seconds?)?$''', s)
res = int(m.group('secs'))
if m.group('hours'):
res += int(m.group('hours')) * 60 * 60","parse_duration,
duration = parse_duration(self._html_search_regex(
r'(?s)<p class=""fileLeng[ht][th]"">.*?class=""bold"">(.*?)<',
(?:
(?P<only_mins>[0-9.]+)\s*(?:mins?|minutes?)\s*|
(?P<only_hours>[0-9.]+)\s*(?:hours?)|
(?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*(?:s|secs?|seconds?)?
)$''', s)
res = 0
if m.group('only_mins'):
return float_or_none(m.group('only_mins'), invscale=60)
if m.group('only_hours'):
return float_or_none(m.group('only_hours'), invscale=60 * 60)
if m.group('secs'):
res += int(m.group('secs'))
if m.group('hours'):
res += int(m.group('hours')) * 60 * 60"
youtube-dl,32,"transform_source=lambda j: re.sub(r'parseMetadata\((.*?)\);\n//.*$', r'\1', j)
return re.sub(r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?\s*$', r'\1', code)","strip_jsonp,
transform_source=strip_jsonp,
return re.sub(
r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)"
youtube-dl,33,"mobj = re.match(self._VALID_URL, url)
video_id = mobj.group('id')
timestamp = parse_iso8601(data['CreatedTime'][:-5])
r'Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',
date_format =  '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)","video_id = self._match_id(url)
timestamp = parse_iso8601(data['CreatedTime'])
r'(\.[0-9]+)?(?:Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',
date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)"
youtube-dl,34,"([0-9.]+|true|false|""[^""]*""|\'[^\']*\'|\[|\{)","if cause:
msg += u' (caused by %r)' % cause
([0-9.]+|true|false|""[^""]*""|\'[^\']*\'|
(?=\[|\{)
)"
youtube-dl,35,"'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),","upload_date_str = player_info.get('shootingDate')
if not upload_date_str:
upload_date_str = player_info.get('VDA', '').split(' ')[0]
'upload_date': unified_strdate(upload_date_str),
'%d/%m/%Y %H:%M:%S',"
youtube-dl,36,(?:[^#?]*\#!/)?,(?:[^#]*?\#!/)?
youtube-dl,37,"lambda m: m.group(0).decode('unicode-escape'), s)","unicode_escape = codecs.getdecoder('unicode_escape')
lambda m: unicode_escape(m.group(0))[0],
s)"
youtube-dl,38,"self.report_login()
login_page = self._download_webpage(login_page_req, None, note=False,
request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))
login_results = compat_urllib_request.urlopen(request).read()
'fb_dtsg': self._search_regex(r'""fb_dtsg"":""(.*?)""', login_results, 'fb_dtsg'),
'submit[Continue]': self._search_regex(r'<input value=""(.*?)"" name=""submit\[Continue\]""', login_results, 'continue'),
check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))
check_response = compat_urllib_request.urlopen(check_req).read()","urlencode_postdata,
login_page = self._download_webpage(login_page_req, None,
note='Downloading login page',
request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))
login_results = self._download_webpage(request, None,
note='Logging in', errnote='unable to fetch login page')
'fb_dtsg': self._search_regex(r'name=""fb_dtsg"" value=""(.+?)""', login_results, 'fb_dtsg'),
'submit[Continue]': self._search_regex(r'<button[^>]+value=""(.*?)""[^>]+name=""submit\[Continue\]""', login_results, 'continue'),
check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))
check_response = self._download_webpage(check_req, None,
note='Confirming login')
return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')"
youtube-dl,39,"if len(video_title) > 80 + 3:
video_title = video_title[:80] + '...'","limit_length,
}, {
'note': 'Video without discernible title',
'url': 'https://www.facebook.com/video.php?v=274175099429670',
'info_dict': {
'id': '274175099429670',
'ext': 'mp4',
'title': 'Facebook video #274175099429670',
}
video_title = limit_length(video_title, 80)
"""""" Add ellipses to overly long strings """"""
if s is None:
return None
ELLIPSES = '...'
if len(s) > length:
return s[:length - len(ELLIPSES)] + ELLIPSES
return s"
youtube-dl,4,"r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]+)\)$' % _NAME_RE, expr)
for v in m.group('args').split(',')])","r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]*)\)$' % _NAME_RE, expr)
for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()"
youtube-dl,40,"return unpack('!Q', self.read(8))[0]
return unpack('!I', self.read(4))[0]
return unpack('!B', self.read(1))[0]
stream.write(pack('!L', len(metadata))[1:])","struct_pack,
struct_unpack,
return struct_unpack('!Q', self.read(8))[0]
return struct_unpack('!I', self.read(4))[0]
return struct_unpack('!B', self.read(1))[0]
stream.write(struct_pack('!L', len(metadata))[1:])
struct.pack(u'!I', 0)
# In Python 2.6 (and some 2.7 versions), struct requires a bytes argument
def struct_pack(spec, *args):
if isinstance(spec, compat_str):
spec = spec.encode('ascii')
return struct.pack(spec, *args)
def struct_unpack(spec, *args):
if isinstance(spec, compat_str):
spec = spec.encode('ascii')
return struct.unpack(spec, *args)
struct_pack = struct.pack
struct_unpack = struct.unpack"
youtube-dl,41,"date_str = date_str.replace(',',' ')
date_str = re.sub(r' ?(\+|-)[0-9:]*$', '', date_str)","date_str = date_str.replace(',', ' ')
date_str = re.sub(r' ?(\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)"
youtube-dl,42,"fix_xml_all_ampersand,
transform_source=fix_xml_all_ampersand)
fix_xml_all_ampersand,
video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)
def fix_ampersand(s):
"""""" Fix unencoded ampersand in XML """"""
return s.replace(u'& ', '&amp; ')
u'Downloading info', transform_source=fix_ampersand)
return xml_str.replace(u'&', u'&amp;')","fix_xml_ampersands
transform_source=fix_xml_ampersands)
fix_xml_ampersands,
video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)
fix_xml_ampersands,
u'Downloading info', transform_source=fix_xml_ampersands)
return re.sub(
r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',
u'&amp;',
xml_str)"
youtube-dl,43,"m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)","m = re.match(r'(?:https?:|)//[^/]+/(?:[^?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)"
youtube-dl,5,"pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)
dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta
return calendar.timegm(timetuple.timetuple())","pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0
dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)
return calendar.timegm(timetuple) + pm_delta * 3600"
youtube-dl,6,"return 0.0
begin_time = parse_dfxp_time_expr(para.attrib['begin'])
end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])","return
begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))
dur = parse_dfxp_time_expr(para.attrib.get('dur'))
if begin_time is None:
continue
if not dur:
continue
end_time = begin_time + dur"
youtube-dl,7,"return v
if v.startswith(""'""):","v = re.sub(r""\\'"", ""'"", v[1:-1])
elif v.startswith(""'""):"
youtube-dl,8,"current_selector = None
selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))","current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])"
youtube-dl,9,"def _parse_format_selection(tokens, endwith=[]):
if string in endwith:
elif string == ')':
# ')' will be handled by the parentheses group
if string == ',':
second_choice = _parse_format_selection(tokens, [','])
current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])
audio_selector = _parse_format_selection(tokens, [','])
current_selector = None
selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))","def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):
if string == ')':
if not inside_group:
# ')' will be handled by the parentheses group
tokens.restore_last_token()
elif inside_merge and string in ['/', ',']:
elif inside_choice and string == ',':
tokens.restore_last_token()
break
elif string == ',':
second_choice = _parse_format_selection(tokens, inside_choice=True)
group = _parse_format_selection(tokens, inside_group=True)
current_selector = FormatSelector(GROUP, group, [])
audio_selector = _parse_format_selection(tokens, inside_merge=True)
current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])"
